<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>从零开始学LLaVA 03 | JiaHe&#39;s Blog</title>
<meta name="keywords" content="vision-language-models, LLaVA">
<meta name="description" content="本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。
模型部分
在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码
model = LlavaLlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                cache_dir=training_args.cache_dir,
                attn_implementation=attn_implementation,
                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
                **bnb_model_from_pretrained_args)
">
<meta name="author" content="">
<link rel="canonical" href="https://attract666.github.io/posts/llava_03/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://attract666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://attract666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://attract666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://attract666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://attract666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://attract666.github.io/posts/llava_03/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js"]
                }
            },
            "HTML-CSS": {
                availableFonts: ["Arial", "TeX"],
                preferredFont: "TeX",
                webFont: "TeX"
            }
        });

        MathJax.Hub.Queue(function () {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for (i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>

<meta property="og:title" content="从零开始学LLaVA 03" />
<meta property="og:description" content="本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。
模型部分
在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码
model = LlavaLlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                cache_dir=training_args.cache_dir,
                attn_implementation=attn_implementation,
                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
                **bnb_model_from_pretrained_args)
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://attract666.github.io/posts/llava_03/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-02T11:27:27+08:00" />
<meta property="article:modified_time" content="2024-12-02T11:27:27+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="从零开始学LLaVA 03"/>
<meta name="twitter:description" content="本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。
模型部分
在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码
model = LlavaLlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                cache_dir=training_args.cache_dir,
                attn_implementation=attn_implementation,
                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
                **bnb_model_from_pretrained_args)
"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://attract666.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "从零开始学LLaVA 03",
      "item": "https://attract666.github.io/posts/llava_03/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "从零开始学LLaVA 03",
  "name": "从零开始学LLaVA 03",
  "description": "本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。\n模型部分 在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码\nmodel = LlavaLlamaForCausalLM.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, attn_implementation=attn_implementation, torch_dtype=(torch.bfloat16 if training_args.bf16 else None), **bnb_model_from_pretrained_args) ",
  "keywords": [
    "vision-language-models", "LLaVA"
  ],
  "articleBody": "本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。\n模型部分 在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码\nmodel = LlavaLlamaForCausalLM.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, attn_implementation=attn_implementation, torch_dtype=(torch.bfloat16 if training_args.bf16 else None), **bnb_model_from_pretrained_args) 根据模型路径加载模型。LlavaLlamaForCausalLM类在我们介绍推理时也曾遇见过，当时我们是直接分析它的generate()方法，现在让我们再详细分析下这个类。\n首先，LlavaLlamaForCausalLM继承了LlamaForCausalLM和LlavaMetaForCausalLM两个类。其中LlamaForCausalLM就是transformers中的实现，而LlavaMetaForCausalLM就是我们之前在推理过程中分析的那个。成员变量中包括了用于处理输入提取特征的model和用于预测输出token的lm_head。由于输入中添加了图像，所以重写了LlamaForCausalLM中的forward()和generate()方法 其中用于输入提取的特征提取器是LLavaLlamaModel的实例。LlavaLlamaModel是一个抽象的组合类，继承了LlavaMetaModel和LlamaModel。组合文本和图像在进入llm之前的特征处理过程。LlavaMetaModel控制vision branch功能函数来完成细节的图像特征提取，而LlamaModel就是正常的LLM的通用功能 总的说来LlavaLlamaForCausalLM用于负责总体逻辑统筹，具体的文本侧和视觉侧则交给LlamaModel和LlavaMetaModel 其中比较关键的在于需要自己实现的LlavaMetaModel和LlavaMetaForCausalLM LlavaMetaForCausalLM 这个是我们的老朋友了，在第二章的推理部分我们也分析过它的prepare_inputs_labels_for_multimodal()方法。\nclass LlavaMetaForCausalLM(ABC): @abstractmethod def get_model(self):... def get_vision_tower(self):... def encode_images(self, images):... def prepare_inputs_labels_for_multimodal( self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_size=None):... def initialize_vision_tokenizer(self, model_args, tokenizer):... 首先第一个方法是get_model()，这是一个抽象方法，子类实现了该抽象方法才能被实例化，基类不能实例化。所以可以在LlavaLlamaForCausalLM类中可以看到重新实现了get_model()，就是返回模型。\n之后是get_vision_tower()，也很简单，返回model的visual encoder。\nencode_images()就是提取图像特征，这里的图像特征是经过visual encoder和投影层之后的特征。\nprepare_inputs_labels_for_multimodal()由于之前介绍过推理时的逻辑，所以这里暂时不展开说，在后面的前向传播时，我们会重新看到它作为训练时的逻辑。\n最后一个方法是initialize_vision_tokenizer()，主要用于初始化视觉模型的tokenizer。\nLlavaMetaModel class LlavaMetaModel: def __init__(self, config):... def get_vision_tower(self):... def initialize_vision_modules(self, model_args, fsdp=None):... 先看初始化__init__()很简单，就是构建visual encoder和projector，值得注意的是，在训练时我们的脚本文件是没有显式指定参数mm_vision_tower的，所以visual encoder和projecto并不是在一开始就初始化好的，而是在LLaVA/llava/train/train.py的第911行通过调用第三个方法进行构建的。\nget_vision_tower()就是返回visual encoder。\ninitialize_vision_modules()才是训练阶段真正构建visual encoder和projector的地方。在这里才添加了参数mm_vision_tower。这里的参数mm_vision_select_layer是-1也就是默认选择最后一层，当然在官方提供的脚本文件中是指定为-2了。\n具体的visual encoder和projector的实现对应于LLaVA/llava/model/multimodal_encoder/builder.py和LLaVA/llava/model/multimodal_projector/builder.py中，对应于其中调用的类。如果想要更换LLaVA的visual encoder和projector可以去对应builder.py文件下import对应的class以及实例化，并在相应的文件夹下实现对应的class\n数据部分 在LLaVA/llava/train/train.py的第959行会出现关于数据集调用的语句\ndata_module = make_surpervised_data_module(tokenizer=tokenizer, data_args=data_args) make_surpervised_data_module()方法在LLaVA/llava/train/train.py的第776行\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -\u003e Dict: \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\" train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator) 这里分别数据集和整理器，我们一一来看\ndataset class LazySupervisedDataset(Dataset): def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, data_args: DataArguments):... def __len__(self):... @property def lengths(self):... @property def modality_lengths(self):... def __getitem__(self, i) -\u003e Dict[str, torch.Tensor]:... 首先作为一个Dataset类，老三样一定少不了。__init__()就是一些初始化，这里也可以看到LLaVA训练过程中用到的是json格式的数据。__len__()就是返回数据的长度。\n关键在于这个__getitem__()，它负责从data_list中按照data_sampler采样，得到样本序号，取出数据并进行处理，以得到模型需要的输入。可以看到稍后返回的是一个字典样式的数据。\n首先判断当前数据项sources[0]中是否包含image 读取图片并对图片进行预处理，判断image_aspect_ratio是什么方式 pad方式，会对图片根据最长边填充为正方形，具体的方法在expand2square()。pad的值为每个channel预处理操作对应的mean，会保证原始的图像内容放置在正中心。这样保持了图像的长宽比例不变（1:1），这样不会导致图像在某一维度上的失真或变形 通过preprocess_multimodal对文本进行预处理，判断is_multimodal 遍历每一个样本的conversation，如果一次对话（即conversation中的一个字典{from, value}条目）中的value有token，就把token插入到新的行前，如果使用图像开始结束token，则添加图像开始结束token 通过preprocess()进一步将conversations list 转化为一段以正常文本形式展现的完整对话，图像仍为占位符形式，该过程借助conversation_lib 将conversations list 添加模板 对conversations进行tokenize，这个就是input_ids 获取targets，只需用上述input_ids进行clone（next-token prediction常规操作） 当我们需要更换LLaVA的LLM时，主要就是要更改这里的preprocess_xxx模板\n剩余的两个方法均被@property装饰，其中lengths用来统计数据集中每个样本的长度（如果包含图像则+128）。modality_lengths用来统计每个样本长度（不包含图像token的长度），不同之处在于返回的信息会区分是否存在图像，为正则样本包含图像，为负样本不包含图像。\ndata_collator @dataclass class DataCollatorForSupervisedDataset(object): tokenizer: transformers.PreTraineTokenizer def __call__(self, instances: Sequence[Dict]) -\u003e Dict[str, torch.Tensor]:... 其输入instances为上述Dataset定义的若干个dict输出组成的Sequence。从每个dict中取出对应的input_ids和label，同时pad到相同长度，最后组合成一个batch的形式，图像也会放入batch中。\n前向传播 Pretrain时的loss计算 了解LLaVA的loss是如何计算的前提，是要了解LLaVA的labels对哪些内容进行mask了。在接下来的部分，我们会更改LLaVA提供的原始Pretrain脚本中的默认conversation模板，因为如果使用原始脚本中的Plain提供的信息太少了，它不提供system模板。我们将使用conv_vicuna_v1作为conversation模板，对应的处理函数在LLaVA/llava/train/train.py的414行preprocess_v1()。\nconv_vicuna_v1对应的conversation模板如下：\nconv_vicuna_v1 = Conversation( system=\"A chat between a curious user and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=(\"USER\", \"ASSISTANT\"), version=\"v1\", messages=(), offset=0, sep_style=SeparatorStyle.TWO, sep=\" \", sep2=\"\", ) 首先，假设我们的sources是如下格式的：\n[{'from': 'human', 'value': '\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'floor plan app floorplugged house and home'}] 先会对sources使用模板，会得到如下格式：\n[\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: \\nGive a brief description of the image. ASSISTANT: floor plan app floorplugged house and home\"] A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: \\n Give a brief description of the image. ASSISTANT: floor plan app floorplugged house and home\n注意只有一次对话结束后，才会插入sep2=，system prompt与对话之间使用sep连接。\n接下来对sources进行tokenize\ntensor([[ 1, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, -200, 29871, 13, 29954, 573, 263, 11473, 6139, 310, 278, 1967, 29889, 319, 1799, 9047, 13566, 29901, 11904, 3814, 623, 11904, 572, 688, 3192, 3699, 322, 3271, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) 其中-200就是token的位置，targets就是inputs_ids直接clone得到的\nmask后的targets为：\ntensor([ -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11904, 3814, 623, 11904, 572, 688, 3192, 3699, 322, 3271, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]) # tokenizer.batch_decode(tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False) # ['floor', 'plan', 'app', 'floor', 'pl', 'ug', 'ged', 'house', 'and', 'home', ''] 这个是prepare_inputs_labels_for_multimodal()中的最初输入的labels，记为_labels，与生成时不同之处在于进行了右填充\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: \\n Give a brief description of the image. ASSISTANT: floor plan app floorplugged house and home\n这里我们再将training过程中的attention_mask也打印出来看一下：\ntensor([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False]) 在prepare_inputs_labels_for_multimodal()中会将pad部分去除掉，最后返回new_input_embeds和new_labels，忽略掉pad token 的labels的shape也会将之前的 _labels不同，因为会将image编码成576个 token，但是这部分在labels中仍然为-100\n从上面的过程中，我们可以看到在PreTrain过程中mask掉的部分system prompt、Instruction和pad部分。对于这个例子只有ASSISTANT说的话会被用来计算loss\n在实际计算loss时，会调用transformers.Trainer的compute_loss()方法，也就意味着如果你需要更改计算loss的方式，需要在LLaVATrainer中重写compute_loss方法。\n而原始的compute_loss()方法，最后会在transformers.LlamaForCausalLM的forward()中进行计算，具体的代码为：\nloss = None if labels is not None: # Shift so that tokens \u003c n predict n shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() # Flatten the tokens loss_fct = CrossEntropyLoss() shift_logits = shift_logits.view(-1, self.config.vocab_size) shift_labels = shift_labels.view(-1) # Enable model parallelism shift_labels = shift_labels.to(shift_logits.device) loss = loss_fct(shift_logits, shift_labels) 参考 [1] Github:LLaVA(https://github.com/haotian-liu/LLaVA)\n[2] Huggingface:Trainer(https://huggingface.co/docs/transformers/main_classes/trainer)\n",
  "wordCount" : "660",
  "inLanguage": "zh",
  "datePublished": "2024-12-02T11:27:27+08:00",
  "dateModified": "2024-12-02T11:27:27+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://attract666.github.io/posts/llava_03/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JiaHe's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://attract666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://attract666.github.io/" accesskey="h" title="JiaHe&#39;s Blog (Alt + H)">JiaHe&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://attract666.github.io/posts/" title="All posts">
                    <span>All posts</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      从零开始学LLaVA 03
    </h1>
    <div class="post-meta"><span title='2024-12-02 11:27:27 +0800 CST'>十二月 2, 2024</span>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e6%a8%a1%e5%9e%8b%e9%83%a8%e5%88%86" aria-label="模型部分">模型部分</a><ul>
                            
                    <li>
                        <a href="#llavametaforcausallm" aria-label="LlavaMetaForCausalLM">LlavaMetaForCausalLM</a></li>
                    <li>
                        <a href="#llavametamodel" aria-label="LlavaMetaModel">LlavaMetaModel</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e6%95%b0%e6%8d%ae%e9%83%a8%e5%88%86" aria-label="数据部分">数据部分</a><ul>
                            
                    <li>
                        <a href="#dataset" aria-label="dataset">dataset</a></li>
                    <li>
                        <a href="#data_collator" aria-label="data_collator">data_collator</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="前向传播">前向传播</a><ul>
                            
                    <li>
                        <a href="#pretrain%e6%97%b6%e7%9a%84loss%e8%ae%a1%e7%ae%97" aria-label="Pretrain时的loss计算">Pretrain时的loss计算</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在<code>LLaVA/scripts/v1_5/pretrain.sh</code>，视觉指令微调启动的脚本在<code>LLaVA/scripts/v1_5/finetune.sh</code>。两个脚本文件执行的都是<code>LLaVA/llava/train/train_mem.py</code>，在<code>LLaVA/llava/train</code>目录下还有一个<code>train.py</code>，这个才是真正的总体路口，不同点在于是否使用<code>flash_attention_2</code>。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。</p>
<h1 id="模型部分">模型部分<a hidden class="anchor" aria-hidden="true" href="#模型部分">#</a></h1>
<p>在<code>LLaVA/llava/train/train.py</code>的第827行会出现初始化模型的代码</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#5bc4bf">=</span> LlavaLlamaForCausalLM<span style="color:#5bc4bf">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>                model_args<span style="color:#5bc4bf">.</span>model_name_or_path,
</span></span><span style="display:flex;"><span>                cache_dir<span style="color:#5bc4bf">=</span>training_args<span style="color:#5bc4bf">.</span>cache_dir,
</span></span><span style="display:flex;"><span>                attn_implementation<span style="color:#5bc4bf">=</span>attn_implementation,
</span></span><span style="display:flex;"><span>                torch_dtype<span style="color:#5bc4bf">=</span>(torch<span style="color:#5bc4bf">.</span>bfloat16 <span style="color:#815ba4">if</span> training_args<span style="color:#5bc4bf">.</span>bf16 <span style="color:#815ba4">else</span> <span style="color:#815ba4">None</span>),
</span></span><span style="display:flex;"><span>                <span style="color:#5bc4bf">**</span>bnb_model_from_pretrained_args)
</span></span></code></pre></div><p>根据模型路径加载模型。<code>LlavaLlamaForCausalLM</code>类在我们介绍推理时也曾遇见过，当时我们是直接分析它的<code>generate()</code>方法，现在让我们再详细分析下这个类。</p>
<ul>
<li>首先，<code>LlavaLlamaForCausalLM</code>继承了<code>LlamaForCausalLM</code>和<code>LlavaMetaForCausalLM</code>两个类。其中<code>LlamaForCausalLM</code>就是transformers中的实现，而<code>LlavaMetaForCausalLM</code>就是我们之前在推理过程中分析的那个。成员变量中包括了用于处理输入提取特征的<code>model</code>和用于预测输出token的<code>lm_head</code>。由于输入中添加了图像，所以重写了<code>LlamaForCausalLM</code>中的<code>forward()</code>和<code>generate()</code>方法</li>
<li>其中用于输入提取的特征提取器是<code>LLavaLlamaModel</code>的实例。<code>LlavaLlamaModel</code>是一个抽象的组合类，继承了<code>LlavaMetaModel</code>和<code>LlamaModel</code>。组合文本和图像在进入llm之前的特征处理过程。<code>LlavaMetaModel</code>控制vision branch功能函数来完成细节的图像特征提取，而<code>LlamaModel</code>就是正常的LLM的通用功能</li>
<li>总的说来<code>LlavaLlamaForCausalLM</code>用于负责总体逻辑统筹，具体的文本侧和视觉侧则交给<code>LlamaModel</code>和<code>LlavaMetaModel</code></li>
<li>其中比较关键的在于需要自己实现的<code>LlavaMetaModel</code>和<code>LlavaMetaForCausalLM</code></li>
</ul>
<h2 id="llavametaforcausallm">LlavaMetaForCausalLM<a hidden class="anchor" aria-hidden="true" href="#llavametaforcausallm">#</a></h2>
<p>这个是我们的老朋友了，在第二章的推理部分我们也分析过它的<code>prepare_inputs_labels_for_multimodal()</code>方法。</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">LlavaMetaForCausalLM</span>(ABC):
</span></span><span style="display:flex;"><span>    <span style="color:#5bc4bf">@abstractmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">get_model</span>(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">get_vision_tower</span>(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">encode_images</span>(self, images):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">prepare_inputs_labels_for_multimodal</span>(
</span></span><span style="display:flex;"><span>        self, input_ids, position_ids, attention_mask, past_key_values, labels,
</span></span><span style="display:flex;"><span>        images, image_size<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">initialize_vision_tokenizer</span>(self, model_args, tokenizer):<span style="color:#5bc4bf">...</span>
</span></span></code></pre></div><p>首先第一个方法是<code>get_model()</code>，这是一个抽象方法，子类实现了该抽象方法才能被实例化，基类不能实例化。所以可以在<code>LlavaLlamaForCausalLM</code>类中可以看到重新实现了<code>get_model()</code>，就是返回模型。</p>
<p>之后是<code>get_vision_tower()</code>，也很简单，返回<code>model</code>的visual encoder。</p>
<p><code>encode_images()</code>就是提取图像特征，这里的图像特征是经过visual encoder和投影层之后的特征。</p>
<p><code>prepare_inputs_labels_for_multimodal()</code>由于之前介绍过推理时的逻辑，所以这里暂时不展开说，在后面的前向传播时，我们会重新看到它作为训练时的逻辑。</p>
<p>最后一个方法是<code>initialize_vision_tokenizer()</code>，主要用于初始化视觉模型的tokenizer。</p>
<h2 id="llavametamodel">LlavaMetaModel<a hidden class="anchor" aria-hidden="true" href="#llavametamodel">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">LlavaMetaModel</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, config):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">get_vision_tower</span>(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">initialize_vision_modules</span>(self, model_args, fsdp<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>):<span style="color:#5bc4bf">...</span>
</span></span></code></pre></div><p>先看初始化<code>__init__()</code>很简单，就是构建visual encoder和projector，值得注意的是，在训练时我们的脚本文件是没有显式指定参数<code>mm_vision_tower</code>的，所以visual encoder和projecto并不是在一开始就初始化好的，而是在<code>LLaVA/llava/train/train.py</code>的第911行通过调用第三个方法进行构建的。</p>
<p><code>get_vision_tower()</code>就是返回visual encoder。</p>
<p><code>initialize_vision_modules()</code>才是训练阶段真正构建visual encoder和projector的地方。在这里才添加了参数<code>mm_vision_tower</code>。这里的参数<code>mm_vision_select_layer</code>是<code>-1</code>也就是默认选择最后一层，当然在官方提供的脚本文件中是指定为<code>-2</code>了。</p>
<p>具体的visual encoder和projector的实现对应于<code>LLaVA/llava/model/multimodal_encoder/builder.py</code>和<code>LLaVA/llava/model/multimodal_projector/builder.py</code>中，对应于其中调用的类。<strong>如果想要更换LLaVA的visual encoder和projector可以去对应<code>builder.py</code>文件下import对应的class以及实例化，并在相应的文件夹下实现对应的class</strong></p>
<hr>
<h1 id="数据部分">数据部分<a hidden class="anchor" aria-hidden="true" href="#数据部分">#</a></h1>
<p>在<code>LLaVA/llava/train/train.py</code>的第959行会出现关于数据集调用的语句</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data_module <span style="color:#5bc4bf">=</span> make_surpervised_data_module(tokenizer<span style="color:#5bc4bf">=</span>tokenizer,
</span></span><span style="display:flex;"><span>                                           data_args<span style="color:#5bc4bf">=</span>data_args)
</span></span></code></pre></div><p><code>make_surpervised_data_module()</code>方法在<code>LLaVA/llava/train/train.py</code>的第776行</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">def</span> <span style="color:#06b6ef">make_supervised_data_module</span>(tokenizer: transformers<span style="color:#5bc4bf">.</span>PreTrainedTokenizer,
</span></span><span style="display:flex;"><span>                                data_args) <span style="color:#5bc4bf">-&gt;</span> Dict:
</span></span><span style="display:flex;"><span>    <span style="color:#48b685">&#34;&#34;&#34;Make dataset and collator for supervised fine-tuning.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    train_dataset <span style="color:#5bc4bf">=</span> LazySupervisedDataset(tokenizer<span style="color:#5bc4bf">=</span>tokenizer,
</span></span><span style="display:flex;"><span>                                data_path<span style="color:#5bc4bf">=</span>data_args<span style="color:#5bc4bf">.</span>data_path,
</span></span><span style="display:flex;"><span>                                data_args<span style="color:#5bc4bf">=</span>data_args)
</span></span><span style="display:flex;"><span>    data_collator <span style="color:#5bc4bf">=</span> DataCollatorForSupervisedDataset(tokenizer<span style="color:#5bc4bf">=</span>tokenizer)
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">return</span> dict(train_dataset<span style="color:#5bc4bf">=</span>train_dataset,
</span></span><span style="display:flex;"><span>                eval_dataset<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>,
</span></span><span style="display:flex;"><span>                data_collator<span style="color:#5bc4bf">=</span>data_collator)
</span></span></code></pre></div><p>这里分别数据集和整理器，我们一一来看</p>
<h2 id="dataset">dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">LazySupervisedDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, data_path: str,
</span></span><span style="display:flex;"><span>                 tokenizer: transformers<span style="color:#5bc4bf">.</span>PreTrainedTokenizer,
</span></span><span style="display:flex;"><span>                 data_args: DataArguments):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __len__(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#5bc4bf">@property</span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">lengths</span>(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#5bc4bf">@property</span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">modality_lengths</span>(self):<span style="color:#5bc4bf">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __getitem__(self, i) <span style="color:#5bc4bf">-&gt;</span> Dict[str, torch<span style="color:#5bc4bf">.</span>Tensor]:<span style="color:#5bc4bf">...</span>
</span></span></code></pre></div><p>首先作为一个<code>Dataset</code>类，老三样一定少不了。<code>__init__()</code>就是一些初始化，这里也可以看到LLaVA训练过程中用到的是json格式的数据。<code>__len__()</code>就是返回数据的长度。</p>
<p>关键在于这个<code>__getitem__()</code>，它负责从data_list中按照data_sampler采样，得到样本序号，取出数据并进行处理，以得到模型需要的输入。可以看到稍后返回的是一个字典样式的数据。</p>
<ul>
<li>首先判断当前数据项<code>sources[0]</code>中是否包含<code>image</code></li>
<li>读取图片并对图片进行预处理，判断<code>image_aspect_ratio</code>是什么方式
<ul>
<li><code>pad</code>方式，会对图片根据最长边填充为正方形，具体的方法在<code>expand2square()</code>。pad的值为每个channel预处理操作对应的mean，会保证原始的图像内容放置在正中心。这样保持了图像的长宽比例不变（1:1），这样不会导致图像在某一维度上的失真或变形</li>
</ul>
</li>
<li>通过<code>preprocess_multimodal</code>对文本进行预处理，判断<code>is_multimodal</code>
<ul>
<li>遍历每一个样本的<code>conversation</code>，如果一次对话（即conversation中的一个字典{from, value}条目）中的<code>value</code>有<code>&lt;image&gt;</code>token，就把<code>&lt;image&gt;</code>token插入到新的行前，如果使用图像开始结束token，则添加图像开始结束token</li>
</ul>
</li>
<li>通过<code>preprocess()</code>进一步将conversations list 转化为一段以正常文本形式展现的完整对话，图像仍为占位符形式，该过程借助<code>conversation_lib</code>
<ul>
<li>将conversations list 添加模板</li>
<li>对conversations进行tokenize，这个就是<code>input_ids</code></li>
<li>获取<code>targets</code>，只需用上述<code>input_ids</code>进行clone（next-token prediction常规操作）</li>
</ul>
</li>
</ul>
<p><strong>当我们需要更换LLaVA的LLM时，主要就是要更改这里的<code>preprocess_xxx</code>模板</strong></p>
<p>剩余的两个方法均被<code>@property</code>装饰，其中<code>lengths</code>用来统计数据集中每个样本的长度（如果包含图像则+128）。<code>modality_lengths</code>用来统计每个样本长度（不包含图像token的长度），不同之处在于返回的信息会区分是否存在图像，为正则样本包含图像，为负样本不包含图像。</p>
<h2 id="data_collator">data_collator<a hidden class="anchor" aria-hidden="true" href="#data_collator">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">DataCollatorForSupervisedDataset</span>(object):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    tokenizer: transformers<span style="color:#5bc4bf">.</span>PreTraineTokenizer
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __call__(self, instances: Sequence[Dict]) <span style="color:#5bc4bf">-&gt;</span> Dict[str, torch<span style="color:#5bc4bf">.</span>Tensor]:<span style="color:#5bc4bf">...</span>
</span></span></code></pre></div><p>其输入<code>instances</code>为上述Dataset定义的若干个dict输出组成的Sequence。从每个dict中取出对应的<code>input_ids</code>和<code>label</code>，同时pad到相同长度，最后组合成一个batch的形式，图像也会放入batch中。</p>
<hr>
<h1 id="前向传播">前向传播<a hidden class="anchor" aria-hidden="true" href="#前向传播">#</a></h1>
<h2 id="pretrain时的loss计算">Pretrain时的loss计算<a hidden class="anchor" aria-hidden="true" href="#pretrain时的loss计算">#</a></h2>
<p>了解LLaVA的loss是如何计算的前提，是要了解LLaVA的<code>labels</code>对哪些内容进行mask了。在接下来的部分，我们会更改LLaVA提供的原始Pretrain脚本中的默认conversation模板，因为如果使用原始脚本中的<code>Plain</code>提供的信息太少了，它不提供<code>system</code>模板。我们将使用<code>conv_vicuna_v1</code>作为conversation模板，对应的处理函数在<code>LLaVA/llava/train/train.py</code>的414行<code>preprocess_v1()</code>。</p>
<p><code>conv_vicuna_v1</code>对应的conversation模板如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conv_vicuna_v1 <span style="color:#5bc4bf">=</span> Conversation(
</span></span><span style="display:flex;"><span>    system<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#34;A chat between a curious user and an artificial intelligence assistant. &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#48b685">&#34;The assistant gives helpful, detailed, and polite answers to the user&#39;s questions.&#34;</span>,
</span></span><span style="display:flex;"><span>    roles<span style="color:#5bc4bf">=</span>(<span style="color:#48b685">&#34;USER&#34;</span>, <span style="color:#48b685">&#34;ASSISTANT&#34;</span>),
</span></span><span style="display:flex;"><span>    version<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#34;v1&#34;</span>,
</span></span><span style="display:flex;"><span>    messages<span style="color:#5bc4bf">=</span>(),
</span></span><span style="display:flex;"><span>    offset<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0</span>,
</span></span><span style="display:flex;"><span>    sep_style<span style="color:#5bc4bf">=</span>SeparatorStyle<span style="color:#5bc4bf">.</span>TWO,
</span></span><span style="display:flex;"><span>    sep<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#34; &#34;</span>,
</span></span><span style="display:flex;"><span>    sep2<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#34;&lt;/s&gt;&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>首先，假设我们的<code>sources</code>是如下格式的：</p>
<pre tabindex="0"><code>[{&#39;from&#39;: &#39;human&#39;, &#39;value&#39;: &#39;&lt;image&gt;\nGive a brief description of the image.&#39;}, {&#39;from&#39;: &#39;gpt&#39;, &#39;value&#39;: &#39;floor plan app floorplugged house and home&#39;}]
</code></pre><p>先会对<code>sources</code>使用模板，会得到如下格式：</p>
<pre tabindex="0"><code>[&#34;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions. USER: &lt;image&gt;\nGive a brief description of the image. ASSISTANT: floor plan app floorplugged house and home&lt;/s&gt;&#34;]
</code></pre><p><font color='blue'>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&rsquo;s questions.</font> <font color='red'>USER: &lt;image&gt;\n Give a brief description of the image.</font> <font color='green'>ASSISTANT: floor plan app floorplugged house and home&lt;/s&gt;</font></p>
<p>注意只有一次对话结束后，才会插入<code>sep2=&lt;/s&gt;</code>，<code>system prompt</code>与对话之间使用<code>sep</code>连接。</p>
<p>接下来对<code>sources</code>进行tokenize</p>
<pre tabindex="0"><code>tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,
         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,
           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,
         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13, 29954,   573,
           263, 11473,  6139,   310,   278,  1967, 29889,   319,  1799,  9047,
         13566, 29901, 11904,  3814,   623, 11904,   572,   688,  3192,  3699,
           322,  3271,     2,     0,      0,    0,     0,     0,     0,     0,
             0,     0,     0]])
</code></pre><p>其中<code>-200</code>就是<code>&lt;image&gt;</code>token的位置，<code>targets</code>就是<code>inputs_ids</code>直接clone得到的</p>
<p>mask后的<code>targets</code>为：</p>
<pre tabindex="0"><code>tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100, 11904,  3814,   623, 11904,   572,   688,  3192,  3699,
          322,  3271,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100])

# tokenizer.batch_decode(tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)
# [&#39;floor&#39;, &#39;plan&#39;, &#39;app&#39;, &#39;floor&#39;, &#39;pl&#39;, &#39;ug&#39;, &#39;ged&#39;, &#39;house&#39;, &#39;and&#39;, &#39;home&#39;, &#39;&lt;/s&gt;&#39;]
</code></pre><p>这个是<code>prepare_inputs_labels_for_multimodal()</code>中的最初输入的labels，记为<code>_labels</code>，与生成时不同之处在于进行了<strong>右填充</strong></p>
<p>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&rsquo;s questions. USER: &lt;image&gt;\n Give a brief description of the image. ASSISTANT: <font color='green'><strong>floor plan app floorplugged house and home&lt;/s&gt;</strong></font></p>
<p>这里我们再将training过程中的<code>attention_mask</code>也打印出来看一下：</p>
<pre tabindex="0"><code>tensor([  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True, False, False, False, False, False, False, False,
         False, False, False])
</code></pre><p>在<code>prepare_inputs_labels_for_multimodal()</code>中会将pad部分去除掉，最后返回<code>new_input_embeds</code>和<code>new_labels</code>，忽略掉<code>pad</code> token 的<code>labels</code>的shape也会将之前的
<code>_labels</code>不同，因为会将image编码成576个 token，但是这部分在<code>labels</code>中仍然为<code>-100</code></p>
<p>从上面的过程中，我们可以看到在PreTrain过程中mask掉的部分<code>system prompt</code>、<code>Instruction</code>和<code>pad</code>部分。对于这个例子只有ASSISTANT说的话会被用来计算loss</p>
<p>在实际计算loss时，会调用<code>transformers.Trainer</code>的<code>compute_loss()</code>方法，也就意味着如果<strong>你需要更改计算loss的方式，需要在LLaVATrainer中重写<code>compute_loss</code>方法。</strong></p>
<p>而原始的<code>compute_loss()</code>方法，最后会在<code>transformers.LlamaForCausalLM</code>的<code>forward()</code>中进行计算，具体的代码为：</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss <span style="color:#5bc4bf">=</span> <span style="color:#815ba4">None</span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">if</span> labels <span style="color:#5bc4bf">is</span> <span style="color:#5bc4bf">not</span> <span style="color:#815ba4">None</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#8d8687"># Shift so that tokens &lt; n predict n</span>
</span></span><span style="display:flex;"><span>    shift_logits <span style="color:#5bc4bf">=</span> logits[<span style="color:#5bc4bf">...</span>, :<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>, :]<span style="color:#5bc4bf">.</span>contiguous()
</span></span><span style="display:flex;"><span>    shift_labels <span style="color:#5bc4bf">=</span> labels[<span style="color:#5bc4bf">...</span>, <span style="color:#f99b15">1</span>:]<span style="color:#5bc4bf">.</span>contiguous()
</span></span><span style="display:flex;"><span>    <span style="color:#8d8687"># Flatten the tokens</span>
</span></span><span style="display:flex;"><span>    loss_fct <span style="color:#5bc4bf">=</span> CrossEntropyLoss()
</span></span><span style="display:flex;"><span>    shift_logits <span style="color:#5bc4bf">=</span> shift_logits<span style="color:#5bc4bf">.</span>view(<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>, self<span style="color:#5bc4bf">.</span>config<span style="color:#5bc4bf">.</span>vocab_size)
</span></span><span style="display:flex;"><span>    shift_labels <span style="color:#5bc4bf">=</span> shift_labels<span style="color:#5bc4bf">.</span>view(<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#8d8687"># Enable model parallelism</span>
</span></span><span style="display:flex;"><span>    shift_labels <span style="color:#5bc4bf">=</span> shift_labels<span style="color:#5bc4bf">.</span>to(shift_logits<span style="color:#5bc4bf">.</span>device)
</span></span><span style="display:flex;"><span>    loss <span style="color:#5bc4bf">=</span> loss_fct(shift_logits, shift_labels)
</span></span></code></pre></div><hr>
<h1 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h1>
<p>[1] Github:LLaVA(<a href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA</a>)</p>
<p>[2] Huggingface:Trainer(<a href="https://huggingface.co/docs/transformers/main_classes/trainer">https://huggingface.co/docs/transformers/main_classes/trainer</a>)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://attract666.github.io/tags/vision-language-models/">Vision-Language-Models</a></li>
      <li><a href="https://attract666.github.io/tags/llava/">LLaVA</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://attract666.github.io/">JiaHe&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

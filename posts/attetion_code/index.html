<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>手撕Multihead Attention | JiaHe&#39;s Blog</title>
<meta name="keywords" content="vision-language-models, LLaVA, LLama">
<meta name="description" content="为了方便讲解，我们先对文本进行tokenize和embedding。我们仅在这一步使用了除torch以外的库
text = &#34;This is your first step to understanding LLM&#34;
kwargs = {&#34;device_map&#34;: &#34;cuda:3&#34;}
tokenizer = AutoTokenizer.from_pretrained(&#34;llama3/Meta-Llama-3-8B&#34;, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(&#34;llama3/Meta-Llama-3-8B&#34;, **kwargs)

inputs = tokenizer(text, return_tensors=&#34;pt&#34;)
inputs = {key: value.to(kwargs[&#34;device_map&#34;]) for key, value in inputs.items()}
inputs = model.get_input_embeddings()(inputs[&#34;input_ids&#34;])
">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/attetion_code/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/posts/attetion_code/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js"]
                }
            },
            "HTML-CSS": {
                availableFonts: ["Arial", "TeX"],
                preferredFont: "TeX",
                webFont: "TeX"
            }
        });

        MathJax.Hub.Queue(function () {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for (i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="JiaHe&#39;s Blog (Alt + H)">JiaHe&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="All posts">
                    <span>All posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      手撕Multihead Attention
    </h1>
    <div class="post-meta"><span title='2024-12-22 19:21:37 +0800 CST'>十二月 22, 2024</span>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e5%85%b3%e6%b3%a8%e8%be%93%e5%85%a5%e7%9a%84%e4%b8%8d%e5%90%8c%e5%8c%ba%e5%9f%9f" aria-label="自注意力机制关注输入的不同区域">自注意力机制关注输入的不同区域</a><ul>
                            
                    <li>
                        <a href="#%e4%b8%8d%e5%8c%85%e5%90%ab%e5%8f%af%e8%ae%ad%e7%bb%83%e6%9d%83%e9%87%8d%e7%9a%84%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6" aria-label="不包含可训练权重的自注意力机制">不包含可训练权重的自注意力机制</a></li>
                    <li>
                        <a href="#%e4%bd%bf%e7%94%a8%e5%8f%af%e8%ae%ad%e7%bb%83%e6%9d%83%e9%87%8d%e5%ae%9e%e7%8e%b0%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6" aria-label="使用可训练权重实现自注意力机制">使用可训练权重实现自注意力机制</a></li>
                    <li>
                        <a href="#%e5%ae%9e%e7%8e%b0%e4%b8%80%e4%b8%aaselfattention%e7%b1%bb" aria-label="实现一个SelfAttention类">实现一个SelfAttention类</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e9%81%ae%e8%94%bd%e4%b8%8b%e6%96%87%e4%bf%a1%e6%81%af%e7%9a%84%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6" aria-label="遮蔽下文信息的注意力机制">遮蔽下文信息的注意力机制</a><ul>
                            
                    <li>
                        <a href="#%e4%bd%bf%e7%94%a8%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%8e%a9%e7%a0%81" aria-label="使用因果注意力掩码">使用因果注意力掩码</a></li>
                    <li>
                        <a href="#%e9%80%9a%e8%bf%87dropout%e6%9d%a5%e5%ae%9e%e7%8e%b0%e9%a2%9d%e5%a4%96%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d%e7%9a%84%e6%8e%a9%e7%a0%81" aria-label="通过dropout来实现额外注意力权重的掩码">通过dropout来实现额外注意力权重的掩码</a></li>
                    <li>
                        <a href="#%e5%ae%9e%e7%8e%b0%e4%b8%80%e4%b8%aacausalattention%e7%b1%bb" aria-label="实现一个CausalAttention类">实现一个CausalAttention类</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%b0%86%e5%8d%95%e5%a4%b4%e6%8b%93%e5%b1%95%e5%88%b0%e5%a4%9a%e5%a4%b4" aria-label="将单头拓展到多头">将单头拓展到多头</a><ul>
                            
                    <li>
                        <a href="#%e7%9b%b4%e6%8e%a5%e5%b0%86%e5%a4%9a%e4%b8%aa%e5%8d%95%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%b1%82%e5%a0%86%e7%a7%af%e8%b5%b7%e6%9d%a5" aria-label="直接将多个单头注意力层堆积起来">直接将多个单头注意力层堆积起来</a></li>
                    <li>
                        <a href="#%e9%80%9a%e8%bf%87%e6%9d%83%e9%87%8d%e5%88%86%e5%89%b2%e5%ae%9e%e7%8e%b0%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b" aria-label="通过权重分割实现多头注意力">通过权重分割实现多头注意力</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>为了方便讲解，我们先对文本进行tokenize和embedding。我们仅在这一步使用了除torch以外的库</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>text <span style="color:#5bc4bf">=</span> <span style="color:#48b685">&#34;This is your first step to understanding LLM&#34;</span>
</span></span><span style="display:flex;"><span>kwargs <span style="color:#5bc4bf">=</span> {<span style="color:#48b685">&#34;device_map&#34;</span>: <span style="color:#48b685">&#34;cuda:3&#34;</span>}
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#5bc4bf">=</span> AutoTokenizer<span style="color:#5bc4bf">.</span>from_pretrained(<span style="color:#48b685">&#34;llama3/Meta-Llama-3-8B&#34;</span>, use_fast<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#5bc4bf">=</span> AutoModelForCausalLM<span style="color:#5bc4bf">.</span>from_pretrained(<span style="color:#48b685">&#34;llama3/Meta-Llama-3-8B&#34;</span>, <span style="color:#5bc4bf">**</span>kwargs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#5bc4bf">=</span> tokenizer(text, return_tensors<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>inputs <span style="color:#5bc4bf">=</span> {key: value<span style="color:#5bc4bf">.</span>to(kwargs[<span style="color:#48b685">&#34;device_map&#34;</span>]) <span style="color:#815ba4">for</span> key, value <span style="color:#5bc4bf">in</span> inputs<span style="color:#5bc4bf">.</span>items()}
</span></span><span style="display:flex;"><span>inputs <span style="color:#5bc4bf">=</span> model<span style="color:#5bc4bf">.</span>get_input_embeddings()(inputs[<span style="color:#48b685">&#34;input_ids&#34;</span>])
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([[[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1.4305e-04</span>,  <span style="color:#f99b15">1.0777e-04</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">1.9646e-04</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">2.0218e-04</span>,
</span></span><span style="display:flex;"><span>           <span style="color:#f99b15">1.4842e-05</span>,  <span style="color:#f99b15">3.0136e-04</span>],
</span></span><span style="display:flex;"><span>         [ <span style="color:#f99b15">1.7090e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">2.4261e-03</span>,  <span style="color:#f99b15">1.0452e-03</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">5.9814e-03</span>,
</span></span><span style="display:flex;"><span>           <span style="color:#f99b15">2.9602e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">2.4414e-03</span>],
</span></span><span style="display:flex;"><span>         [<span style="color:#5bc4bf">-</span><span style="color:#f99b15">2.9144e-03</span>,  <span style="color:#f99b15">1.5335e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">2.4605e-04</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">9.7046e-03</span>,
</span></span><span style="display:flex;"><span>           <span style="color:#f99b15">4.2114e-03</span>,  <span style="color:#f99b15">7.9956e-03</span>],
</span></span><span style="display:flex;"><span>         <span style="color:#5bc4bf">...</span>,
</span></span><span style="display:flex;"><span>         [ <span style="color:#f99b15">1.6968e-02</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">1.1749e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">4.4556e-03</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">7.4463e-03</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#5bc4bf">-</span><span style="color:#f99b15">1.6785e-03</span>,  <span style="color:#f99b15">1.2878e-02</span>],
</span></span><span style="display:flex;"><span>         [<span style="color:#5bc4bf">-</span><span style="color:#f99b15">9.9487e-03</span>,  <span style="color:#f99b15">2.3651e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">2.1057e-03</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">8.2397e-03</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#5bc4bf">-</span><span style="color:#f99b15">3.9978e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">3.4637e-03</span>],
</span></span><span style="display:flex;"><span>         [<span style="color:#5bc4bf">-</span><span style="color:#f99b15">8.3923e-05</span>,  <span style="color:#f99b15">1.3428e-03</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">3.6011e-03</span>,  <span style="color:#5bc4bf">...</span>,  <span style="color:#f99b15">3.7384e-03</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#5bc4bf">-</span><span style="color:#f99b15">1.3184e-02</span>, <span style="color:#5bc4bf">-</span><span style="color:#f99b15">3.7537e-03</span>]]], device<span style="color:#5bc4bf">=</span><span style="color:#48b685">&#39;cuda:3&#39;</span>,
</span></span><span style="display:flex;"><span>       grad_fn<span style="color:#5bc4bf">=&lt;</span>EmbeddingBackward0<span style="color:#5bc4bf">&gt;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># shape: [1, 10, 4096]</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 因为包含开始和结束token 所以第二个维度是10</span>
</span></span></code></pre></div><h1 id="自注意力机制关注输入的不同区域">自注意力机制关注输入的不同区域<a hidden class="anchor" aria-hidden="true" href="#自注意力机制关注输入的不同区域">#</a></h1>
<h2 id="不包含可训练权重的自注意力机制">不包含可训练权重的自注意力机制<a hidden class="anchor" aria-hidden="true" href="#不包含可训练权重的自注意力机制">#</a></h2>
<ul>
<li>按照惯例，未经归一化的注意力权重被称为<strong>“注意力分数”</strong>，经过归一化的注意力分数被称为<strong>“注意力权重”</strong></li>
<li>这一步我们的目标是获得输入序列的上下文向量</li>
<li>上下文向量相当于输入序列的变体，它不仅包含了输入序列特定元素的信息，还包含了当任务相关的所有其他输入元素的信息</li>
</ul>
<p>让我们先以第4个元素为例（即your对应的embedding），计算它对应的上下文向量，后面我们会推广这个方法来计算所有的上下文向量</p>
<ul>
<li>第一步：计算注意力分数，通过查询$x^{(4)}$与所有其他输入token之间的点积来实现：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>query <span style="color:#5bc4bf">=</span> embeddings[<span style="color:#f99b15">0</span>, <span style="color:#f99b15">3</span>, :]
</span></span><span style="display:flex;"><span>device <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 创建一个空的张量来存储注意力分数，其形状与输入序列的批次大小相同</span>
</span></span><span style="display:flex;"><span>attn_scores_4 <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>empty(embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">1</span>], device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">for</span> i, x_i <span style="color:#5bc4bf">in</span> enumerate(embeddings[<span style="color:#f99b15">0</span>]):
</span></span><span style="display:flex;"><span>    attn_scores_4[i] <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>dot(x_i, query)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(attn_scores_4)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([0.0052, 0.0204, 0.0254, 0.1827, 0.0237, 0.0168, 0.0257, 0.0229, 0.0092,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         0.0067], device=&#39;cuda:3&#39;, grad_fn=&lt;CopySlices&gt;)</span>
</span></span></code></pre></div><ul>
<li>第二步：归一化注意力分数。在实际应用中，使用softmax进行归一化更为常见</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attn_weights_4 <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores_4, dim<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#48b685">f</span><span style="color:#48b685">&#34;Attention weights: </span><span style="color:#f99b15">{</span>attn_weights_4<span style="color:#f99b15">}</span><span style="color:#48b685">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#48b685">f</span><span style="color:#48b685">&#34;Sum: </span><span style="color:#f99b15">{</span>attn_weights_4<span style="color:#5bc4bf">.</span>sum()<span style="color:#f99b15">}</span><span style="color:#48b685">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># Attention weights: tensor([0.0971, 0.0985, 0.0990, 0.1159, 0.0989, 0.0982, 0.0991, 0.0988, 0.0974,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         0.0972], device=&#39;cuda:3&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># Sum: 1.0</span>
</span></span></code></pre></div><ul>
<li>第三步：将输入序列与注意力权重相乘，并加起来，得到上下文向量</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>query <span style="color:#5bc4bf">=</span> embeddings[<span style="color:#f99b15">0</span>, <span style="color:#f99b15">3</span>, :]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context_vec_4 <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>zeros(query<span style="color:#5bc4bf">.</span>shape, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">for</span> i, x_i <span style="color:#5bc4bf">in</span> enumerate(embeddings[<span style="color:#f99b15">0</span>]):
</span></span><span style="display:flex;"><span>    context_vec_4 <span style="color:#5bc4bf">+=</span> x_i <span style="color:#5bc4bf">*</span> attn_weights_4[i]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(context_vec_4)
</span></span><span style="display:flex;"><span>print(context_vec_4<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([ 0.0001, -0.0021, -0.0020,  ...,  0.0031, -0.0035,  0.0007],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        device=&#39;cuda:3&#39;, grad_fn=&lt;AddBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([4096])</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># shape : 10 * (10, 4096)</span>
</span></span></code></pre></div><p>在上面的过程中，我们计算了第4个元素的上下文向量，接下来我们将其推广到整个输入序列</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">0</span>]
</span></span><span style="display:flex;"><span>seq_length <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">1</span>]
</span></span><span style="display:flex;"><span>emb_size <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">2</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 创建空的注意力分数矩阵</span>
</span></span><span style="display:flex;"><span>attn_scores <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>empty(b, seq_length, seq_length, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>attn_scores <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>bmm(embeddings, embeddings<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(attn_scores<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 归一化注意力分数矩阵</span>
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#48b685">f</span><span style="color:#48b685">&#34;Sum :</span><span style="color:#f99b15">{</span>attn_weights<span style="color:#5bc4bf">.</span>sum(dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)<span style="color:#f99b15">}</span><span style="color:#48b685">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 获得上下文向量</span>
</span></span><span style="display:flex;"><span>all_context_vec <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>bmm(attn_weights, embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(all_context_vec<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 10])</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># Sum :tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          1.0000]], device=&#39;cuda:3&#39;, grad_fn=&lt;SumBackward1&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 4096])</span>
</span></span></code></pre></div><h2 id="使用可训练权重实现自注意力机制">使用可训练权重实现自注意力机制<a hidden class="anchor" aria-hidden="true" href="#使用可训练权重实现自注意力机制">#</a></h2>
<ul>
<li>这种注意力机制也被称为“缩放点积注意力”</li>
<li>与上一节相同，我们需要注意力权重，但不同的在于这个注意力权重在模型训练期间可以更新，以便学习“good”上下文</li>
<li>在实现自注意力机制之前，我们首先引入三个训练权重矩阵$W_q$、$W_k$和$W_v$，这三个矩阵将embedding的特定元素向量投影到三个向量：
<ul>
<li>query：$q^{(i)} = W_q \cdot x^{(i)}$</li>
<li>key： $k^{(i)} = W_k \cdot x^{(i)}$</li>
<li>value：$v^{(i)} = W_v \cdot x^{(i)}$</li>
</ul>
</li>
<li>输入维度与输出维度可以相同也可以不同，这里我们假设它们不同</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_4 <span style="color:#5bc4bf">=</span> embeddings[<span style="color:#f99b15">0</span>, <span style="color:#f99b15">3</span>, :]
</span></span><span style="display:flex;"><span>d_in <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">2</span>] <span style="color:#8d8687"># 4096</span>
</span></span><span style="display:flex;"><span>d_out <span style="color:#5bc4bf">=</span> <span style="color:#f99b15">2048</span>
</span></span></code></pre></div><ul>
<li>下面我们将初始化三个权重矩阵；请注意，我们会设置<code>requires_grad=False</code>以减少输出的混乱，但在实际训练过程中<code>requires_grad</code>应设置为<code>True</code>，以便更新这些矩阵</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>W_q <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>nn<span style="color:#5bc4bf">.</span>Parameter(torch<span style="color:#5bc4bf">.</span>rand(b, d_in, d_out), requires_grad<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>)
</span></span><span style="display:flex;"><span>W_k <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>nn<span style="color:#5bc4bf">.</span>Parameter(torch<span style="color:#5bc4bf">.</span>rand(b, d_in, d_out), requires_grad<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>)
</span></span><span style="display:flex;"><span>W_v <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>nn<span style="color:#5bc4bf">.</span>Parameter(torch<span style="color:#5bc4bf">.</span>rand(b, d_in, d_out), requires_grad<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>)
</span></span></code></pre></div><ul>
<li>接下来分别计算三个向量</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>query_4 <span style="color:#5bc4bf">=</span> x_4 <span style="color:#5bc4bf">@</span> W_q
</span></span><span style="display:flex;"><span>key_4 <span style="color:#5bc4bf">=</span> x_4 <span style="color:#5bc4bf">@</span> W_k
</span></span><span style="display:flex;"><span>value_4 <span style="color:#5bc4bf">=</span> x_4 <span style="color:#5bc4bf">@</span> W_v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(query_4)
</span></span><span style="display:flex;"><span>print(query_4<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([0.3062, 0.2997, 0.4767,  ..., 0.4066, 0.5410, 0.4954], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;SqueezeBackward4&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([2048])</span>
</span></span></code></pre></div><ul>
<li>我们将所有token的从4096维度映射到了2048维度</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>keys <span style="color:#5bc4bf">=</span> embeddings[<span style="color:#f99b15">0</span>] <span style="color:#5bc4bf">@</span> W_k
</span></span><span style="display:flex;"><span>values <span style="color:#5bc4bf">=</span> embeddings[<span style="color:#f99b15">0</span>] <span style="color:#5bc4bf">@</span> W_v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(keys<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>print(values<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([10, 2048])</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([10, 2048])</span>
</span></span></code></pre></div><ul>
<li>接下来，我通过计算query和每个key向量之间的点积来计算注意力分数，会得到10个注意力分数</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attn_scores_4 <span style="color:#5bc4bf">=</span> query_4 <span style="color:#5bc4bf">@</span> keys<span style="color:#5bc4bf">.</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(attn_scores_4)
</span></span><span style="display:flex;"><span>print(attn_scores_4<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([ 58.8918, 166.3115,  81.8653, 339.3221,  96.8350, 457.5436,  -6.2441,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          56.3547, 162.6453, 406.4980], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;SqueezeBackward4&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([10])</span>
</span></span></code></pre></div><ul>
<li>下一步是归一化注意力分数，但与之前不同的是，之前使用softmax归一化注意力分数。我们现在通过除以key和query的纬度的平方根来缩放注意力分数
<ul>
<li>这里通过除以key的维度的平方根，来防止过大的点积，导致softmax梯度过小</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>d_k <span style="color:#5bc4bf">=</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">1</span>] <span style="color:#8d8687"># 2048</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>attn_weights_4 <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores_4 <span style="color:#5bc4bf">/</span> d_k<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(attn_weights_4)
</span></span><span style="display:flex;"><span>print(attn_weights_4<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([1.0662e-04, 1.1447e-03, 1.7713e-04, 5.2362e-02, 2.4658e-04, 7.1375e-01,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         2.5278e-05, 1.0080e-04, 1.0556e-03, 2.3104e-01], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;SoftmaxBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([10])</span>
</span></span></code></pre></div><ul>
<li>获得上下文向量</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>context_vec_4 <span style="color:#5bc4bf">=</span> attn_weights_4 <span style="color:#5bc4bf">@</span> values
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(context_vec_4)
</span></span><span style="display:flex;"><span>print(context_vec_4<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([0.5827, 0.6454, 0.4521,  ..., 0.4937, 0.3820, 0.3779], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;SqueezeBackward4&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([2048])</span>
</span></span></code></pre></div><h2 id="实现一个selfattention类">实现一个SelfAttention类<a hidden class="anchor" aria-hidden="true" href="#实现一个selfattention类">#</a></h2>
<p>最后让我们整合一下，就可以得到一个紧凑的SelfAttention类</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch.nn</span> <span style="color:#815ba4">as</span> <span style="color:#fec418">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">SelfAttention</span>(nn<span style="color:#5bc4bf">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, d_in, d_out, qkv_bias<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#5bc4bf">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>d_out <span style="color:#5bc4bf">=</span> d_out
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_query <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_key <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_value <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        keys <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_key(x)
</span></span><span style="display:flex;"><span>        queries <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_query(x)
</span></span><span style="display:flex;"><span>        values <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_value(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#5bc4bf">=</span> queries <span style="color:#5bc4bf">@</span> keys<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores <span style="color:#5bc4bf">/</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#5bc4bf">=</span> attn_weights <span style="color:#5bc4bf">@</span> values
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#815ba4">return</span> context_vec
</span></span></code></pre></div><h1 id="遮蔽下文信息的注意力机制">遮蔽下文信息的注意力机制<a hidden class="anchor" aria-hidden="true" href="#遮蔽下文信息的注意力机制">#</a></h1>
<h2 id="使用因果注意力掩码">使用因果注意力掩码<a hidden class="anchor" aria-hidden="true" href="#使用因果注意力掩码">#</a></h2>
<ul>
<li>
<p>在本节，我们将前文的自注意力机制转化为因果自注意力机制</p>
</li>
<li>
<p>核心在于：模型对序列中某个位置的预测只依赖于前面位置的已知输出，而不依赖于未来的位置</p>
</li>
<li>
<p>我们先试用上面的自注意力类来获得注意力权重。值得注意的是，我们的自注意力使用<code>nn.Linear</code>来初始化权重矩阵的，<code>nn.Linea</code>自带一种优选的权重初始化方案，这有助于实现更稳定的模型训练。</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">1024</span>)
</span></span><span style="display:flex;"><span>device <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d_in <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape[<span style="color:#f99b15">2</span>]
</span></span><span style="display:flex;"><span>d_out <span style="color:#5bc4bf">=</span> <span style="color:#f99b15">2048</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>selfattention <span style="color:#5bc4bf">=</span> SelfAttention(d_in, d_out, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>querys <span style="color:#5bc4bf">=</span> selfattention<span style="color:#5bc4bf">.</span>W_query(embeddings)
</span></span><span style="display:flex;"><span>keys <span style="color:#5bc4bf">=</span> selfattention<span style="color:#5bc4bf">.</span>W_key(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>attn_scores <span style="color:#5bc4bf">=</span> querys <span style="color:#5bc4bf">@</span> keys<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores <span style="color:#5bc4bf">/</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># print(attn_weights)</span>
</span></span><span style="display:flex;"><span>print(attn_weights<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000]]], device=&#39;cuda:3&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 10])</span>
</span></span></code></pre></div><ul>
<li>屏蔽未来的注意力权重最简单的方法是通过PyTorch的tril函数创建一个掩码，主对角线（包括对角线本身）以下的元素设置为1，主对角线以上的元素设置为0：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>block_size <span style="color:#5bc4bf">=</span> attn_scores<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]
</span></span><span style="display:flex;"><span>mask_simple <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>tril(torch<span style="color:#5bc4bf">.</span>ones(block_size, block_size))<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>print(mask_simple)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device=&#39;cuda:3&#39;)</span>
</span></span></code></pre></div><ul>
<li>然后我们将注意力权重与掩码矩阵相乘（此处是对应元素相乘），将主对角线以上的权重归零：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask_simple <span style="color:#5bc4bf">=</span> attn_weights<span style="color:#5bc4bf">*</span>mask_simple
</span></span><span style="display:flex;"><span>print(mask_simple)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000]]], device=&#39;cuda:3&#39;, grad_fn=&lt;MulBackward0&gt;)</span>
</span></span></code></pre></div><ul>
<li>然而，如果我们像这样在softmax之后进行掩码，它会破坏softmax创建的概率分布，简单来说就是我们输出的总和不在为1</li>
<li>因此，为了确保输出的值总和为1，我们对注意力分数进行掩码，用负无穷大来掩盖对角线以上的部分</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>triu(torch<span style="color:#5bc4bf">.</span>ones(block_size, block_size), diagonal<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>masked <span style="color:#5bc4bf">=</span> attn_scores<span style="color:#5bc4bf">.</span>masked_fill(mask<span style="color:#5bc4bf">.</span>bool(), <span style="color:#5bc4bf">-</span>torch<span style="color:#5bc4bf">.</span>inf)
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(masked <span style="color:#5bc4bf">/</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>print(attn_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1111, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1000, 0.1000]]], device=&#39;cuda:3&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)</span>
</span></span></code></pre></div><h2 id="通过dropout来实现额外注意力权重的掩码">通过dropout来实现额外注意力权重的掩码<a hidden class="anchor" aria-hidden="true" href="#通过dropout来实现额外注意力权重的掩码">#</a></h2>
<ul>
<li>此外，我还可以在训练阶段使用dropout来减少过拟合</li>
<li>dropout可以应用在下面多个地方：
<ul>
<li>计算注意力权重后</li>
<li>将注意力权重与values相乘后</li>
</ul>
</li>
<li>我们在此处演示更为常见的情况，在计算注意力权重之后dropout，并使用<code>p=0.2</code>，那么未被屏蔽的元素会以<code>x / (1 - p)</code>进行缩放</li>
<li>dropout只在训练阶段使用，在推理阶段是不使用的</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">1024</span>)
</span></span><span style="display:flex;"><span>dropout <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>nn<span style="color:#5bc4bf">.</span>Dropout(p<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0.2</span>)
</span></span><span style="display:flex;"><span>print(dropout(attn_weights))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.6250, 0.6250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.4167, 0.4167, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.3125, 0.3125, 0.3125, 0.3125, 0.0000, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.2083, 0.0000, 0.2083, 0.2083, 0.2083, 0.2083, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1786, 0.1786, 0.0000, 0.1786, 0.1786, 0.1786, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1563, 0.1562, 0.1563, 0.1562, 0.1563, 0.1562, 0.1563, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.1389, 0.1389, 0.1389, 0.1389, 0.1389, 0.0000, 0.0000, 0.0000,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.1389, 0.0000],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000, 0.1250, 0.1250,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           0.0000, 0.1250]]], device=&#39;cuda:3&#39;, grad_fn=&lt;NativeDropoutBackward0&gt;)</span>
</span></span></code></pre></div><h2 id="实现一个causalattention类">实现一个CausalAttention类<a hidden class="anchor" aria-hidden="true" href="#实现一个causalattention类">#</a></h2>
<p>最后，让我们来整合一下上面所有的内容，就可以得到带有dropout的因果自注意力类</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch.nn</span> <span style="color:#815ba4">as</span> <span style="color:#fec418">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">CausalAttention</span>(nn<span style="color:#5bc4bf">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, d_in, d_out, block_size, dropout, qkv_bias<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>, device<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#5bc4bf">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>d_out <span style="color:#5bc4bf">=</span> d_out
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>device <span style="color:#5bc4bf">=</span> device
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_query <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_key <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_value <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>dropout <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Dropout(dropout)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>register_buffer(<span style="color:#48b685">&#39;mask&#39;</span>, torch<span style="color:#5bc4bf">.</span>triu(torch<span style="color:#5bc4bf">.</span>ones(block_size, block_size), diagonal<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>)<span style="color:#5bc4bf">.</span>to(device))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        b, seq_length, d_in <span style="color:#5bc4bf">=</span> x<span style="color:#5bc4bf">.</span>shape
</span></span><span style="display:flex;"><span>        keys <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_key(x)
</span></span><span style="display:flex;"><span>        queries <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_query(x)
</span></span><span style="display:flex;"><span>        values <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_value(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#5bc4bf">=</span> queries <span style="color:#5bc4bf">@</span> keys<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># _ ops are in-place</span>
</span></span><span style="display:flex;"><span>        attn_scores<span style="color:#5bc4bf">.</span>masked_fill_(self<span style="color:#5bc4bf">.</span>mask<span style="color:#5bc4bf">.</span>bool()[:seq_length, :seq_length], <span style="color:#5bc4bf">-</span>torch<span style="color:#5bc4bf">.</span>inf)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores <span style="color:#5bc4bf">/</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>dropout(attn_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#5bc4bf">=</span> attn_weights <span style="color:#5bc4bf">@</span> values
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#815ba4">return</span> context_vec
</span></span></code></pre></div><p>让我们来测试一下</p>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">1024</span>)
</span></span><span style="display:flex;"><span>b, seq_length, hidden_size <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape
</span></span><span style="display:flex;"><span>device <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>causalattention <span style="color:#5bc4bf">=</span> CausalAttention(d_in<span style="color:#5bc4bf">=</span>hidden_size, d_out<span style="color:#5bc4bf">=</span>hidden_size, block_size<span style="color:#5bc4bf">=</span>seq_length, dropout<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0.2</span>, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output <span style="color:#5bc4bf">=</span> causalattention(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(output)
</span></span><span style="display:flex;"><span>print(output<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            0.0000e+00,  0.0000e+00],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-3.4140e-03, -3.2753e-03, -6.7516e-04,  ..., -2.5813e-05,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -9.6949e-04, -6.0929e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.6352e-03, -2.6163e-03, -3.5899e-04,  ..., -2.7544e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -6.0539e-04, -2.1978e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          ...,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-3.7001e-03, -5.4613e-04,  1.4258e-04,  ..., -3.8889e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -3.4689e-03, -1.3611e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.9000e-03, -1.6623e-03,  1.5887e-04,  ...,  3.9370e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -1.9343e-03,  1.0009e-04],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-1.3459e-04, -7.6909e-04, -4.2374e-05,  ...,  4.8359e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -2.5928e-03,  1.2988e-03]]], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;UnsafeViewBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 4096])</span>
</span></span></code></pre></div><h1 id="将单头拓展到多头">将单头拓展到多头<a hidden class="anchor" aria-hidden="true" href="#将单头拓展到多头">#</a></h1>
<h2 id="直接将多个单头注意力层堆积起来">直接将多个单头注意力层堆积起来<a hidden class="anchor" aria-hidden="true" href="#直接将多个单头注意力层堆积起来">#</a></h2>
<ul>
<li>我们可以简单地将多个单头注意力层堆积在一起实现多头注意力层</li>
<li>多头注意力机制的主要思想是使用不同的、已学习的权重矩阵，并行运行注意力机制。这使得模型能够在不同位置的不同表示子空间中联合关注信息</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch.nn</span> <span style="color:#815ba4">as</span> <span style="color:#fec418">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">MultiHeadAttentionWrapper</span>(nn<span style="color:#5bc4bf">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>, device<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#5bc4bf">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>heads <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>ModuleList([CausalAttention(d_in, d_out, block_size, dropout, qkv_bias, device) <span style="color:#815ba4">for</span> _ <span style="color:#5bc4bf">in</span> range(num_heads)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#815ba4">return</span> torch<span style="color:#5bc4bf">.</span>cat([head(x) <span style="color:#815ba4">for</span> head <span style="color:#5bc4bf">in</span> self<span style="color:#5bc4bf">.</span>heads], dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span></code></pre></div><ul>
<li>简单测试一下</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">1024</span>)
</span></span><span style="display:flex;"><span>b, seq_length, hidden_size <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape
</span></span><span style="display:flex;"><span>device <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mha <span style="color:#5bc4bf">=</span> MultiHeadAttentionWrapper(d_in<span style="color:#5bc4bf">=</span>hidden_size, d_out<span style="color:#5bc4bf">=</span>hidden_size, block_size<span style="color:#5bc4bf">=</span>seq_length, dropout<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0.2</span>, num_heads<span style="color:#5bc4bf">=</span><span style="color:#f99b15">12</span>, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context_vec <span style="color:#5bc4bf">=</span> mha(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(context_vec)
</span></span><span style="display:flex;"><span>print(context_vec<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.3230e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -4.0065e-03,  2.3815e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-3.4140e-03, -3.2753e-03, -6.7516e-04,  ..., -2.9023e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            9.8894e-04,  7.2249e-04],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.6352e-03, -2.6163e-03, -3.5899e-04,  ..., -1.9348e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            6.5939e-04,  4.8163e-04],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          ...,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-3.7001e-03, -5.4613e-04,  1.4258e-04,  ..., -4.5610e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            1.7907e-03, -1.7851e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.9000e-03, -1.6623e-03,  1.5887e-04,  ..., -4.9142e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            1.5937e-03, -2.5343e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-1.3459e-04, -7.6909e-04, -4.2374e-05,  ..., -5.9090e-04,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            6.2077e-04, -1.3590e-03]]], device=&#39;cuda:3&#39;, grad_fn=&lt;CatBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 49152])</span>
</span></span></code></pre></div><ul>
<li>我们使用了相同的<code>d_in</code>和<code>d_out</code>，并且使用了12个头，所以最终的维度会是<code>4096 * 12 = 49152</code></li>
</ul>
<h2 id="通过权重分割实现多头注意力">通过权重分割实现多头注意力<a hidden class="anchor" aria-hidden="true" href="#通过权重分割实现多头注意力">#</a></h2>
<ul>
<li>尽管上述多头注意力的实现是最直观且功能完整的（将CausalAttention实现封装在内），但我们也可以编写一个独立类来实现相同的功能</li>
<li>对于这个独立的MultiHeadAttention类，我们不会将单个注意力头连接在一起。相反，我们创建单个<code>W_query</code>、<code>W_key</code>和<code>W_value</code>权重矩阵，然后将它们拆分为每个注意力头的独立矩阵</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch.nn</span> <span style="color:#815ba4">as</span> <span style="color:#fec418">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#5bc4bf">import</span> <span style="color:#fec418">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">class</span> <span style="color:#fec418">MultiHeadAttention</span>(nn<span style="color:#5bc4bf">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>, device<span style="color:#5bc4bf">=</span><span style="color:#815ba4">None</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#5bc4bf">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#815ba4">assert</span> d_out <span style="color:#5bc4bf">%</span> num_heads <span style="color:#5bc4bf">==</span> <span style="color:#f99b15">0</span>, <span style="color:#48b685">&#34;d_out should be divisible by num_heads&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>d_out <span style="color:#5bc4bf">=</span> d_out
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>num_heads <span style="color:#5bc4bf">=</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>head_dim <span style="color:#5bc4bf">=</span> d_out <span style="color:#5bc4bf">//</span> num_heads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_query <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_key <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>W_value <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_in, d_out, bias<span style="color:#5bc4bf">=</span>qkv_bias)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>output_proj <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Linear(d_out, d_out)<span style="color:#5bc4bf">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>dropout <span style="color:#5bc4bf">=</span> nn<span style="color:#5bc4bf">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>        self<span style="color:#5bc4bf">.</span>register_buffer(<span style="color:#48b685">&#34;mask&#34;</span>, torch<span style="color:#5bc4bf">.</span>triu(torch<span style="color:#5bc4bf">.</span>ones(block_size, block_size), diagonal<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>)<span style="color:#5bc4bf">.</span>to(device))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">def</span> <span style="color:#06b6ef">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        b, seq_length, d_in <span style="color:#5bc4bf">=</span> x<span style="color:#5bc4bf">.</span>shape
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># shape : (b, seq_length, d_out)</span>
</span></span><span style="display:flex;"><span>        keys <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_key(x)
</span></span><span style="display:flex;"><span>        queries <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_query(x)
</span></span><span style="display:flex;"><span>        values <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>W_value(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># shape : (b, seq_length, num_heads, head_dim)</span>
</span></span><span style="display:flex;"><span>        keys <span style="color:#5bc4bf">=</span> keys<span style="color:#5bc4bf">.</span>view(b, seq_length, self<span style="color:#5bc4bf">.</span>num_heads, self<span style="color:#5bc4bf">.</span>head_dim)
</span></span><span style="display:flex;"><span>        queries <span style="color:#5bc4bf">=</span> queries<span style="color:#5bc4bf">.</span>view(b, seq_length, self<span style="color:#5bc4bf">.</span>num_heads, self<span style="color:#5bc4bf">.</span>head_dim)
</span></span><span style="display:flex;"><span>        values <span style="color:#5bc4bf">=</span> values<span style="color:#5bc4bf">.</span>view(b, seq_length, self<span style="color:#5bc4bf">.</span>num_heads, self<span style="color:#5bc4bf">.</span>head_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># shape : (b, num_heads, seq_length, head_dim)</span>
</span></span><span style="display:flex;"><span>        keys <span style="color:#5bc4bf">=</span> keys<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>        queries <span style="color:#5bc4bf">=</span> queries<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>        values <span style="color:#5bc4bf">=</span> values<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#5bc4bf">=</span> queries <span style="color:#5bc4bf">@</span> keys<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">2</span>, <span style="color:#f99b15">3</span>)
</span></span><span style="display:flex;"><span>        mask_bool <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>mask<span style="color:#5bc4bf">.</span>bool()[:seq_length, :seq_length]
</span></span><span style="display:flex;"><span>        mask_unsqueeze <span style="color:#5bc4bf">=</span> mask_bool<span style="color:#5bc4bf">.</span>unsqueeze(<span style="color:#f99b15">0</span>)<span style="color:#5bc4bf">.</span>unsqueeze(<span style="color:#f99b15">0</span>)
</span></span><span style="display:flex;"><span>        attn_scores<span style="color:#5bc4bf">.</span>masked_fill_(mask_unsqueeze, <span style="color:#5bc4bf">-</span>torch<span style="color:#5bc4bf">.</span>inf)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>softmax(attn_scores <span style="color:#5bc4bf">/</span> keys<span style="color:#5bc4bf">.</span>shape[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>]<span style="color:#5bc4bf">**</span><span style="color:#f99b15">0.5</span>, dim<span style="color:#5bc4bf">=-</span><span style="color:#f99b15">1</span>)
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>dropout(attn_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># shape : (b, seq_length, num_heads, head_dim)</span>
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#5bc4bf">=</span> (attn_weights <span style="color:#5bc4bf">@</span> values)<span style="color:#5bc4bf">.</span>transpose(<span style="color:#f99b15">1</span>, <span style="color:#f99b15">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8d8687"># view require the tensor to be contiguous</span>
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#5bc4bf">=</span> context_vec<span style="color:#5bc4bf">.</span>contiguous()<span style="color:#5bc4bf">.</span>view(b, seq_length, self<span style="color:#5bc4bf">.</span>d_out)
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#5bc4bf">=</span> self<span style="color:#5bc4bf">.</span>output_proj(context_vec)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#815ba4">return</span> context_vec
</span></span></code></pre></div><ul>
<li>测试一下</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#5bc4bf">.</span>manual_seed(<span style="color:#f99b15">1024</span>)
</span></span><span style="display:flex;"><span>b, seq_length, hidden_size <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>shape
</span></span><span style="display:flex;"><span>device <span style="color:#5bc4bf">=</span> embeddings<span style="color:#5bc4bf">.</span>device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mha <span style="color:#5bc4bf">=</span> MultiHeadAttention(d_in<span style="color:#5bc4bf">=</span>hidden_size, d_out<span style="color:#5bc4bf">=</span>hidden_size<span style="color:#5bc4bf">*</span><span style="color:#f99b15">12</span>, block_size<span style="color:#5bc4bf">=</span>seq_length, 
</span></span><span style="display:flex;"><span>                         dropout<span style="color:#5bc4bf">=</span><span style="color:#f99b15">0.2</span>, num_heads<span style="color:#5bc4bf">=</span><span style="color:#f99b15">12</span>, device<span style="color:#5bc4bf">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context_vec <span style="color:#5bc4bf">=</span> mha(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(context_vec)
</span></span><span style="display:flex;"><span>print(context_vec<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># tensor([[[-1.7976e-03, -7.1362e-03,  3.0706e-04,  ...,  5.2778e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -3.9758e-04, -3.7783e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.2841e-04, -6.1624e-03,  1.7655e-03,  ...,  4.7084e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -3.8035e-03, -4.4949e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-9.7516e-04, -5.9961e-03, -6.8417e-04,  ...,  4.7274e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -4.7388e-04, -4.6040e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          ...,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-1.9826e-03, -3.5002e-03, -9.4147e-04,  ...,  3.3272e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -8.4172e-04, -3.7638e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-1.6038e-03, -3.2562e-03, -8.1033e-04,  ...,  4.3648e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#           -2.4421e-04, -4.4432e-03],</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#          [-2.4065e-03, -3.8717e-03, -4.4748e-04,  ...,  3.1679e-03,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#            5.7510e-05, -3.3277e-03]]], device=&#39;cuda:3&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687">#        grad_fn=&lt;ViewBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># torch.Size([1, 10, 49152])</span>
</span></span></code></pre></div><ul>
<li>以上内容实际上是<code>MultiHeadAttentionWrapper</code>更高效版本</li>
<li>在上面的实现中，我们添加了一个投影层，这不是必须的，在LLM中只是一种标准惯例（最新的研究表明，它可以被移除而不会影响建模性能）</li>
<li>我们可以简单计算一下参数量</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">def</span> <span style="color:#06b6ef">count_parameters</span>(model):
</span></span><span style="display:flex;"><span>    <span style="color:#815ba4">return</span> sum(p<span style="color:#5bc4bf">.</span>numel() <span style="color:#815ba4">for</span> p <span style="color:#5bc4bf">in</span> model<span style="color:#5bc4bf">.</span>parameters() <span style="color:#815ba4">if</span> p<span style="color:#5bc4bf">.</span>requires_grad)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(count_parameters(mha))
</span></span><span style="display:flex;"><span>print(<span style="color:#f99b15">3</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">12</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">4096</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">4096</span> <span style="color:#5bc4bf">+</span> <span style="color:#f99b15">4096</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">12</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">12</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">4096</span> <span style="color:#5bc4bf">+</span> <span style="color:#f99b15">4096</span><span style="color:#5bc4bf">*</span><span style="color:#f99b15">12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 3019948032</span>
</span></span><span style="display:flex;"><span><span style="color:#8d8687"># 3019948032</span>
</span></span></code></pre></div><ul>
<li>第一项是三个权重矩阵的参数（<code>d_in</code> $\cdot$ <code>d_out</code>），第二项是投影层的权重（<code>d_out</code> $\cdot$ <code>d_out</code>），第三项是投影层的bias（<code>d_out</code>）</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/vision-language-models/">Vision-Language-Models</a></li>
      <li><a href="http://localhost:1313/tags/llava/">LLaVA</a></li>
      <li><a href="http://localhost:1313/tags/llama/">LLama</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">JiaHe&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Diffusion Models | JiaHe&#39;s Blog</title>
<meta name="keywords" content="generative-model, image-generation, VAE">
<meta name="description" content="什么是Generative Model?
我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。
在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。
AE &amp; VAE
我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。

Autoencoder.(Image source:Lilian Weng)
这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\phi}(x)$，重构后的输出是$x&rsquo; = f_{\theta}(g_{\phi}(x))$。参数$(\theta, \phi)$的学习目标是输入与输出尽可能相似，即$x \approx x&rsquo;$，这个损失函数我们也称之为重构损失。
Variational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\theta}$，参数为$\theta$，于是我们可以有如下定义：

先验概率$p_{\theta}(z)$
似然性（likelihood）$p_{\theta}(x | z)$
后验概率$p_{\theta}(z | x)$

假设我们知道这个分布的真实参数$\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：

先从已知的分布$p_{\theta^{*}}(z)$中采样$z^{(i)}$
然后从条件分布$p_{\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$

那么这个分布我们似乎就可以写出来了:
pθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz">
<meta name="author" content="">
<link rel="canonical" href="https://attract666.github.io/posts/ddpm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://attract666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://attract666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://attract666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://attract666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://attract666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://attract666.github.io/posts/ddpm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js"]
                }
            },
            "HTML-CSS": {
                availableFonts: ["Arial", "TeX"],
                preferredFont: "TeX",
                webFont: "TeX"
            }
        });

        MathJax.Hub.Queue(function () {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for (i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>

<meta property="og:title" content="Diffusion Models" />
<meta property="og:description" content="什么是Generative Model?
我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。
在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。
AE &amp; VAE
我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。

Autoencoder.(Image source:Lilian Weng)
这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\phi}(x)$，重构后的输出是$x&rsquo; = f_{\theta}(g_{\phi}(x))$。参数$(\theta, \phi)$的学习目标是输入与输出尽可能相似，即$x \approx x&rsquo;$，这个损失函数我们也称之为重构损失。
Variational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\theta}$，参数为$\theta$，于是我们可以有如下定义：

先验概率$p_{\theta}(z)$
似然性（likelihood）$p_{\theta}(x | z)$
后验概率$p_{\theta}(z | x)$

假设我们知道这个分布的真实参数$\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：

先从已知的分布$p_{\theta^{*}}(z)$中采样$z^{(i)}$
然后从条件分布$p_{\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$

那么这个分布我们似乎就可以写出来了:
pθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://attract666.github.io/posts/ddpm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-09-09T15:58:57+08:00" />
<meta property="article:modified_time" content="2024-09-09T15:58:57+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Diffusion Models"/>
<meta name="twitter:description" content="什么是Generative Model?
我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。
在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。
AE &amp; VAE
我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。

Autoencoder.(Image source:Lilian Weng)
这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\phi}(x)$，重构后的输出是$x&rsquo; = f_{\theta}(g_{\phi}(x))$。参数$(\theta, \phi)$的学习目标是输入与输出尽可能相似，即$x \approx x&rsquo;$，这个损失函数我们也称之为重构损失。
Variational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\theta}$，参数为$\theta$，于是我们可以有如下定义：

先验概率$p_{\theta}(z)$
似然性（likelihood）$p_{\theta}(x | z)$
后验概率$p_{\theta}(z | x)$

假设我们知道这个分布的真实参数$\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：

先从已知的分布$p_{\theta^{*}}(z)$中采样$z^{(i)}$
然后从条件分布$p_{\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$

那么这个分布我们似乎就可以写出来了:
pθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://attract666.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Diffusion Models",
      "item": "https://attract666.github.io/posts/ddpm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Diffusion Models",
  "name": "Diffusion Models",
  "description": "什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性（likelihood）$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n",
  "keywords": [
    "generative-model", "image-generation", "VAE"
  ],
  "articleBody": "什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026 VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x’ = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x’$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性（likelihood）$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\\theta^{*}$，即：\nθ∗=argmaxθ∏i=1npθ(x(i))=argmaxθ∑i=1nlog⁡pθ(x(i))\n但是，直接计算似然性$p_{\\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\\theta}(x)$可以依靠概率的链式法则$p_{\\theta}(x) = \\frac{p_{\\theta}(x | z)p_{\\theta}(z)}{p_{\\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为Evidence Lower Bound(ELBO)。因此，我们可以将最大化ELBO作为优化潜变量模型的代理目标，即：\nlog⁡pθ(x)≥Eqϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n此处的$q_{\\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\\theta}(z|x)$。现在，让我们尝试更深入地探讨为什么ELBO是我们希望最大化的目标。我们在这里给出第一种证明方法：\nlog⁡pθ(x)=log⁡∫pθ(x|z)pθ(z)dz=log⁡∫pθ(x|z)pθ(z)qϕ(z|x)qϕ(z|x)dz=log⁡Ex∼qϕ(z|x)[pθ(x|z)pθ(z)qϕ(z|x)]≥Ex∼qϕ(z|x)[pθ(x|z)pθ(z)qϕ(z|x)]\n在上面的推导中，我们直接使用琴生不等式。然而，这并没有为我们提供太多有用的信息，仅知道ELBO是数据的下界并不能真正告诉我们为什么要最大化它作为目标，为此我们给出另一种证明，在证明开始之前，我想先补充一点关于KL散度的知识。\nKL散度$D_{KL}(P|Q)$用来衡量使用分布$Q$表示分布$P$会丢失多少信息，一般分布$P$是数据的真实分布，分布$Q$是数据的理论分布、估计模型的分布或是$X$的近似分布。KL散度仅当概率分布$P$和$Q$各自总和均为1，且对于任何x均满足$P(X) \u003e 0$及$Q(x) \u003e 0$才有定义。对于连续随机变量，其概率分布$P$和$Q$的KL散度可按积分方式定义为：\nDKL(P|Q)=∫p(x)ln⁡p(x)q(x)dx=Ex∼p(x)[ln⁡p(x)q(x)]\n接下来我们进行证明：\nlog⁡pθ(x)=log⁡pθ(x)∫qϕ(z|x)dz=∫qϕ(z|x)(log⁡pθ(x))dz=Ex∼qϕ(z|x)[log⁡pθ(x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)pθ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)pθ(z|x)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]+Ex∼qϕ(z|x)[log⁡qϕ(z|x)pθ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]+DKL(qϕ(z|x)||pθ(z|x))≥Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n从这个推导中，我们可以清楚看到证据等于ELBO加上近似后验概率和真实后验概率的KL散度。事实上，正是这个KL散度被琴生不等式神奇地删除了，尽管这个KL散度似乎与我们上面的定义有所不同，我们先将它放在一边。理解这个不仅是理解ELBO与证据之间的关键，更是理解为什么优化ELBO是一个合适的目标的关键。\n首先，我们明确ELBO确实是一个下界，因为证据和ELBO之间的差异是严格非负项。其次，我们想要优化近似后验$q_{\\phi}(z|x)$的参数，以精确匹配真实后验$p_{\\theta}(z|x)$，这是通过KL散度来实现的。但正如我们前面所说我们无法获得真实后验$p_{\\theta}(z|x)$。然而注意到推导式中，证据$\\log p_{\\theta}(x)$对于参数$\\phi$始终是一个常数。由于ELBO和KL散度之和是一个常数，因此ELBO对于$\\phi$的最大化必然导致KL散度的最小化。因此，最大化ELBO可以作为学习如何完美建模真实潜在后验分布的代理。\n对于推导过程中出现的$D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z|x))$（逆KL散度）而不是$D_{KL}(p_{\\theta}||q_{\\phi})$（前向KL散度），在Eric Jang关于贝叶斯变分法的文章中给出了很好的解释。简单来说就是，前向KL散度更在意真实分布中的常见事件，而逆KL散度更在意真实分布中的罕见事件，如下图所示：\nForward and reversed KL divergence have different demands on how to match two distributions.(Image source:Eric Jang) 让我们进一步剖析ELBO：\nEx∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)]+Ex∼qϕ(z|x)[log⁡pθ(z)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)]−DKL(qϕ(z|x)||pθ(z))\n整理一下：\nlog⁡pθ(x)−DKL(qϕ(z|x)||pθ(z|x))=Ex∼qϕ(z|x)[log⁡pθ(x|z)]−DKL(qϕ(z|x)||pθ(z))\n上式的否定定义就是我们损失函数的定义：\nLVAE(θ,ϕ)=−Ex∼qϕ(z|x)[log⁡pθ(x|z)]+DKL(qϕ(z|x)||pθ(z))θ∗,ϕ∗=argminminθ,ϕ⁡LVAE\n我们现在试图用图形来绘制上述过程:\nThe graphical model involved in VAE.(Image source:Lilian Weng) 条件概率$p_{\\theta}(x|z)$定义了一个生成模型，类似于AE中的解码器 近似后验概率$q_{\\phi}(z|x)$是概率编码器，类似于AE中的编码器 损失函数中的重构项需要$z \\sim q_{\\phi}(z|x)$。然而采样是一个随机过程，因此我们无法反向传播梯度。VAE的一个决定性特征就是如何对参数$\\theta$和$\\phi$同时优化。VAE编码器通常选择对具有对角协方差的多元高斯进行建模，并且先验通常选择标准多元高斯：\nz∼qϕ(z|x)=N(z;μϕ(x),σϕ2(x)I)z=μ+σ⊙ϵ,whereϵ∼N(0,I)\n其中，$\\odot$表示逐元素乘积。换句话说，任意高斯分布都可以解释为标准高斯分布。在这个重参数版本下，可以根据需要计算$\\phi$的梯度，以优化$\\mu_{\\phi}$和$\\sigma_{\\phi}$。\nIllustration of variational autoencoder model with the multivariate Gaussian assumption.(Image source:Lilian Weng) HVAE 什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) 参考 [1] Blog:Lilian-VAE(https://lilianweng.github.io/posts/2018-08-12-vae/)\n[2] Blog:evjang-Variational bayes(https://blog.evjang.com/2016/08/variational-bayes.html)\n[3] Paper:Denoising Diffusion Probabilistic Models(https://arxiv.org/abs/2006.11239)\n[4] Paper:Understanding Diffusion Models: A Unified Perspective(https://arxiv.org/abs/2208.11970)\n[5] Wiki:KL散度(https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)\n",
  "wordCount" : "158",
  "inLanguage": "zh",
  "datePublished": "2024-09-09T15:58:57+08:00",
  "dateModified": "2024-09-09T15:58:57+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://attract666.github.io/posts/ddpm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JiaHe's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://attract666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://attract666.github.io/" accesskey="h" title="JiaHe&#39;s Blog (Alt + H)">JiaHe&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://attract666.github.io/posts/" title="All posts">
                    <span>All posts</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://attract666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Diffusion Models
    </h1>
    <div class="post-meta"><span title='2024-09-09 15:58:57 +0800 CST'>九月 9, 2024</span>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e4%bb%80%e4%b9%88%e6%98%afgenerative-model" aria-label="什么是Generative Model?">什么是Generative Model?</a><ul>
                            
                    <li>
                        <a href="#ae--vae" aria-label="AE &amp; VAE">AE &amp; VAE</a></li>
                    <li>
                        <a href="#hvae" aria-label="HVAE">HVAE</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%bb%80%e4%b9%88%e6%98%afdiffusion-models" aria-label="什么是Diffusion Models？">什么是Diffusion Models？</a><ul>
                            
                    <li>
                        <a href="#training" aria-label="Training">Training</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h1 id="什么是generative-model">什么是Generative Model?<a hidden class="anchor" aria-hidden="true" href="#什么是generative-model">#</a></h1>
<p>我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。</p>
<p>在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。</p>
<h2 id="ae--vae">AE &amp; VAE<a hidden class="anchor" aria-hidden="true" href="#ae--vae">#</a></h2>
<p>我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。</p>
<center><img src="/img/DDPM/AE.png" width="70%" title=""></center>
<center>Autoencoder.(Image source:<a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lilian Weng</a>)</center><br/>
<p>这就是<strong>Autoencoder(AE)</strong>的想法，<strong>AE</strong>包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\phi}(x)$，重构后的输出是$x&rsquo; = f_{\theta}(g_{\phi}(x))$。参数$(\theta, \phi)$的学习目标是输入与输出尽可能相似，即$x \approx x&rsquo;$，这个损失函数我们也称之为重构损失。</p>
<p><strong>Variational Autoencoder（VAE）</strong>的思想实际上与<strong>AE</strong>不太相似。在<strong>VAE</strong>中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\theta}$，参数为$\theta$，于是我们可以有如下定义：</p>
<ul>
<li>先验概率$p_{\theta}(z)$</li>
<li>似然性（likelihood）$p_{\theta}(x | z)$</li>
<li>后验概率$p_{\theta}(z | x)$</li>
</ul>
<p>假设我们知道这个分布的真实参数$\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：</p>
<ol>
<li>先从已知的分布$p_{\theta^{*}}(z)$中采样$z^{(i)}$</li>
<li>然后从条件分布$p_{\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$</li>
</ol>
<p>那么这个分布我们似乎就可以写出来了:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>p</mi><mrow><msup><mi>θ</mi><mrow><mo>∗</mo></mrow></msup></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mo data-mjx-texclass="OP">∫</mo><msub><mi>p</mi><mrow><msup><mi>θ</mi><mrow><mo>∗</mo></mrow></msup></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><msup><mi>θ</mi><mrow><mo>∗</mo></mrow></msup></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mi>d</mi><mi>z</mi></math></p>
<p>为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\theta^{*}$，即：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>θ</mi><mrow><mo>∗</mo></mrow></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>θ</mi></munder><munderover><mo data-mjx-texclass="OP">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></munderover><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>θ</mi></munder><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></munderover><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></math></p>
<p>但是，直接计算似然性$p_{\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\theta}(x)$可以依靠概率的链式法则$p_{\theta}(x) = \frac{p_{\theta}(x | z)p_{\theta}(z)}{p_{\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为<strong>Evidence Lower Bound(ELBO)</strong>。因此，我们可以将最大化<strong>ELBO</strong>作为优化潜变量模型的代理目标，即：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≥</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></math></p>
<p>此处的$q_{\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\theta}(z|x)$。现在，让我们尝试更深入地探讨为什么<strong>ELBO</strong>是我们希望最大化的目标。我们在这里给出第一种证明方法：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" rowspacing="3pt"><mtr><mtd><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mtd><mtd><mi></mi><mo>=</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mo data-mjx-texclass="OP">∫</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mi>d</mi><mi>z</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mo data-mjx-texclass="OP">∫</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mi>d</mi><mi>z</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>≥</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr></mtable></math></p>
<p>在上面的推导中，我们直接使用琴生不等式。然而，这并没有为我们提供太多有用的信息，仅知道<strong>ELBO</strong>是数据的下界并不能真正告诉我们为什么要最大化它作为目标，为此我们给出另一种证明，在证明开始之前，我想先补充一点关于KL散度的知识。</p>
<p>KL散度$D_{KL}(P|Q)$用来衡量使用分布$Q$表示分布$P$会丢失多少信息，一般分布$P$是数据的真实分布，分布$Q$是数据的理论分布、估计模型的分布或是$X$的近似分布。KL散度仅当概率分布$P$和$Q$各自总和均为1，且对于任何x均满足$P(X) &gt; 0$及$Q(x) &gt; 0$才有定义。对于连续随机变量，其概率分布$P$和$Q$的KL散度可按积分方式定义为：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mrow><mo stretchy="false">|</mo></mrow><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mo data-mjx-texclass="OP">∫</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>ln</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mi>d</mi><mi>x</mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>ln</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></math></p>
<p>接下来我们进行证明：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" rowspacing="3pt"><mtr><mtd><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mtd><mtd><mi></mi><mo>=</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo data-mjx-texclass="OP">∫</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mi>d</mi><mi>z</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mo data-mjx-texclass="OP">∫</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>d</mi><mi>z</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo><mo>+</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo><mo>+</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mrow><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>≥</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr></mtable></math></p>
<p>从这个推导中，我们可以清楚看到证据等于<strong>ELBO</strong>加上近似后验概率和真实后验概率的KL散度。事实上，正是这个KL散度被琴生不等式神奇地删除了，尽管这个KL散度似乎与我们上面的定义有所不同，我们先将它放在一边。理解这个不仅是理解<strong>ELBO</strong>与证据之间的关键，更是理解为什么优化<strong>ELBO</strong>是一个合适的目标的关键。</p>
<p>首先，我们明确<strong>ELBO</strong>确实是一个下界，因为证据和<strong>ELBO</strong>之间的差异是严格非负项。其次，我们想要优化近似后验$q_{\phi}(z|x)$的参数，以精确匹配真实后验$p_{\theta}(z|x)$，这是通过KL散度来实现的。但正如我们前面所说我们无法获得真实后验$p_{\theta}(z|x)$。然而注意到推导式中，证据$\log p_{\theta}(x)$对于参数$\phi$始终是一个常数。由于<strong>ELBO</strong>和KL散度之和是一个常数，因此<strong>ELBO</strong>对于$\phi$的最大化必然导致KL散度的最小化。因此，最大化<strong>ELBO</strong>可以作为学习如何完美建模真实潜在后验分布的代理。</p>
<p>对于推导过程中出现的$D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$（逆KL散度）而不是$D_{KL}(p_{\theta}||q_{\phi})$（前向KL散度），在Eric Jang关于<a href="https://blog.evjang.com/2016/08/variational-bayes.html">贝叶斯变分法</a>的文章中给出了很好的解释。简单来说就是，前向KL散度更在意真实分布中的常见事件，而逆KL散度更在意真实分布中的罕见事件，如下图所示：</p>
<center><img src="/img/DDPM/KL.png" width="80%" titile="" /></center>
<center> Forward and reversed KL divergence have different demands on how to match two distributions.(Image source:<a href="https://blog.evjang.com/2016/08/variational-bayes.html">Eric Jang</a>)</center><br/>
<p>让我们进一步剖析<strong>ELBO</strong>：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" rowspacing="3pt"><mtr><mtd><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mrow><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr></mtable></math></p>
<p>整理一下：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" rowspacing="3pt"><mtr><mtd><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mrow><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mrow><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr></mtable></math></p>
<p>上式的否定定义就是我们损失函数的定义：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" rowspacing="3pt"><mtr><mtd><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow><mrow><mi>V</mi><mi>A</mi><mi>E</mi></mrow></msub><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mtd><mtd><mi></mi><mo>=</mo><mo>−</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mi>x</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mrow><mo stretchy="false">|</mo></mrow><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mrow><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi>θ</mi><mrow><mo>∗</mo></mrow></msup><mo>,</mo><msup><mi>ϕ</mi><mrow><mo>∗</mo></mrow></msup></mtd><mtd><mi></mi><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mrow data-mjx-texclass="OP"><mrow data-mjx-texclass="OP"><mrow><mpadded width="0"><mrow><mphantom><mo data-mjx-texclass="OP">min</mo></mphantom></mrow></mpadded></mrow></mrow><mstyle scriptlevel="0"><mspace width="negativethinmathspace"></mspace></mstyle><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP">min</mo></mrow></mrow><mrow><mi>θ</mi><mo>,</mo><mi>ϕ</mi></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow><mrow><mi>V</mi><mi>A</mi><mi>E</mi></mrow></msub></mtd></mtr></mtable></math></p>
<p>我们现在试图用图形来绘制上述过程:</p>
<center><img src="/img/DDPM/VAE-p.png" width="80%" titile="" /></center>
<center>The graphical model involved in VAE.(Image source:<a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lilian Weng</a>)</center><br/>
<ul>
<li>条件概率$p_{\theta}(x|z)$定义了一个生成模型，类似于<strong>AE</strong>中的解码器</li>
<li>近似后验概率$q_{\phi}(z|x)$是概率编码器，类似于<strong>AE</strong>中的编码器</li>
</ul>
<p>损失函数中的重构项需要$z \sim q_{\phi}(z|x)$。然而采样是一个随机过程，因此我们无法反向传播梯度。<strong>VAE</strong>的一个决定性特征就是如何对参数$\theta$和$\phi$同时优化。VAE编码器通常选择对具有对角协方差的多元高斯进行建模，并且先验通常选择标准多元高斯：</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnspacing="1em" rowspacing="4pt"><mtr><mtd><mi>z</mi><mo>∼</mo><msub><mi>q</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi></mrow><mo stretchy="false">(</mo><mi>z</mi><mo>;</mo><msub><mi>μ</mi><mrow><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>,</mo><msubsup><mi>σ</mi><mrow><mi>ϕ</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>I</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>z</mi><mo>=</mo><mi>μ</mi><mo>+</mo><mi>σ</mi><mo>⊙</mo><mi>ϵ</mi><mo>,</mo><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>ϵ</mi><mo>∼</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi></mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></math></p>
<p>其中，$\odot$表示逐元素乘积。换句话说，任意高斯分布都可以解释为标准高斯分布。在这个重参数版本下，可以根据需要计算$\phi$的梯度，以优化$\mu_{\phi}$和$\sigma_{\phi}$。</p>
<center><img src="/img/DDPM/VAE.png" width="80%" titile="" /></center>
<center>Illustration of variational autoencoder model with the multivariate Gaussian assumption.(Image source:<a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lilian Weng</a>)</center><br/>
<h2 id="hvae">HVAE<a hidden class="anchor" aria-hidden="true" href="#hvae">#</a></h2>
<hr>
<h1 id="什么是diffusion-models">什么是Diffusion Models？<a hidden class="anchor" aria-hidden="true" href="#什么是diffusion-models">#</a></h1>
<p>Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。</p>
<center><img src="/img/DDPM/diffusion.png" width="80%" titile="" /></center>
<center>Diffusion Process.(Image source:<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf">Hung-yi Lee</a>)</center><br/>
<p>根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在<em>denoising diffusion probabilistic models</em>（<strong>DDPM;</strong><a href="https://arxiv.org/abs/2006.11239">Ho et al.2020</a>）中的算法并不这么简单。</p>
<h2 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h2>
<center><img src="/img/DDPM/training.png" width="70%" title=""></center>
<center>Training.(Image source:<a href="https://arxiv.org/abs/2006.11239">DDPM</a>)</center><br/>
<hr>
<h1 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h1>
<p>[1] Blog:Lilian-VAE(<a href="https://lilianweng.github.io/posts/2018-08-12-vae/">https://lilianweng.github.io/posts/2018-08-12-vae/</a>)</p>
<p>[2] Blog:evjang-Variational bayes(<a href="https://blog.evjang.com/2016/08/variational-bayes.html">https://blog.evjang.com/2016/08/variational-bayes.html</a>)</p>
<p>[3] Paper:Denoising Diffusion Probabilistic Models(<a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a>)</p>
<p>[4] Paper:Understanding Diffusion Models: A Unified Perspective(<a href="https://arxiv.org/abs/2208.11970">https://arxiv.org/abs/2208.11970</a>)</p>
<p>[5] Wiki:KL散度(<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions</a>)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://attract666.github.io/tags/generative-model/">Generative-Model</a></li>
      <li><a href="https://attract666.github.io/tags/image-generation/">Image-Generation</a></li>
      <li><a href="https://attract666.github.io/tags/vae/">VAE</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://attract666.github.io/">JiaHe&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

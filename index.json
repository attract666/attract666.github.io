[{"content":"什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$\n什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) ","permalink":"https://attract666.github.io/posts/ddpm/","summary":"\u003ch1 id=\"什么是generative-model\"\u003e什么是Generative Model?\u003c/h1\u003e\n\u003cp\u003e我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\u003c/p\u003e\n\u003cp\u003e在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\u003c/p\u003e\n\u003ch2 id=\"ae--vae\"\u003eAE \u0026amp; VAE\u003c/h2\u003e\n\u003cp\u003e我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/AE.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eAutoencoder.(Image source:\u003ca href=\"https://lilianweng.github.io/posts/2018-08-12-vae/\"\u003eLilian Weng\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e这就是\u003cstrong\u003eAutoencoder(AE)\u003c/strong\u003e的想法，\u003cstrong\u003eAE\u003c/strong\u003e包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariational Autoencoder（VAE）\u003c/strong\u003e的思想实际上与\u003cstrong\u003eAE\u003c/strong\u003e不太相似。在\u003cstrong\u003eVAE\u003c/strong\u003e中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$\u003c/p\u003e\n\u003ch1 id=\"什么是diffusion-models\"\u003e什么是Diffusion Models？\u003c/h1\u003e\n\u003cp\u003eDiffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/diffusion.png\" width=\"80%\" titile=\"\" /\u003e\u003c/center\u003e\n\u003ccenter\u003eDiffusion Process.(Image source:\u003ca href=\"https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf\"\u003eHung-yi Lee\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在\u003cem\u003edenoising diffusion probabilistic models\u003c/em\u003e（\u003cstrong\u003eDDPM;\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2006.11239\"\u003eHo et al.2020\u003c/a\u003e）中的算法并不这么简单。\u003c/p\u003e\n\u003ch2 id=\"training\"\u003eTraining\u003c/h2\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/training.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eTraining.(Image source:\u003ca href=\"https://arxiv.org/abs/2006.11239\"\u003eDDPM\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e","title":"Diffusion Models"},{"content":"","permalink":"https://attract666.github.io/about/","summary":"about","title":"关于"}]
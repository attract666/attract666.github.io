[{"content":"从LLaVA-1.0论文开始 LLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\nPaper link: https://arxiv.org/abs/2304.08485\nGithub: https://github.com/haotian-liu/LLaVA\n论文亮点 多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类 多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space 网络架构 结构上vision encoder使用的是CLIP-L/14（使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征）进行编码，之后使用一个投影层（全连接层）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\nLLaVA Architecture.(Image source:LLaVA)\r数据集构成及数据收集细节 训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\n预训练数据集（CC3M过滤）：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\nCC3M Filtered.(Image source:LLaVA)\r将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\nCC3M image description.(Image source:LLaVA)\r指令格式如下：\nCC3M instructions.(Image source:LLaVA)\r指令微调数据集：使用COCO数据集，将图像描述、bbox、类别等信息输入给GPT4-text-only生成对话、细节描述和复杂推理三种类型instruction-following数据。最终生成58K对话、23K细节描述和77K复杂推理。\n对话：对图像内容进行多轮回答，且只考虑有明确答案的问题，通过这样的方式保证答案的质量 细节描述：使用另外一些描述的提示，随机让GPT4-text-only生成详细描述 复杂推理：上面都是视觉本身内容，设计了一些需要深度推理的问题，答案要有严格的逻辑 训练方式 训练的目标就是最大化似然概率\nTrain traget.(Image source:LLaVA)\r其中\nInstruction.(Image source:LLaVA)\r在计算自回归模型的loss时，只考虑回答$X_a$和\u0026lt;STOP\u0026gt;的tokens\nInput sequence.(Image source:LLaVA)\r推理 在我们阅读完LLaVA论文后，想要获得自己的LLaVA，第一件要做的事很自然是先跑通LLaVA的推理脚本。这里我考虑的是直接跑LLaVA仓库的代码，加载官方给出的模型，而不是huggingface集成好的模型。推理的入口函数是eval_model()。\n它的实现如下：\n# llava/eval/run_llava.py def eval_model(args): # Model # 将linear层和layernorm层的reset_parameters方法用空lambda函数代替，达到禁用pytorch默认初始化 disable_torch_init() # 初始化模型 model_name = get_model_name_from_path(args.model_path) tokenizer, model, image_processor, context_len = load_pretrained_model( args.model_path, args.model_base, model_name ) # 数据预处理 qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs if \u0026#34;llama-2\u0026#34; in model_name.lower(): conv_mode = \u0026#34;llava_llama_2\u0026#34; ... else: conv_mode = \u0026#34;llava_v0\u0026#34; if args.conv_mode is not None and conv_mode != args.conv_mode: print( \u0026#34;[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\u0026#34;.format( conv_mode, args.conv_mode, args.conv_mode ) ) else: args.conv_mode = conv_mode conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) # 推理 with torch.inference_mode(): output_ids = model.generate( input_ids, images=images_tensor, image_sizes=image_sizes, do_sample=True if args.temperature \u0026gt; 0 else False, temperature=args.temperature, top_p=args.top_p, num_beams=args.num_beams, max_new_tokens=args.max_new_tokens, use_cache=True, ) # 解码 outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() print(outputs) 整块函数的实现框架是：\n初始化模型：通过disable_torch_init()来加速模型加载 数据预处理：拼接caption和image token 推理：接受input_ids和images作为输入，输出output_ids 解码 之后具体来看看各个部分的实现\n初始化模型 初始化模型主要有两个函数get_model_name_from_path()和load_pretrained_model()。\nget_model_name_from_path()的实现比较简单，就是把模型路径名去除首尾\u0026quot;/\u0026ldquo;后，在将字符串按\u0026rdquo;/\u0026ldquo;分割成一个列表，之后根据列表最后一个元素是否已\u0026quot;checkpoint-\u0026ldquo;开头，返回model_name。\nload_pretrained_model()的实现则比较长，但并不复杂。\n# llava/model/builder.py def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\u0026#34;auto\u0026#34;, device=\u0026#34;cuda\u0026#34;, use_flash_attn=False, **kwargs): ... # 前面主要是一些关于device、量化参数和flash_attention的设置 # Load LLaVA model if \u0026#39;llava\u0026#39; in model.name.lower(): if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is None: # model_name要包含lora，同时提供model_base ... if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is not None: # 使用LlavaConfig从model_path中加载lora配置 ... # 通过参数model_base加载tokenizer tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) # 从base model中加载LLaVA，确保 token_num, token_dim = lm_head.out_features, lm_head.in_features model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs) ... # 加载LLaVA额外权重，检查是否存在non_lora_trainables文件，没有就假定从HF hub下载 ... # 加载lora权重 model = PeftModel.from_pretrained(model, model_path) # 合并lora权重 model = model.merge_and_unload() elif model_base is not None: ... else: if \u0026#39;mpt\u0026#39; in model_name.lower(): ... elif \u0026#39;mistral\u0026#39; in model_name.lower(): ... else: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) model = LlavaLlamaForCausalLM.from_pretrained( model_path, low_cpu_mem_usage=True, **kwargs ) else: # Load language model ... image_processor = None if \u0026#39;llava\u0026#39; in model_name.lower(): # 从模型的config中获取start和end token，如果不存在则默认为false。获取patch token，不存在默认为True mm_use_im_start_end = getattr(model.config, \u0026#34;mm_use_im_start_end\u0026#34;, False) mm_use_im_patch_token = getattr(model.config, \u0026#34;mm_use_im_patch_token\u0026#34;, True) if mm_use_im_patch_token: tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True) if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True) model.resize_token_embeddings(len(tokenizer)) # 加载vision tower vision_tower = model.get_vision_tower() if not vision_tower.is_loaded: vision_tower.load_model(device_map=device_map) if device_map != \u0026#39;auto\u0026#39;: vision_tower.to(device=device_map, dtype=torch.float16) image_processor = vision_tower.image_processor # 检查最大输入序列长度 if hasattr(model.config, \u0026#34;max_sequence_length\u0026#34;): context_len = model.config.max_sequence_length else: context_len = 2048 return tokenizer, model, image_processor, context_len load_pretrained_model()的实现逻辑是：\n初始化tokenizer、`model`` 从model中加载vision_tower的image_processor 数据预处理 接下来是，是数据预处理的实现\n# llava/eval/run_llava.py def eval_model(args): ... # 加载query和定义image token qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN # 如果query中存在图片占位符 if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs # 获取对话模板并拼接成prompt if \u0026#34;llama-2\u0026#34; in model_name.lower(): ... conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() # 加载图片并处理图片 image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) # tokenize prompt input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) 我们可以简单画一张图来表示数据预处理流程：\n","permalink":"https://attract666.github.io/posts/llava_01/","summary":"\u003ch1 id=\"从llava-10论文开始\"\u003e从LLaVA-1.0论文开始\u003c/h1\u003e\n\u003cp\u003eLLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\u003c/p\u003e\n\u003cp\u003ePaper link: \u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003e\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003ehttps://arxiv.org/abs/2304.08485\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGithub: \u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003e\u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003ehttps://github.com/haotian-liu/LLaVA\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"论文亮点\"\u003e论文亮点\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类\u003c/li\u003e\n\u003cli\u003e多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"网络架构\"\u003e网络架构\u003c/h2\u003e\n\u003cp\u003e结构上vision encoder使用的是CLIP-L/14（\u003cstrong\u003e使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征\u003c/strong\u003e）进行编码，之后使用一个投影层（\u003cstrong\u003e全连接层\u003c/strong\u003e）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/LLaVA.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eLLaVA Architecture.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003ch2 id=\"数据集构成及数据收集细节\"\u003e数据集构成及数据收集细节\u003c/h2\u003e\n\u003cp\u003e训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e预训练数据集（CC3M过滤）\u003c/strong\u003e：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_filtered.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M Filtered.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_prompt.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M image description.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e指令格式如下：\u003c/p\u003e","title":"从零开始学LLaVA 01"},{"content":"什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\\theta^{*}$，即：\nθ∗=argmaxθ∏i=1npθ(x(i))=argmaxθ∑i=1nlog⁡pθ(x(i))\n但是，直接计算似然性$p_{\\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\\theta}(x)$可以依靠概率的链式法则$p_{\\theta}(x) = \\frac{p_{\\theta}(x | z)p_{\\theta}(z)}{p_{\\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为Evidence Lower Bound(ELBO)。因此，我们可以将最大化ELBO作为优化潜变量模型的代理目标，即：\nlog⁡pθ(x)≥Eqϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n此处的$q_{\\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\\theta}(z|x)$\n什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) ","permalink":"https://attract666.github.io/posts/ddpm/","summary":"\u003ch1 id=\"什么是generative-model\"\u003e什么是Generative Model?\u003c/h1\u003e\n\u003cp\u003e我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\u003c/p\u003e\n\u003cp\u003e在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\u003c/p\u003e\n\u003ch2 id=\"ae--vae\"\u003eAE \u0026amp; VAE\u003c/h2\u003e\n\u003cp\u003e我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/AE.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eAutoencoder.(Image source:\u003ca href=\"https://lilianweng.github.io/posts/2018-08-12-vae/\"\u003eLilian Weng\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e这就是\u003cstrong\u003eAutoencoder(AE)\u003c/strong\u003e的想法，\u003cstrong\u003eAE\u003c/strong\u003e包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariational Autoencoder（VAE）\u003c/strong\u003e的思想实际上与\u003cstrong\u003eAE\u003c/strong\u003e不太相似。在\u003cstrong\u003eVAE\u003c/strong\u003e中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先验概率$p_{\\theta}(z)$\u003c/li\u003e\n\u003cli\u003e似然性$p_{\\theta}(x | z)$\u003c/li\u003e\n\u003cli\u003e后验概率$p_{\\theta}(z | x)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$\u003c/li\u003e\n\u003cli\u003e然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e那么这个分布我们似乎就可以写出来了:\u003c/p\u003e\n\u003cp\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo data-mjx-texclass=\"OP\"\u003e∫\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e|\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ez\u003c/mi\u003e\u003c/math\u003e\u003c/p\u003e","title":"Diffusion Models"},{"content":"","permalink":"https://attract666.github.io/about/","summary":"about","title":"关于"}]
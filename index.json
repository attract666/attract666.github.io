[{"content":"LLaVA-1.5悟道 LLaVA-NEXT 与LLaVA-1.5相比，主要有以下提升：\n将输入图像分辨率提高 4 倍，使得它能够捕捉更多的视觉细节。它支持 3 种不同比例的分辨率，分别是 672x672、336x1344、1344x336。 改变视觉指令微调数据的比例，实现更好的视觉推理和 OCR 能力 更好的视觉对话，适用于更多场景，涵盖不同应用。 使用 SGLang 实现高效部署和推理。 构造多模态数据 常言道：“工欲善其事，必先利其器。”LLaVA这篇论文的另一个核心贡献就是在于将图像文本对转换成了多模态instruction-following数据。LLaVA构造了三种多模态instruction-following数据：\n对话（Conversation） 细节描述（Detailed description） 复杂推理（Complex reasoning） Conversation构造 构造的思路如下：\n给出一个system prompt，要求纯文本大模型根据图片的描述生成多轮对话 提供几个示例，便于纯文本大模型理解以及生成类似的多轮对话 将上面的内容输入给纯文本大模型，纯文本大模型返回多轮对话 system prompt和示例的内容可以见LLaVA/playground/data/prompts，在LLaVA论文中也给出了构造数据的示例代码：\nSample Codes.(Image source:LLaVA) 我这里由于没有gpt4的api，所以决定用llama3来代替，使用了vllm框架\n具体的代码实现如下：\n# 先启动服务 # python3 -m vllm.entrypoints.openai.api_server --model ./Meta-Llama-3-8B-Instruct --dtype auto --api-key 123456 from openai import OpenAI openai_api_key = \u0026#34;123456\u0026#34; # Same as --api-key in the deployment command openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; client = OpenAI(api_key=openai_api_key, base_url=openai_api_base) model = \u0026#39;./Meta-Llama-3-8B-Instruct\u0026#39; SYSTEM_CONTENT = \u0026#34;\u0026#34;\u0026#34; You are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary. \u0026#34;\u0026#34;\u0026#34; SAMPLE_CONTENT_1 = \u0026#34;\u0026#34;\u0026#34; There is a movie theater that displays the show times above the doors. A red fire hydrant is deep in the snow. The fire hydrant is in the snow near a recently plowed sidewalk. This city has had a very hard winter with snow. A hotel for dogs in the snow in winter.\u0026#34;\u0026#34;\u0026#34; SAMPLE_RESPONSE_1 = \u0026#34;\u0026#34;\u0026#34; Question: What color is the fire hydrant in the image? === Answer: The fire hydrant in the image is red. === Question: Is the fire hydrant fully covered by snow or partially visible? === Answer: The fire hydrant is partially visible, with a significant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the fire hydrant? === Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. === Question: What does the image suggest about the winter season in the city? === Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. For instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city\u0026#39;s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts. Furthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages. In conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure. \u0026#34;\u0026#34;\u0026#34; SAMPLE_CONTENT_2 = \u0026#34;\u0026#34;\u0026#34; A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range. A skier looks at mountains while standing near a trail sign. a single person skiing by an area with a lot of bushes \u0026#34;\u0026#34;\u0026#34; SAMPLE_RESPONSE_2 = \u0026#34;\u0026#34;\u0026#34; Question: What is the skier doing? === Answer: The skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing near a trail sign, which indicates their location on the mountain. === Question: Are there any other people or skiers visible in the image? === Answer: No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. === Question: What kind of skiing is the person doing? Cross-country or downhill? === Answer: Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that the skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts, whereas cross-country skiing often occurs on relatively flatter terrain with marked trails. === Question: Can you explain cross-country and downhill skiing a bit more? === Answer: Cross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences. 1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. There are two main techniques in cross-country skiing: - Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward. - Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It requires a smooth, groomed surface and is typically faster than the classic technique. Cross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used in downhill skiing, allowing for better ankle movement and control. 2. Downhill Skiing: Also known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and even jumps. Downhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing. Each discipline has its own set of rules, courses, and ski equipment. Downhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The boots are more rigid to provide better support and control during high-speed descents and sharp turns. In summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants. \u0026#34;\u0026#34;\u0026#34; messages = [ { \u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: SYSTEM_CONTENT, } ] fewshot_samples = [ { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_1, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_1, }, { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_2, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_2, } ] for sample in fewshot_samples: messages.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;context\u0026#39;]}) messages.append({\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;response\u0026#39;]}) query = \u0026#34;\u0026#34;\u0026#34; A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. \u0026#34;\u0026#34;\u0026#34; messages.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}) response = client.chat.completions.create(model=model, messages=messages).choices[0].message.content print(response) ","permalink":"https://attract666.github.io/posts/llava_02/","summary":"\u003ch1 id=\"llava-15悟道\"\u003eLLaVA-1.5悟道\u003c/h1\u003e\n\u003chr\u003e\n\u003ch1 id=\"llava-next\"\u003eLLaVA-NEXT\u003c/h1\u003e\n\u003cp\u003e与LLaVA-1.5相比，主要有以下提升：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将输入图像分辨率提高 4 倍，使得它能够捕捉更多的视觉细节。它支持 3 种不同比例的分辨率，分别是 672x672、336x1344、1344x336。\u003c/li\u003e\n\u003cli\u003e改变视觉指令微调数据的比例，实现更好的视觉推理和 OCR 能力\u003c/li\u003e\n\u003cli\u003e更好的视觉对话，适用于更多场景，涵盖不同应用。\u003c/li\u003e\n\u003cli\u003e使用 SGLang 实现高效部署和推理。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"构造多模态数据\"\u003e构造多模态数据\u003c/h1\u003e\n\u003cp\u003e常言道：“工欲善其事，必先利其器。”LLaVA这篇论文的另一个核心贡献就是在于将图像文本对转换成了多模态instruction-following数据。LLaVA构造了三种多模态instruction-following数据：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对话（Conversation）\u003c/li\u003e\n\u003cli\u003e细节描述（Detailed description）\u003c/li\u003e\n\u003cli\u003e复杂推理（Complex reasoning）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"conversation构造\"\u003eConversation构造\u003c/h2\u003e\n\u003cp\u003e构造的思路如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e给出一个system prompt，要求纯文本大模型根据图片的描述生成多轮对话\u003c/li\u003e\n\u003cli\u003e提供几个示例，便于纯文本大模型理解以及生成类似的多轮对话\u003c/li\u003e\n\u003cli\u003e将上面的内容输入给纯文本大模型，纯文本大模型返回多轮对话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003esystem prompt和示例的内容可以见\u003ccode\u003eLLaVA/playground/data/prompts\u003c/code\u003e，在LLaVA论文中也给出了构造数据的示例代码：\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_02/sample_codes.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eSample Codes.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e我这里由于没有gpt4的api，所以决定用llama3来代替，使用了vllm框架\u003c/p\u003e\n\u003cp\u003e具体的代码实现如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#8d8687\"\u003e# 先启动服务\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#8d8687\"\u003e# python3 -m vllm.entrypoints.openai.api_server --model ./Meta-Llama-3-8B-Instruct --dtype auto --api-key 123456\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#5bc4bf\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#fec418\"\u003eopenai\u003c/span\u003e \u003cspan style=\"color:#5bc4bf\"\u003eimport\u003c/span\u003e OpenAI\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eopenai_api_key \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;123456\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#8d8687\"\u003e# Same as --api-key in the deployment command\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eopenai_api_base \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;http://localhost:8000/v1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eclient \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e OpenAI(api_key\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003eopenai_api_key, base_url\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003eopenai_api_base)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#39;./Meta-Llama-3-8B-Instruct\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSYSTEM_CONTENT \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eYou are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eDesign a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAsk diverse questions and give corresponding answers.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eInclude questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have definite answers:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e(1) one can see the content in the image that the question asks about and can answer confidently;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e(2) one can determine confidently from the image that it is not in the image.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eDo not ask any question that cannot be answered confidently.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAlso include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eProvide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized.  You can include multiple paragraphs if necessary.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSAMPLE_CONTENT_1 \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThere is a movie theater that displays the show times above the doors.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA red fire hydrant is deep in the snow.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe fire hydrant is in the snow near a recently plowed sidewalk.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThis city has had a very hard winter with snow.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA hotel for dogs in the snow in winter.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSAMPLE_RESPONSE_1 \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eWhat color is the fire hydrant in the image?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe fire hydrant in the image is red.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eIs the fire hydrant fully covered by snow or partially visible?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe fire hydrant is partially visible, with a significant portion of it covered by the deep snow.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eWhat is the condition of the sidewalk near the fire hydrant?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eWhat does the image suggest about the winter season in the city?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eFor instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city\u0026#39;s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eFurthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eIn conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSAMPLE_CONTENT_2 \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA man is skiing in the open snow covered hills\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA skier is making his way into the snow.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA skier on the foothills of a huge mountain range.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA skier looks at mountains while standing near a trail sign.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003ea single person skiing by an area with a lot of bushes\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSAMPLE_RESPONSE_2 \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eWhat is the skier doing?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing near a trail sign, which indicates their location on the mountain.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAre there any other people or skiers visible in the image?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eNo, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eWhat kind of skiing is the person doing? Cross-country or downhill?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eBased on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that the skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts, whereas cross-country skiing often occurs on relatively flatter terrain with marked trails.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eQuestion:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eCan you explain cross-country and downhill skiing a bit more?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e===\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAnswer:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eCross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e1. Cross-Country Skiing:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAlso known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThere are two main techniques in cross-country skiing:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e- Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e- Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It requires a smooth, groomed surface and is typically faster than the classic technique. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eCross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used in downhill skiing, allowing for better ankle movement and control. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e2. Downhill Skiing:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eAlso known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and even jumps. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eDownhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing. Each discipline has its own set of rules, courses, and ski equipment. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eDownhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The boots are more rigid to provide better support and control during high-speed descents and sharp turns. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eIn summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emessages \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;role\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#48b685\"\u003e\u0026#39;system\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;content\u0026#39;\u003c/span\u003e: SYSTEM_CONTENT,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efewshot_samples \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;context\u0026#39;\u003c/span\u003e: SAMPLE_CONTENT_1,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;response\u0026#39;\u003c/span\u003e: SAMPLE_RESPONSE_1,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    },\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;context\u0026#39;\u003c/span\u003e: SAMPLE_CONTENT_2,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#48b685\"\u003e\u0026#39;response\u0026#39;\u003c/span\u003e: SAMPLE_RESPONSE_2,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#815ba4\"\u003efor\u003c/span\u003e sample \u003cspan style=\"color:#5bc4bf\"\u003ein\u003c/span\u003e fewshot_samples:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    messages\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eappend({\u003cspan style=\"color:#48b685\"\u003e\u0026#39;role\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#48b685\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#48b685\"\u003e\u0026#39;content\u0026#39;\u003c/span\u003e: sample[\u003cspan style=\"color:#48b685\"\u003e\u0026#39;context\u0026#39;\u003c/span\u003e]})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    messages\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eappend({\u003cspan style=\"color:#48b685\"\u003e\u0026#39;role\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#48b685\"\u003e\u0026#39;assistant\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#48b685\"\u003e\u0026#39;content\u0026#39;\u003c/span\u003e: sample[\u003cspan style=\"color:#48b685\"\u003e\u0026#39;response\u0026#39;\u003c/span\u003e]})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003equery \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eA group of people standing outside of a black vehicle with various luggage.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eLuggage surrounds a vehicle in an underground parking area\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003ePeople try to fit all of their luggage in an SUV.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eThe sport utility vehicle is parked in the public garage, being packed for a trip\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003eSome people with luggage near a van that is transporting it.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emessages\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eappend({\u003cspan style=\"color:#48b685\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#48b685\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#48b685\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: query})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresponse \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e client\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003echat\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003ecompletions\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003ecreate(model\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003emodel, messages\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003emessages)\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003echoices[\u003cspan style=\"color:#f99b15\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003emessage\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003econtent\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(response)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pr","title":"从零开始学LLaVA 02"},{"content":"从LLaVA-1.0论文开始 LLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\nPaper link: https://arxiv.org/abs/2304.08485\nGithub: https://github.com/haotian-liu/LLaVA\n论文亮点 多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类 多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space 网络架构 结构上vision encoder使用的是CLIP-L/14（使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征）进行编码，之后使用一个投影层（全连接层）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\nLLaVA Architecture.(Image source:LLaVA)\r数据集构成及数据收集细节 训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\n预训练数据集（CC3M过滤）：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\nCC3M Filtered.(Image source:LLaVA)\r将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\nCC3M image description.(Image source:LLaVA)\r指令格式如下：\nCC3M instructions.(Image source:LLaVA)\r指令微调数据集：使用COCO数据集，将图像描述、bbox、类别等信息输入给GPT4-text-only生成对话、细节描述和复杂推理三种类型instruction-following数据。最终生成58K对话、23K细节描述和77K复杂推理。\n对话：对图像内容进行多轮回答，且只考虑有明确答案的问题，通过这样的方式保证答案的质量 细节描述：使用另外一些描述的提示，随机让GPT4-text-only生成详细描述 复杂推理：上面都是视觉本身内容，设计了一些需要深度推理的问题，答案要有严格的逻辑 训练方式 训练的目标就是最大化似然概率\nTrain traget.(Image source:LLaVA)\r其中\nInstruction.(Image source:LLaVA)\r在计算自回归模型的loss时，只考虑回答$X_a$和\u0026lt;STOP\u0026gt;的tokens\nInput sequence.(Image source:LLaVA)\r推理 在我们阅读完LLaVA论文后，想要获得自己的LLaVA，第一件要做的事很自然是先跑通LLaVA的推理脚本。这里我考虑的是直接跑LLaVA仓库的代码，加载官方给出的模型，而不是huggingface集成好的模型。推理的入口函数是eval_model()。\n它的实现如下：\n# llava/eval/run_llava.py def eval_model(args): # Model # 将linear层和layernorm层的reset_parameters方法用空lambda函数代替，达到禁用pytorch默认初始化 disable_torch_init() # 初始化模型 model_name = get_model_name_from_path(args.model_path) tokenizer, model, image_processor, context_len = load_pretrained_model( args.model_path, args.model_base, model_name ) # 数据预处理 qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs if \u0026#34;llama-2\u0026#34; in model_name.lower(): conv_mode = \u0026#34;llava_llama_2\u0026#34; ... else: conv_mode = \u0026#34;llava_v0\u0026#34; if args.conv_mode is not None and conv_mode != args.conv_mode: print( \u0026#34;[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\u0026#34;.format( conv_mode, args.conv_mode, args.conv_mode ) ) else: args.conv_mode = conv_mode conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) # 推理 with torch.inference_mode(): output_ids = model.generate( input_ids, images=images_tensor, image_sizes=image_sizes, do_sample=True if args.temperature \u0026gt; 0 else False, temperature=args.temperature, top_p=args.top_p, num_beams=args.num_beams, max_new_tokens=args.max_new_tokens, use_cache=True, ) # 解码 outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() print(outputs) 整块函数的实现框架是：\n初始化模型：通过disable_torch_init()来加速模型加载 数据预处理：拼接caption和image token 推理：接受input_ids和images作为输入，输出output_ids 解码 之后具体来看看各个部分的实现\n初始化模型 初始化模型主要有两个函数get_model_name_from_path()和load_pretrained_model()。\nget_model_name_from_path()的实现比较简单，就是把模型路径名去除首尾\u0026quot;/\u0026ldquo;后，在将字符串按\u0026rdquo;/\u0026ldquo;分割成一个列表，之后根据列表最后一个元素是否已\u0026quot;checkpoint-\u0026ldquo;开头，返回model_name。\nload_pretrained_model()的实现则比较长，但并不复杂。\n# llava/model/builder.py def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\u0026#34;auto\u0026#34;, device=\u0026#34;cuda\u0026#34;, use_flash_attn=False, **kwargs): ... # 前面主要是一些关于device、量化参数和flash_attention的设置 # Load LLaVA model if \u0026#39;llava\u0026#39; in model.name.lower(): if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is None: # model_name要包含lora，同时提供model_base ... if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is not None: # 使用LlavaConfig从model_path中加载lora配置 ... # 通过参数model_base加载tokenizer tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) # 从base model中加载LLaVA，确保 token_num, token_dim = lm_head.out_features, lm_head.in_features model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs) ... # 加载LLaVA额外权重，检查是否存在non_lora_trainables文件，没有就假定从HF hub下载 ... # 加载lora权重 model = PeftModel.from_pretrained(model, model_path) # 合并lora权重 model = model.merge_and_unload() elif model_base is not None: ... else: if \u0026#39;mpt\u0026#39; in model_name.lower(): ... elif \u0026#39;mistral\u0026#39; in model_name.lower(): ... else: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) model = LlavaLlamaForCausalLM.from_pretrained( model_path, low_cpu_mem_usage=True, **kwargs ) else: # Load language model ... image_processor = None if \u0026#39;llava\u0026#39; in model_name.lower(): # 从模型的config中获取start和end token，如果不存在则默认为false。获取patch token，不存在默认为True mm_use_im_start_end = getattr(model.config, \u0026#34;mm_use_im_start_end\u0026#34;, False) mm_use_im_patch_token = getattr(model.config, \u0026#34;mm_use_im_patch_token\u0026#34;, True) if mm_use_im_patch_token: tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True) if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True) model.resize_token_embeddings(len(tokenizer)) # 加载vision tower vision_tower = model.get_vision_tower() if not vision_tower.is_loaded: vision_tower.load_model(device_map=device_map) if device_map != \u0026#39;auto\u0026#39;: vision_tower.to(device=device_map, dtype=torch.float16) image_processor = vision_tower.image_processor # 检查最大输入序列长度 if hasattr(model.config, \u0026#34;max_sequence_length\u0026#34;): context_len = model.config.max_sequence_length else: context_len = 2048 return tokenizer, model, image_processor, context_len load_pretrained_model()的实现逻辑是：\n初始化tokenizer、model 从model中加载vision_tower的image_processor 数据预处理 接下来是，是数据预处理的实现\n# llava/eval/run_llava.py def eval_model(args): ... # 加载query和定义image token qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN # 如果query中存在图片占位符 if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs # 获取对话模板并拼接成prompt if \u0026#34;llama-2\u0026#34; in model_name.lower(): ... conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() # 加载图片并处理图片 image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) # tokenize prompt input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) 其中conv是Conversation类的实例，具体如下：\n# llava/conversation.py @dataclasses.dataclass class Conversation: system: str roles: List[str] messages: List[List[str]] offset: int sep_style: SeparatorStyle = SeparatorStyle.SINGLE sep: str = \u0026#34;###\u0026#34; sep2: str = None version: str = \u0026#34;Unknown\u0026#34; skip_next: bool = False def get_prompt(self): messages = self.messages if len(messages) \u0026gt; 0 and type(messages[0][1]) is tuple: ... if self.sep_style == SeparatorStyle.SINGLE: ... elif self.sep_style == SeparatorStyle.TWO: ... elif self.sep_style == SeparatorStyle.MPT: ... elif self.sep_style == SeparatorStyle.LLAMA_2: ... elif self.sep_style == SeparatorStyle.PLAIN: ... else: ... return ret def append_message(self, role, message): self.messages.append([role, message]) def process_image(self, image, image_process_mode, return_pil=False, image_format=\u0026#39;PNG\u0026#39;, max_len=1344, min_len=672): ... def get_images(self, return_pil=False): ... def to_gradio_chatbot(self): ... def copy(self): return Conversation( system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version) def dict(self): ... # messages: [[\u0026#39;USER\u0026#39;, \u0026#39;\u0026lt;image\u0026gt;\\nHow many pizzas are there in the image?\u0026#39;], [\u0026#39;ASSISTANT\u0026#39;, None]] 数据预处理的逻辑如下：\nquery拼接image占位符并用对话模板拼接成prompt 加载图像并resize tokenize prompt 我们可以简单画一张图来表示数据预处理流程：\nPreprocessing.(Image source:COCO-val-2014)\r模型推理 在推理时，主要会调用model.generate()，该方法定义在llava/model/language_model/llava_llama.py\n# llava/model/language_model/llava_llama.py class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM): config_class = LlavaConfig @torch.no_grad() def generate( self, inputs: Optional[torch.Tensor] = None, images: Optional[torch.Tensor] = None, image_sizes: Optional[torch.Tensor] = None, **kwargs, ) -\u0026gt; Union[GenerateOutput, torch.LongTensor]: position_ids = kwargs.pop(\u0026#34;position_ids\u0026#34;, None) attention_mask = kwargs.pop(\u0026#34;attention_mask\u0026#34;, None) if \u0026#34;inputs_embeds\u0026#34; in kwargs: raise NotImplementedError(\u0026#34;`inputs_embeds` is not supported\u0026#34;) if images is not None: ( inputs, position_ids, attention_mask, _, inputs_embeds, _ ) = self.prepare_inputs_labels_for_multimodal( inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes ) else: inputs_embeds = self.get_model().embed_tokens(inputs) return super().generate( position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs ) 整体的代码很短，主要集中在prepare_inputs_labels_for_multimodal()方法上\nprepare_inputs_labels_for_multimodal()的实现如下：\n# llava/model/llava_arch.py class LlavaMetaForCausalLM(ABC): def prepare_inputs_labels_for_multimodal( self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None ): vision_tower = self.get_vision_tower() if vision_tower is None or images is None or input_ids.shape[1] == 1: ... # 视觉特征 if type(images) is list or images.ndim == 5: ... else: image_features = self.encode_images(images) # TODO: image start / end is not implemented here to support pretraining. if getattr(self.config, \u0026#39;tune_mm_mlp_adapter\u0026#39;, False) and getattr(self.config, \u0026#39;mm_use_im_start_end\u0026#39;, False): raise NotImplementedError # 在推理时，attention_mask、position_ids和labels都为None # 所以实际是： # attention_mask = torch.ones_like(input_ids, dtype=torch.bool) # position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device) # labels = torch.full_like(input_ids, IGNORE_INDEX) # 使用attention_mask移除padding ... new_input_embeds = [] new_labels = [] cur_image_idx = 0 for batch_idx, cur_input_ids in enumerate(input_ids): num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum() if num_images == 0: ... # 计算文本特征 image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]] cur_input_ids_noim = [] cur_labels = labels[batch_idx] cur_labels_noim = [] for i in range(len(image_token_indices) - 1): cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]]) cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]]) split_sizes = [x.shape[0] for x in cur_labels_noim] cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim)) cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0) cur_new_input_embeds = [] cur_new_labels = [] # 将文本特征与图像特征进行拼接 for i in range(num_images + 1): cur_new_input_embeds.append(cur_input_embeds_no_im[i]) cur_new_labels.append(cur_labels_noim[i]) if i \u0026lt; num_images: cur_image_features = image_features[cur_image_idx] cur_image_idx += 1 cur_new_input_embeds.append(cur_image_features) cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype)) cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds] cur_new_input_embeds = torch.cat(cur_new_input_embeds) cur_new_labels = torch.cat(cur_new_labels) new_input_embeds.append(cur_new_input_embeds) new_labels.append(cur_new_labels) # 将输入特征进行padding ... 值得注意是提取图像特征，如果使用官方的llava-v1.5-7b的checkpoint，可以看到model中的两个参数select_layer: -2和select_feature: 'patch'，它们分别控制选择CLIP输出第几层特征和是否保留\u0026lt;CLS\u0026gt; token的特征。这也就是论文中提到的使用倒数第二层特征，具体的代码如下：\n# llava/model/multimodal_encoder/clip_encoder.py class CLIPVisionTower(nn.Module): def feature_select(self, image_forward_outs): image_features = image_forward_outs.hidden_states[self.select_layer] if self.select_feature == \u0026#39;patch\u0026#39;: image_features = image_features[:, 1:] elif self.select_feature == \u0026#39;cls_patch\u0026#39;: image_features = image_features else: raise ValueError(f\u0026#39;Unexpected select feature: {self.select_feature}\u0026#39;) return image_features prepare_inputs_labels_for_multimodal()的整体逻辑如下图所示：\nTokenize.(Image source:COCO-val-2014)\r得到inputs_embeds后，会将其传给父类的generate方法，这里会调用LlamaForCausalLM的generate()方法，它的实现定义在transformers的llama/modeling_llama.py中。\n输出后处理 最后一步就是通过tokenizer的batch_decode()将得到的output_ids解码成文本。\n# llava/eval/run_llava.py def eval_model(args): ... outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() print(outputs) 至此，LLaVA的推理过程就结束了，我们用一张图来表示整个过程：\nLLaVA Inference.(Image source:COCO-val-2014)\r参考 [1] https://arxiv.org/abs/2304.08485\n[2] https://github.com/haotian-liu/LLaVA\n[3] https://zhuanlan.zhihu.com/p/696112028\n[4] https://zhuanlan.zhihu.com/p/696654492\n","permalink":"https://attract666.github.io/posts/llava_01/","summary":"\u003ch1 id=\"从llava-10论文开始\"\u003e从LLaVA-1.0论文开始\u003c/h1\u003e\n\u003cp\u003eLLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\u003c/p\u003e\n\u003cp\u003ePaper link: \u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003e\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003ehttps://arxiv.org/abs/2304.08485\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGithub: \u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003e\u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003ehttps://github.com/haotian-liu/LLaVA\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"论文亮点\"\u003e论文亮点\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类\u003c/li\u003e\n\u003cli\u003e多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"网络架构\"\u003e网络架构\u003c/h2\u003e\n\u003cp\u003e结构上vision encoder使用的是CLIP-L/14（\u003cstrong\u003e使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征\u003c/strong\u003e）进行编码，之后使用一个投影层（\u003cstrong\u003e全连接层\u003c/strong\u003e）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/LLaVA.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eLLaVA Architecture.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003ch2 id=\"数据集构成及数据收集细节\"\u003e数据集构成及数据收集细节\u003c/h2\u003e\n\u003cp\u003e训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e预训练数据集（CC3M过滤）\u003c/strong\u003e：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_filtered.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M Filtered.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_prompt.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M image description.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e指令格式如下：\u003c/p\u003e","title":"从零开始学LLaVA 01"},{"content":"什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\\theta^{*}$，即：\nθ∗=argmaxθ∏i=1npθ(x(i))=argmaxθ∑i=1nlog⁡pθ(x(i))\n但是，直接计算似然性$p_{\\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\\theta}(x)$可以依靠概率的链式法则$p_{\\theta}(x) = \\frac{p_{\\theta}(x | z)p_{\\theta}(z)}{p_{\\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为Evidence Lower Bound(ELBO)。因此，我们可以将最大化ELBO作为优化潜变量模型的代理目标，即：\nlog⁡pθ(x)≥Eqϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n此处的$q_{\\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\\theta}(z|x)$\n什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) ","permalink":"https://attract666.github.io/posts/ddpm/","summary":"\u003ch1 id=\"什么是generative-model\"\u003e什么是Generative Model?\u003c/h1\u003e\n\u003cp\u003e我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\u003c/p\u003e\n\u003cp\u003e在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\u003c/p\u003e\n\u003ch2 id=\"ae--vae\"\u003eAE \u0026amp; VAE\u003c/h2\u003e\n\u003cp\u003e我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/AE.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eAutoencoder.(Image source:\u003ca href=\"https://lilianweng.github.io/posts/2018-08-12-vae/\"\u003eLilian Weng\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e这就是\u003cstrong\u003eAutoencoder(AE)\u003c/strong\u003e的想法，\u003cstrong\u003eAE\u003c/strong\u003e包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariational Autoencoder（VAE）\u003c/strong\u003e的思想实际上与\u003cstrong\u003eAE\u003c/strong\u003e不太相似。在\u003cstrong\u003eVAE\u003c/strong\u003e中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先验概率$p_{\\theta}(z)$\u003c/li\u003e\n\u003cli\u003e似然性$p_{\\theta}(x | z)$\u003c/li\u003e\n\u003cli\u003e后验概率$p_{\\theta}(z | x)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$\u003c/li\u003e\n\u003cli\u003e然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e那么这个分布我们似乎就可以写出来了:\u003c/p\u003e\n\u003cp\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo data-mjx-texclass=\"OP\"\u003e∫\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e|\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ez\u003c/mi\u003e\u003c/math\u003e\u003c/p\u003e","title":"Diffusion Models"},{"content":"","permalink":"https://attract666.github.io/about/","summary":"about","title":"关于"}]
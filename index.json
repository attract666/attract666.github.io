[{"content":"为了方便讲解，我们先对文本进行tokenize和embedding。我们仅在这一步使用了除torch以外的库\ntext = \u0026#34;This is your first step to understanding LLM\u0026#34; kwargs = {\u0026#34;device_map\u0026#34;: \u0026#34;cuda:3\u0026#34;} tokenizer = AutoTokenizer.from_pretrained(\u0026#34;llama3/Meta-Llama-3-8B\u0026#34;, use_fast=False) model = AutoModelForCausalLM.from_pretrained(\u0026#34;llama3/Meta-Llama-3-8B\u0026#34;, **kwargs) inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;) inputs = {key: value.to(kwargs[\u0026#34;device_map\u0026#34;]) for key, value in inputs.items()} inputs = model.get_input_embeddings()(inputs[\u0026#34;input_ids\u0026#34;]) tensor([[[-1.4305e-04, 1.0777e-04, -1.9646e-04, ..., 2.0218e-04, 1.4842e-05, 3.0136e-04], [ 1.7090e-03, -2.4261e-03, 1.0452e-03, ..., 5.9814e-03, 2.9602e-03, -2.4414e-03], [-2.9144e-03, 1.5335e-03, -2.4605e-04, ..., 9.7046e-03, 4.2114e-03, 7.9956e-03], ..., [ 1.6968e-02, -1.1749e-03, -4.4556e-03, ..., 7.4463e-03, -1.6785e-03, 1.2878e-02], [-9.9487e-03, 2.3651e-03, -2.1057e-03, ..., 8.2397e-03, -3.9978e-03, -3.4637e-03], [-8.3923e-05, 1.3428e-03, -3.6011e-03, ..., 3.7384e-03, -1.3184e-02, -3.7537e-03]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;EmbeddingBackward0\u0026gt;) # shape: [1, 10, 4096] # 因为包含开始和结束token 所以第二个维度是10 自注意力机制关注输入的不同区域 不包含可训练权重的自注意力机制 按照惯例，未经归一化的注意力权重被称为“注意力分数”，经过归一化的注意力分数被称为“注意力权重” 这一步我们的目标是获得输入序列的上下文向量 上下文向量相当于输入序列的变体，它不仅包含了输入序列特定元素的信息，还包含了当任务相关的所有其他输入元素的信息 让我们先以第4个元素为例（即your对应的embedding），计算它对应的上下文向量，后面我们会推广这个方法来计算所有的上下文向量\n第一步：计算注意力分数，通过查询$x^{(4)}$与所有其他输入token之间的点积来实现： query = embeddings[0, 3, :] device = embeddings.device # 创建一个空的张量来存储注意力分数，其形状与输入序列的批次大小相同 attn_scores_4 = torch.empty(embeddings.shape[1], device=device) for i, x_i in enumerate(embeddings[0]): attn_scores_4[i] = torch.dot(x_i, query) print(attn_scores_4) # tensor([0.0052, 0.0204, 0.0254, 0.1827, 0.0237, 0.0168, 0.0257, 0.0229, 0.0092, # 0.0067], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;CopySlices\u0026gt;) 第二步：归一化注意力分数。在实际应用中，使用softmax进行归一化更为常见 attn_weights_4 = torch.softmax(attn_scores_4, dim=0) print(f\u0026#34;Attention weights: {attn_weights_4}\u0026#34;) print(f\u0026#34;Sum: {attn_weights_4.sum()}\u0026#34;) # Attention weights: tensor([0.0971, 0.0985, 0.0990, 0.1159, 0.0989, 0.0982, 0.0991, 0.0988, 0.0974, # 0.0972], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;) # Sum: 1.0 第三步：将输入序列与注意力权重相乘，并加起来，得到上下文向量 query = embeddings[0, 3, :] context_vec_4 = torch.zeros(query.shape, device=device) for i, x_i in enumerate(embeddings[0]): context_vec_4 += x_i * attn_weights_4[i] print(context_vec_4) print(context_vec_4.shape) # tensor([ 0.0001, -0.0021, -0.0020, ..., 0.0031, -0.0035, 0.0007], # device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;AddBackward0\u0026gt;) # torch.Size([4096]) # shape : 10 * (10, 4096) 在上面的过程中，我们计算了第4个元素的上下文向量，接下来我们将其推广到整个输入序列\nb = embeddings.shape[0] seq_length = embeddings.shape[1] emb_size = embeddings.shape[2] # 创建空的注意力分数矩阵 attn_scores = torch.empty(b, seq_length, seq_length, device=device) attn_scores = torch.bmm(embeddings, embeddings.transpose(1, 2)) print(attn_scores.shape) # 归一化注意力分数矩阵 attn_weights = torch.softmax(attn_scores, dim=-1) print(f\u0026#34;Sum :{attn_weights.sum(dim=-1)}\u0026#34;) # 获得上下文向量 all_context_vec = torch.bmm(attn_weights, embeddings) print(all_context_vec.shape) # torch.Size([1, 10, 10]) # Sum :tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, # 1.0000]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;SumBackward1\u0026gt;) # torch.Size([1, 10, 4096]) 使用可训练权重实现自注意力机制 这种注意力机制也被称为“缩放点积注意力” 与上一节相同，我们需要注意力权重，但不同的在于这个注意力权重在模型训练期间可以更新，以便学习“good”上下文 在实现自注意力机制之前，我们首先引入三个训练权重矩阵$W_q$、$W_k$和$W_v$，这三个矩阵将embedding的特定元素向量投影到三个向量： query：$q^{(i)} = W_q \\cdot x^{(i)}$ key： $k^{(i)} = W_k \\cdot x^{(i)}$ value：$v^{(i)} = W_v \\cdot x^{(i)}$ 输入维度与输出维度可以相同也可以不同，这里我们假设它们不同 x_4 = embeddings[0, 3, :] d_in = embeddings.shape[2] # 4096 d_out = 2048 下面我们将初始化三个权重矩阵；请注意，我们会设置requires_grad=False以减少输出的混乱，但在实际训练过程中requires_grad应设置为True，以便更新这些矩阵 torch.manual_seed(42) W_q = torch.nn.Parameter(torch.rand(b, d_in, d_out), requires_grad=False) W_k = torch.nn.Parameter(torch.rand(b, d_in, d_out), requires_grad=False) W_v = torch.nn.Parameter(torch.rand(b, d_in, d_out), requires_grad=False) 接下来分别计算三个向量 query_4 = x_4 @ W_q key_4 = x_4 @ W_k value_4 = x_4 @ W_v print(query_4) print(query_4.shape) # tensor([0.3062, 0.2997, 0.4767, ..., 0.4066, 0.5410, 0.4954], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;SqueezeBackward4\u0026gt;) # torch.Size([2048]) 我们将所有token的从4096维度映射到了2048维度 keys = embeddings[0] @ W_k values = embeddings[0] @ W_v print(keys.shape) print(values.shape) # torch.Size([10, 2048]) # torch.Size([10, 2048]) 接下来，我通过计算query和每个key向量之间的点积来计算注意力分数，会得到10个注意力分数 attn_scores_4 = query_4 @ keys.T print(attn_scores_4) print(attn_scores_4.shape) # tensor([ 58.8918, 166.3115, 81.8653, 339.3221, 96.8350, 457.5436, -6.2441, # 56.3547, 162.6453, 406.4980], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;SqueezeBackward4\u0026gt;) # torch.Size([10]) 下一步是归一化注意力分数，但与之前不同的是，之前使用softmax归一化注意力分数。我们现在通过除以key和query的纬度的平方根来缩放注意力分数 这里通过除以key的维度的平方根，来防止过大的点积，导致softmax梯度过小 d_k = keys.shape[1] # 2048 attn_weights_4 = torch.softmax(attn_scores_4 / d_k**0.5, dim=-1) print(attn_weights_4) print(attn_weights_4.shape) # tensor([1.0662e-04, 1.1447e-03, 1.7713e-04, 5.2362e-02, 2.4658e-04, 7.1375e-01, # 2.5278e-05, 1.0080e-04, 1.0556e-03, 2.3104e-01], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;) # torch.Size([10]) 获得上下文向量 context_vec_4 = attn_weights_4 @ values print(context_vec_4) print(context_vec_4.shape) # tensor([0.5827, 0.6454, 0.4521, ..., 0.4937, 0.3820, 0.3779], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;SqueezeBackward4\u0026gt;) # torch.Size([2048]) 实现一个SelfAttention类 最后让我们整合一下，就可以得到一个紧凑的SelfAttention类\nimport torch.nn as nn import torch class SelfAttention(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): super().__init__() self.d_out = d_out self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) def forward(self, x): keys = self.W_key(x) queries = self.W_query(x) values = self.W_value(x) attn_scores = queries @ keys.transpose(1, 2) attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) context_vec = attn_weights @ values return context_vec 遮蔽下文信息的注意力机制 使用因果注意力掩码 在本节，我们将前文的自注意力机制转化为因果自注意力机制\n核心在于：模型对序列中某个位置的预测只依赖于前面位置的已知输出，而不依赖于未来的位置\n我们先试用上面的自注意力类来获得注意力权重。值得注意的是，我们的自注意力使用nn.Linear来初始化权重矩阵的，nn.Linea自带一种优选的权重初始化方案，这有助于实现更稳定的模型训练。\ntorch.manual_seed(1024) device = embeddings.device d_in = embeddings.shape[2] d_out = 2048 selfattention = SelfAttention(d_in, d_out, device=device) querys = selfattention.W_query(embeddings) keys = selfattention.W_key(embeddings) attn_scores = querys @ keys.transpose(1, 2) attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # print(attn_weights) print(attn_weights.shape) # tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;) # torch.Size([1, 10, 10]) 屏蔽未来的注意力权重最简单的方法是通过PyTorch的tril函数创建一个掩码，主对角线（包括对角线本身）以下的元素设置为1，主对角线以上的元素设置为0： block_size = attn_scores.shape[-1] mask_simple = torch.tril(torch.ones(block_size, block_size)).to(device) print(mask_simple) # tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.], # [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.], # [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.], # [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.], # [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.], # [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.], # [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.], # [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.], # [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device=\u0026#39;cuda:3\u0026#39;) 然后我们将注意力权重与掩码矩阵相乘（此处是对应元素相乘），将主对角线以上的权重归零： mask_simple = attn_weights*mask_simple print(mask_simple) # tensor([[[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.0000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;MulBackward0\u0026gt;) 然而，如果我们像这样在softmax之后进行掩码，它会破坏softmax创建的概率分布，简单来说就是我们输出的总和不在为1 因此，为了确保输出的值总和为1，我们对注意力分数进行掩码，用负无穷大来掩盖对角线以上的部分 mask = torch.triu(torch.ones(block_size, block_size), diagonal=1).to(device) masked = attn_scores.masked_fill(mask.bool(), -torch.inf) attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1) print(attn_weights) # tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000, # 0.0000, 0.0000], # [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, # 0.0000, 0.0000], # [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, # 0.1111, 0.0000], # [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, # 0.1000, 0.1000]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;) 通过dropout来实现额外注意力权重的掩码 此外，我还可以在训练阶段使用dropout来减少过拟合 dropout可以应用在下面多个地方： 计算注意力权重后 将注意力权重与values相乘后 我们在此处演示更为常见的情况，在计算注意力权重之后dropout，并使用p=0.2，那么未被屏蔽的元素会以x / (1 - p)进行缩放 dropout只在训练阶段使用，在推理阶段是不使用的 torch.manual_seed(1024) dropout = torch.nn.Dropout(p=0.2) print(dropout(attn_weights)) # tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.6250, 0.6250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.4167, 0.4167, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.3125, 0.3125, 0.3125, 0.3125, 0.0000, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.2083, 0.0000, 0.2083, 0.2083, 0.2083, 0.2083, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1786, 0.1786, 0.0000, 0.1786, 0.1786, 0.1786, 0.0000, 0.0000, # 0.0000, 0.0000], # [0.1563, 0.1562, 0.1563, 0.1562, 0.1563, 0.1562, 0.1563, 0.0000, # 0.0000, 0.0000], # [0.1389, 0.1389, 0.1389, 0.1389, 0.1389, 0.0000, 0.0000, 0.0000, # 0.1389, 0.0000], # [0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000, 0.1250, 0.1250, # 0.0000, 0.1250]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;NativeDropoutBackward0\u0026gt;) 实现一个CausalAttention类 最后，让我们来整合一下上面所有的内容，就可以得到带有dropout的因果自注意力类\nimport torch.nn as nn import torch class CausalAttention(nn.Module): def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False, device=None): super().__init__() self.d_out = d_out self.device = device self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.dropout = nn.Dropout(dropout).to(device) self.register_buffer(\u0026#39;mask\u0026#39;, torch.triu(torch.ones(block_size, block_size), diagonal=1).to(device)) def forward(self, x): b, seq_length, d_in = x.shape keys = self.W_key(x) queries = self.W_query(x) values = self.W_value(x) attn_scores = queries @ keys.transpose(1, 2) # _ ops are in-place attn_scores.masked_fill_(self.mask.bool()[:seq_length, :seq_length], -torch.inf) attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) attn_weights = self.dropout(attn_weights) context_vec = attn_weights @ values return context_vec 让我们来测试一下\ntorch.manual_seed(1024) b, seq_length, hidden_size = embeddings.shape device = embeddings.device causalattention = CausalAttention(d_in=hidden_size, d_out=hidden_size, block_size=seq_length, dropout=0.2, device=device) output = causalattention(embeddings) print(output) print(output.shape) # tensor([[[ 0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, # 0.0000e+00, 0.0000e+00], # [-3.4140e-03, -3.2753e-03, -6.7516e-04, ..., -2.5813e-05, # -9.6949e-04, -6.0929e-03], # [-2.6352e-03, -2.6163e-03, -3.5899e-04, ..., -2.7544e-03, # -6.0539e-04, -2.1978e-03], # ..., # [-3.7001e-03, -5.4613e-04, 1.4258e-04, ..., -3.8889e-04, # -3.4689e-03, -1.3611e-03], # [-2.9000e-03, -1.6623e-03, 1.5887e-04, ..., 3.9370e-04, # -1.9343e-03, 1.0009e-04], # [-1.3459e-04, -7.6909e-04, -4.2374e-05, ..., 4.8359e-04, # -2.5928e-03, 1.2988e-03]]], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;UnsafeViewBackward0\u0026gt;) # torch.Size([1, 10, 4096]) 将单头拓展到多头 直接将多个单头注意力层堆积起来 我们可以简单地将多个单头注意力层堆积在一起实现多头注意力层 多头注意力机制的主要思想是使用不同的、已学习的权重矩阵，并行运行注意力机制。这使得模型能够在不同位置的不同表示子空间中联合关注信息 import torch.nn as nn import torch class MultiHeadAttentionWrapper(nn.Module): def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False, device=None): super().__init__() self.heads = nn.ModuleList([CausalAttention(d_in, d_out, block_size, dropout, qkv_bias, device) for _ in range(num_heads)]) def forward(self, x): return torch.cat([head(x) for head in self.heads], dim=-1) 简单测试一下 torch.manual_seed(1024) b, seq_length, hidden_size = embeddings.shape device = embeddings.device mha = MultiHeadAttentionWrapper(d_in=hidden_size, d_out=hidden_size, block_size=seq_length, dropout=0.2, num_heads=12, device=device) context_vec = mha(embeddings) print(context_vec) print(context_vec.shape) # tensor([[[ 0.0000e+00, 0.0000e+00, 0.0000e+00, ..., -6.3230e-03, # -4.0065e-03, 2.3815e-03], # [-3.4140e-03, -3.2753e-03, -6.7516e-04, ..., -2.9023e-03, # 9.8894e-04, 7.2249e-04], # [-2.6352e-03, -2.6163e-03, -3.5899e-04, ..., -1.9348e-03, # 6.5939e-04, 4.8163e-04], # ..., # [-3.7001e-03, -5.4613e-04, 1.4258e-04, ..., -4.5610e-04, # 1.7907e-03, -1.7851e-03], # [-2.9000e-03, -1.6623e-03, 1.5887e-04, ..., -4.9142e-04, # 1.5937e-03, -2.5343e-03], # [-1.3459e-04, -7.6909e-04, -4.2374e-05, ..., -5.9090e-04, # 6.2077e-04, -1.3590e-03]]], device=\u0026#39;cuda:3\u0026#39;, grad_fn=\u0026lt;CatBackward0\u0026gt;) # torch.Size([1, 10, 49152]) 我们使用了相同的d_in和d_out，并且使用了12个头，所以最终的维度会是4096 * 12 = 49152 通过权重分割实现多头注意力 尽管上述多头注意力的实现是最直观且功能完整的（将CausalAttention实现封装在内），但我们也可以编写一个独立类来实现相同的功能 对于这个独立的MultiHeadAttention类，我们不会将单个注意力头连接在一起。相反，我们创建单个W_query、W_key和W_value权重矩阵，然后将它们拆分为每个注意力头的独立矩阵 import torch.nn as nn import torch class MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False, device=None): super().__init__() assert d_out % num_heads == 0, \u0026#34;d_out should be divisible by num_heads\u0026#34; self.d_out = d_out self.num_heads = num_heads self.head_dim = d_out // num_heads self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias).to(device) self.output_proj = nn.Linear(d_out, d_out).to(device) self.dropout = nn.Dropout(dropout) self.register_buffer(\u0026#34;mask\u0026#34;, torch.triu(torch.ones(block_size, block_size), diagonal=1).to(device)) def forward(self, x): b, seq_length, d_in = x.shape # shape : (b, seq_length, d_out) keys = self.W_key(x) queries = self.W_query(x) values = self.W_value(x) # shape : (b, seq_length, num_heads, head_dim) keys = keys.view(b, seq_length, self.num_heads, self.head_dim) queries = queries.view(b, seq_length, self.num_heads, self.head_dim) values = values.view(b, seq_length, self.num_heads, self.head_dim) # shape : (b, num_heads, seq_length, head_dim) keys = keys.transpose(1, 2) queries = queries.transpose(1, 2) values = values.transpose(1, 2) attn_scores = queries @ keys.transpose(2, 3) mask_bool = self.mask.bool()[:seq_length, :seq_length] mask_unsqueeze = mask_bool.unsqueeze(0).unsqueeze(0) attn_scores.masked_fill_(mask_unsqueeze, -torch.inf) attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) attn_weights = self.dropout(attn_weights) # shape : (b, seq_length, num_heads, head_dim) context_vec = (attn_weights @ values).transpose(1, 2) # view require the tensor to be contiguous context_vec = context_vec.contiguous().view(b, seq_length, self.d_out) context_vec = self.output_proj(context_vec) return context_vec 测试一下 torch.manual_seed(1024) b, seq_length, hidden_size = embeddings.shape device = embeddings.device mha = MultiHeadAttention(d_in=hidden_size, d_out=hidden_size*12, block_size=seq_length, dropout=0.2, num_heads=12, device=device) context_vec = mha(embeddings) print(context_vec) print(context_vec.shape) # tensor([[[-1.7976e-03, -7.1362e-03, 3.0706e-04, ..., 5.2778e-03, # -3.9758e-04, -3.7783e-03], # [-2.2841e-04, -6.1624e-03, 1.7655e-03, ..., 4.7084e-03, # -3.8035e-03, -4.4949e-03], # [-9.7516e-04, -5.9961e-03, -6.8417e-04, ..., 4.7274e-03, # -4.7388e-04, -4.6040e-03], # ..., # [-1.9826e-03, -3.5002e-03, -9.4147e-04, ..., 3.3272e-03, # -8.4172e-04, -3.7638e-03], # [-1.6038e-03, -3.2562e-03, -8.1033e-04, ..., 4.3648e-03, # -2.4421e-04, -4.4432e-03], # [-2.4065e-03, -3.8717e-03, -4.4748e-04, ..., 3.1679e-03, # 5.7510e-05, -3.3277e-03]]], device=\u0026#39;cuda:3\u0026#39;, # grad_fn=\u0026lt;ViewBackward0\u0026gt;) # torch.Size([1, 10, 49152]) 以上内容实际上是MultiHeadAttentionWrapper更高效版本 在上面的实现中，我们添加了一个投影层，这不是必须的，在LLM中只是一种标准惯例（最新的研究表明，它可以被移除而不会影响建模性能） 我们可以简单计算一下参数量 def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) print(count_parameters(mha)) print(3*12*4096*4096 + 4096*12*12*4096 + 4096*12) # 3019948032 # 3019948032 第一项是三个权重矩阵的参数（d_in $\\cdot$ d_out），第二项是投影层的权重（d_out $\\cdot$ d_out），第三项是投影层的bias（d_out） ","permalink":"http://localhost:1313/posts/attetion_code/","summary":"\u003cp\u003e为了方便讲解，我们先对文本进行tokenize和embedding。我们仅在这一步使用了除torch以外的库\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etext \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#48b685\"\u003e\u0026#34;This is your first step to understanding LLM\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekwargs \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#48b685\"\u003e\u0026#34;device_map\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#48b685\"\u003e\u0026#34;cuda:3\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etokenizer \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e AutoTokenizer\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003efrom_pretrained(\u003cspan style=\"color:#48b685\"\u003e\u0026#34;llama3/Meta-Llama-3-8B\u0026#34;\u003c/span\u003e, use_fast\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#815ba4\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e AutoModelForCausalLM\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003efrom_pretrained(\u003cspan style=\"color:#48b685\"\u003e\u0026#34;llama3/Meta-Llama-3-8B\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#5bc4bf\"\u003e**\u003c/span\u003ekwargs)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003einputs \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e tokenizer(text, return_tensors\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#48b685\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003einputs \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e {key: value\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eto(kwargs[\u003cspan style=\"color:#48b685\"\u003e\u0026#34;device_map\u0026#34;\u003c/span\u003e]) \u003cspan style=\"color:#815ba4\"\u003efor\u003c/span\u003e key, value \u003cspan style=\"color:#5bc4bf\"\u003ein\u003c/span\u003e inputs\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eitems()}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003einputs \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003eget_input_embeddings()(inputs[\u003cspan style=\"color:#48b685\"\u003e\u0026#34;input_ids\u0026#34;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pr","title":"手撕Multihead Attention"},{"content":"在使用VSCode调试PyTorch多卡程序时，我们经常会发现启动脚本的参数很多，因此配置~/.vscode/launch.json时体验不佳。如果你也遇到了上面的问题，可以考虑本文的方法，以更优雅的方式调试PyTorch多卡程序。本文以调试LLaVA的训练代码为例。\n安装 debugpy pip install debugpy 写配置 首先需要配置的还是~/.vscode/launch.json，但与将参数列表填入launch.json文件的args列表不同。本文的方法无需在每次调试时都更改args列表，只需更改你原始的脚本文件。\n我们只需在launch.json文件添加一个新的配置，配置如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;sh_file_debug\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;debugpy\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;justMyCode\u0026#34;: false, \u0026#34;connect\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5678 } } 下一步在我们要运行的脚本文件待执行的文件中添加部分代码即可，下图是LLaVA的预训练脚本：\nLLaVA pertrain.sh 可以看到我们待执行的文件是llava/train/train_mem.py，之后我们进入这个文件，在这个文件的最前面添加如下代码：\nimport debugpy try: debugpy.listen((\u0026#34;localhost\u0026#34;, 5678)) print(\u0026#34;Waiting for debugger attach\u0026#34;) debugpy.wait_for_client() debugpy.breakpoint() except Exception as e: pass 需要注意端口号需要保持一致，这里使用的是vscode默认的远程调试端口号，下面展示了添加完成后的trian_mem.py\n添加配置后的train_mem.py 至此，所有的配置都已经完成了。\nDebug 接下来，我们可以进行Debug，步骤也很简单：\n在想要调试的地方，打上断点，例如我想看LazySupervisedDataset的__getitem__()的步骤，我在697行打下了断点 之后我们在服务器上正常运行原始的脚本文件，等待片刻后，会看到打印出了Waiting for debugger attach 看到打印出的结果后，需要来到VScode的调试页面，运行我们刚刚配置的sh_file_debug 此时可以看到我们的VScode运行停止在train_mem.py，这是因为，我们在上面调用了debugpy.breakpoint()，如果不喜欢可以注释掉。 接下来就与正常debug相同，按下F5就行，可以看到停在了我们刚刚打上断点的第697行，在左侧的调试页面也可以看到数据的信息 在不需要调试时，记住要将待执行文件中的debug代码注释掉！\n在不需要调试时，记住要将待执行文件中的debug代码注释掉！\n在不需要调试时，记住要将待执行文件中的debug代码注释掉！\n","permalink":"http://localhost:1313/posts/vscode_debug/","summary":"\u003cp\u003e在使用VSCode调试PyTorch多卡程序时，我们经常会发现启动脚本的参数很多，因此配置\u003ccode\u003e~/.vscode/launch.json\u003c/code\u003e时体验不佳。如果你也遇到了上面的问题，可以考虑本文的方法，以更优雅的方式调试PyTorch多卡程序。本文以调试LLaVA的训练代码为例。\u003c/p\u003e\n\u003ch1 id=\"安装-debugpy\"\u003e安装 debugpy\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epip install debugpy\n\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"写配置\"\u003e写配置\u003c/h1\u003e\n\u003cp\u003e首先需要配置的还是\u003ccode\u003e~/.vscode/launch.json\u003c/code\u003e，但与将参数列表填入\u003ccode\u003elaunch.json\u003c/code\u003e文件的\u003ccode\u003eargs\u003c/code\u003e列表不同。本文的方法无需在每次调试时都更改\u003ccode\u003eargs\u003c/code\u003e列表，只需更改你原始的脚本文件。\u003c/p\u003e\n\u003cp\u003e我们只需在\u003ccode\u003elaunch.json\u003c/code\u003e文件添加一个新的配置，配置如下：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\n    \u0026#34;name\u0026#34;: \u0026#34;sh_file_debug\u0026#34;,\n    \u0026#34;type\u0026#34;: \u0026#34;debugpy\u0026#34;,\n    \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;,\n    \u0026#34;justMyCode\u0026#34;: false,\n    \u0026#34;connect\u0026#34;: {\n        \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;,\n        \u0026#34;port\u0026#34;: 5678\n    }\n}\n\u003c/code\u003e\u003c/pr","title":"如何使用VSCode优雅地调试PyTorch多卡程序"},{"content":"本文参考官方教程介绍如何训练LLaVA v1.5 多模态模型。值得注意的是本文将使用LLama3-8B-Instruct代替LLaVA v1.5中的Vicuna。如果还不熟悉LLaVA，请先阅读前面三篇文章：从零开始学LLaVA 01，从零开始学LLaVA 02和从零开始学LLaVA 03\n","permalink":"http://localhost:1313/posts/llava_04/","summary":"\u003cp\u003e本文参考\u003ca href=\"https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#train\"\u003e官方教程\u003c/a\u003e介绍如何训练LLaVA v1.5 多模态模型。值得注意的是本文将使用LLama3-8B-Instruct代替LLaVA v1.5中的Vicuna。如果还不熟悉LLaVA，请先阅读前面三篇文章：\u003ca href=\"https://attract666.github.io/posts/llava_01/\"\u003e从零开始学LLaVA 01\u003c/a\u003e，\u003ca href=\"https://attract666.github.io/posts/llava_02/\"\u003e从零开始学LLaVA 02\u003c/a\u003e和\u003ca href=\"https://attract666.github.io/posts/llava_03/\"\u003e从零开始学LLaVA 03\u003c/a\u003e\u003c/p\u003e","title":"从零开始学LLaVA 04"},{"content":"本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在LLaVA/scripts/v1_5/pretrain.sh，视觉指令微调启动的脚本在LLaVA/scripts/v1_5/finetune.sh。两个脚本文件执行的都是LLaVA/llava/train/train_mem.py，在LLaVA/llava/train目录下还有一个train.py，这个才是真正的总体路口，不同点在于是否使用flash_attention_2。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。\n模型部分 在LLaVA/llava/train/train.py的第827行会出现初始化模型的代码\nmodel = LlavaLlamaForCausalLM.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, attn_implementation=attn_implementation, torch_dtype=(torch.bfloat16 if training_args.bf16 else None), **bnb_model_from_pretrained_args) 根据模型路径加载模型。LlavaLlamaForCausalLM类在我们介绍推理时也曾遇见过，当时我们是直接分析它的generate()方法，现在让我们再详细分析下这个类。\n首先，LlavaLlamaForCausalLM继承了LlamaForCausalLM和LlavaMetaForCausalLM两个类。其中LlamaForCausalLM就是transformers中的实现，而LlavaMetaForCausalLM就是我们之前在推理过程中分析的那个。成员变量中包括了用于处理输入提取特征的model和用于预测输出token的lm_head。由于输入中添加了图像，所以重写了LlamaForCausalLM中的forward()和generate()方法 其中用于输入提取的特征提取器是LLavaLlamaModel的实例。LlavaLlamaModel是一个抽象的组合类，继承了LlavaMetaModel和LlamaModel。组合文本和图像在进入llm之前的特征处理过程。LlavaMetaModel控制vision branch功能函数来完成细节的图像特征提取，而LlamaModel就是正常的LLM的通用功能 总的说来LlavaLlamaForCausalLM用于负责总体逻辑统筹，具体的文本侧和视觉侧则交给LlamaModel和LlavaMetaModel 其中比较关键的在于需要自己实现的LlavaMetaModel和LlavaMetaForCausalLM LlavaMetaForCausalLM 这个是我们的老朋友了，在第二章的推理部分我们也分析过它的prepare_inputs_labels_for_multimodal()方法。\nclass LlavaMetaForCausalLM(ABC): @abstractmethod def get_model(self):... def get_vision_tower(self):... def encode_images(self, images):... def prepare_inputs_labels_for_multimodal( self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_size=None):... def initialize_vision_tokenizer(self, model_args, tokenizer):... 首先第一个方法是get_model()，这是一个抽象方法，子类实现了该抽象方法才能被实例化，基类不能实例化。所以可以在LlavaLlamaForCausalLM类中可以看到重新实现了get_model()，就是返回模型。\n之后是get_vision_tower()，也很简单，返回model的visual encoder。\nencode_images()就是提取图像特征，这里的图像特征是经过visual encoder和投影层之后的特征。\nprepare_inputs_labels_for_multimodal()由于之前介绍过推理时的逻辑，所以这里暂时不展开说，在后面的前向传播时，我们会重新看到它作为训练时的逻辑。\n最后一个方法是initialize_vision_tokenizer()，主要用于初始化视觉模型的tokenizer。\nLlavaMetaModel class LlavaMetaModel: def __init__(self, config):... def get_vision_tower(self):... def initialize_vision_modules(self, model_args, fsdp=None):... 先看初始化__init__()很简单，就是构建visual encoder和projector，值得注意的是，在训练时我们的脚本文件是没有显式指定参数mm_vision_tower的，所以visual encoder和projecto并不是在一开始就初始化好的，而是在LLaVA/llava/train/train.py的第911行通过调用第三个方法进行构建的。\nget_vision_tower()就是返回visual encoder。\ninitialize_vision_modules()才是训练阶段真正构建visual encoder和projector的地方。在这里才添加了参数mm_vision_tower。这里的参数mm_vision_select_layer是-1也就是默认选择最后一层，当然在官方提供的脚本文件中是指定为-2了。\n具体的visual encoder和projector的实现对应于LLaVA/llava/model/multimodal_encoder/builder.py和LLaVA/llava/model/multimodal_projector/builder.py中，对应于其中调用的类。如果想要更换LLaVA的visual encoder和projector可以去对应builder.py文件下import对应的class以及实例化，并在相应的文件夹下实现对应的class\n数据部分 在LLaVA/llava/train/train.py的第959行会出现关于数据集调用的语句\ndata_module = make_surpervised_data_module(tokenizer=tokenizer, data_args=data_args) make_surpervised_data_module()方法在LLaVA/llava/train/train.py的第776行\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Make dataset and collator for supervised fine-tuning.\u0026#34;\u0026#34;\u0026#34; train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator) 这里分别数据集和整理器，我们一一来看\ndataset class LazySupervisedDataset(Dataset): def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, data_args: DataArguments):... def __len__(self):... @property def lengths(self):... @property def modality_lengths(self):... def __getitem__(self, i) -\u0026gt; Dict[str, torch.Tensor]:... 首先作为一个Dataset类，老三样一定少不了。__init__()就是一些初始化，这里也可以看到LLaVA训练过程中用到的是json格式的数据。__len__()就是返回数据的长度。\n关键在于这个__getitem__()，它负责从data_list中按照data_sampler采样，得到样本序号，取出数据并进行处理，以得到模型需要的输入。可以看到稍后返回的是一个字典样式的数据。\n首先判断当前数据项sources[0]中是否包含image 读取图片并对图片进行预处理，判断image_aspect_ratio是什么方式 pad方式，会对图片根据最长边填充为正方形，具体的方法在expand2square()。pad的值为每个channel预处理操作对应的mean，会保证原始的图像内容放置在正中心。这样保持了图像的长宽比例不变（1:1），这样不会导致图像在某一维度上的失真或变形 通过preprocess_multimodal对文本进行预处理，判断is_multimodal 遍历每一个样本的conversation，如果一次对话（即conversation中的一个字典{from, value}条目）中的value有\u0026lt;image\u0026gt;token，就把\u0026lt;image\u0026gt;token插入到新的行前，如果使用图像开始结束token，则添加图像开始结束token 通过preprocess()进一步将conversations list 转化为一段以正常文本形式展现的完整对话，图像仍为占位符形式，该过程借助conversation_lib 将conversations list 添加模板 对conversations进行tokenize，这个就是input_ids 获取targets，只需用上述input_ids进行clone（next-token prediction常规操作） 当我们需要更换LLaVA的LLM时，主要就是要更改这里的preprocess_xxx模板\n剩余的两个方法均被@property装饰，其中lengths用来统计数据集中每个样本的长度（如果包含图像则+128）。modality_lengths用来统计每个样本长度（不包含图像token的长度），不同之处在于返回的信息会区分是否存在图像，为正则样本包含图像，为负样本不包含图像。\ndata_collator @dataclass class DataCollatorForSupervisedDataset(object): tokenizer: transformers.PreTraineTokenizer def __call__(self, instances: Sequence[Dict]) -\u0026gt; Dict[str, torch.Tensor]:... 其输入instances为上述Dataset定义的若干个dict输出组成的Sequence。从每个dict中取出对应的input_ids和label，同时pad到相同长度，最后组合成一个batch的形式，图像也会放入batch中。\n前向传播 Pretrain时的loss计算 了解LLaVA的loss是如何计算的前提，是要了解LLaVA的labels对哪些内容进行mask了。在接下来的部分，我们会更改LLaVA提供的原始Pretrain脚本中的默认conversation模板，因为如果使用原始脚本中的Plain提供的信息太少了，它不提供system模板。我们将使用conv_vicuna_v1作为conversation模板，对应的处理函数在LLaVA/llava/train/train.py的414行preprocess_v1()。\nconv_vicuna_v1对应的conversation模板如下：\nconv_vicuna_v1 = Conversation( system=\u0026#34;A chat between a curious user and an artificial intelligence assistant. \u0026#34; \u0026#34;The assistant gives helpful, detailed, and polite answers to the user\u0026#39;s questions.\u0026#34;, roles=(\u0026#34;USER\u0026#34;, \u0026#34;ASSISTANT\u0026#34;), version=\u0026#34;v1\u0026#34;, messages=(), offset=0, sep_style=SeparatorStyle.TWO, sep=\u0026#34; \u0026#34;, sep2=\u0026#34;\u0026lt;/s\u0026gt;\u0026#34;, ) 首先，假设我们的sources是如下格式的：\n[{\u0026#39;from\u0026#39;: \u0026#39;human\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;\u0026lt;image\u0026gt;\\nGive a brief description of the image.\u0026#39;}, {\u0026#39;from\u0026#39;: \u0026#39;gpt\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;floor plan app floorplugged house and home\u0026#39;}] 先会对sources使用模板，会得到如下格式：\n[\u0026#34;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\u0026#39;s questions. USER: \u0026lt;image\u0026gt;\\nGive a brief description of the image. ASSISTANT: floor plan app floorplugged house and home\u0026lt;/s\u0026gt;\u0026#34;] 注意只有一次对话结束后，才会插入sep2=\u0026lt;/s\u0026gt;，system prompt与对话之间使用sep连接。\n接下来对sources进行tokenize\ntensor([[ 1, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, -200, 29871, 13, 29954, 573, 263, 11473, 6139, 310, 278, 1967, 29889, 319, 1799, 9047, 13566, 29901, 11904, 3814, 623, 11904, 572, 688, 3192, 3699, 322, 3271, 2]]) 其中-200就是\u0026lt;image\u0026gt;token的位置，targets就是inputs_ids直接clone得到的\nmask后的targets为：\ntensor([ -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11904, 3814, 623, 11904, 572, 688, 3192, 3699, 322, 3271, 2]) 从上面的过程中，我们可以看到在PreTrain过程中mask掉的部分system prompt、Instruction和pad部分。\n在实际计算loss时，会调用transformers.Trainer的compute_loss()方法，也就意味着如果你需要更改计算loss的方式，需要在LLaVATrainer中重写compute_loss方法。\n而原始的compute_loss()方法，最后会在transformers.LlamaForCausalLM的forward()中进行计算，具体的代码为：\nloss = None if labels is not None: # Shift so that tokens \u0026lt; n predict n shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() # Flatten the tokens loss_fct = CrossEntropyLoss() shift_logits = shift_logits.view(-1, self.config.vocab_size) shift_labels = shift_labels.view(-1) # Enable model parallelism shift_labels = shift_labels.to(shift_logits.device) loss = loss_fct(shift_logits, shift_labels) 参考 [1] Github:LLaVA(https://github.com/haotian-liu/LLaVA)\n[2] Huggingface:Trainer(https://huggingface.co/docs/transformers/main_classes/trainer)\n","permalink":"http://localhost:1313/posts/llava_03/","summary":"\u003cp\u003e本文将详细介绍与LLaVA v1.5训练相关的代码。LLaVA训练包括两个阶段，分别是特征对齐和视觉指令微调。特征对齐启动的脚本在\u003ccode\u003eLLaVA/scripts/v1_5/pretrain.sh\u003c/code\u003e，视觉指令微调启动的脚本在\u003ccode\u003eLLaVA/scripts/v1_5/finetune.sh\u003c/code\u003e。两个脚本文件执行的都是\u003ccode\u003eLLaVA/llava/train/train_mem.py\u003c/code\u003e，在\u003ccode\u003eLLaVA/llava/train\u003c/code\u003e目录下还有一个\u003ccode\u003etrain.py\u003c/code\u003e，这个才是真正的总体路口，不同点在于是否使用\u003ccode\u003eflash_attention_2\u003c/code\u003e。阅读的建议是先找到总体思路，在对照各个部分的代码段分块理解。\u003c/p\u003e\n\u003ch1 id=\"模型部分\"\u003e模型部分\u003c/h1\u003e\n\u003cp\u003e在\u003ccode\u003eLLaVA/llava/train/train.py\u003c/code\u003e的第827行会出现初始化模型的代码\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#2f1e2e;background-color:#e7e9db;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e LlavaLlamaForCausalLM\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                model_args\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003emodel_name_or_path,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cache_dir\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003etraining_args\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003ecache_dir,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                attn_implementation\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003eattn_implementation,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                torch_dtype\u003cspan style=\"color:#5bc4bf\"\u003e=\u003c/span\u003e(torch\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003ebfloat16 \u003cspan style=\"color:#815ba4\"\u003eif\u003c/span\u003e training_args\u003cspan style=\"color:#5bc4bf\"\u003e.\u003c/span\u003ebf16 \u003cspan style=\"color:#815ba4\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#815ba4\"\u003eNone\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#5bc4bf\"\u003e**\u003c/span\u003ebnb_model_from_pretrained_args)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pr","title":"从零开始学LLaVA 03"},{"content":"LLaVA-1.5悟道 LLaVA-1.5在LLaVA的基础上做了一小部分修改。模型上把之前的线性层换成了MLP，visual encoder从ViT-L-14换成了ViT-L-14-336px，数据集进行了扩充（Visual Instruction Tuning从158K涨到了665K）\n论文贡献 论文主要贡献：\n验证了LLaVA架构的高效性：使用最少的计算量和数据达到了更优的性能 简单修改了LLaVA的架构 在prompt中明确回复格式以平衡模型的长短回复 分析了分辨率、数据规模和LLM大小对性能的影响 网络架构 整体架构上没有太大的改变，如下图所示：\nLLaVA v1.5.(Image source:LLaVA v1.5) 很重要的一点，是作者使用了AnyRes策略，来拓展视觉的分辨率（因为CLIP支持的最大分辨率是336px）\nLLaVA v1.5 HD.(Image source:LLaVA v1.5) AnyRes策略的具体步骤：\n将高分辨率图像切分成块，每块的大小取决于visual encoder能处理的分辨率大小。visual encoder单独处理每一块 将高分辨图像resize成visual encoder可以处理的大小进行编码 将上面两步的结果拼接起来一起作为视觉特征 在LLM中，模型的幻觉归因于训练数据集中的错误或幻觉。作者认为输入分辨率不足以让模型识别数据中的所有细节，模型就会产生幻觉。\n数据集规模 Instruction-following Data.(Image source:LLaVA v1.5) LLaVA-NEXT 与LLaVA-1.5相比，主要有以下提升：\n将输入图像分辨率提高 4 倍，使得它能够捕捉更多的视觉细节。它支持 3 种不同比例的分辨率，分别是 672x672、336x1344、1344x336 改变视觉指令微调数据的比例，实现更好的视觉推理和 OCR 能力 更好的视觉对话，适用于更多场景，涵盖不同应用 使用 SGLang 实现高效部署和推理 构造多模态数据 常言道：“工欲善其事，必先利其器。”LLaVA这篇论文的另一个核心贡献就是在于将图像文本对转换成了多模态instruction-following数据。LLaVA构造了三种多模态instruction-following数据：\n对话（Conversation） 细节描述（Detailed description） 复杂推理（Complex reasoning） Conversation构造 构造的思路如下：\n给出一个system prompt，要求纯文本大模型根据图片的描述生成多轮对话 提供几个示例，便于纯文本大模型理解以及生成类似的多轮对话 将上面的内容输入给纯文本大模型，纯文本大模型返回多轮对话 system prompt和示例的内容可以见LLaVA/playground/data/prompts，在LLaVA论文中也给出了构造数据的示例代码：\nSample Codes.(Image source:LLaVA) 我这里由于没有gpt4的api，所以决定用llama3来代替，使用了vllm框架\n具体的代码实现如下：\n# 先启动服务 # python3 -m vllm.entrypoints.openai.api_server --model ./Meta-Llama-3-8B-Instruct --dtype auto --api-key 123456 from openai import OpenAI openai_api_key = \u0026#34;123456\u0026#34; # Same as --api-key in the deployment command openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; client = OpenAI(api_key=openai_api_key, base_url=openai_api_base) model = \u0026#39;./Meta-Llama-3-8B-Instruct\u0026#39; SYSTEM_CONTENT = \u0026#34;\u0026#34;\u0026#34; You are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary. \u0026#34;\u0026#34;\u0026#34; SAMPLE_CONTENT_1 = \u0026#34;\u0026#34;\u0026#34; There is a movie theater that displays the show times above the doors. A red fire hydrant is deep in the snow. The fire hydrant is in the snow near a recently plowed sidewalk. This city has had a very hard winter with snow. A hotel for dogs in the snow in winter.\u0026#34;\u0026#34;\u0026#34; SAMPLE_RESPONSE_1 = \u0026#34;\u0026#34;\u0026#34; Question: What color is the fire hydrant in the image? === Answer: The fire hydrant in the image is red. === Question: Is the fire hydrant fully covered by snow or partially visible? === Answer: The fire hydrant is partially visible, with a significant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the fire hydrant? === Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. === Question: What does the image suggest about the winter season in the city? === Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. For instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city\u0026#39;s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts. Furthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages. In conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure. \u0026#34;\u0026#34;\u0026#34; SAMPLE_CONTENT_2 = \u0026#34;\u0026#34;\u0026#34; A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range. A skier looks at mountains while standing near a trail sign. a single person skiing by an area with a lot of bushes \u0026#34;\u0026#34;\u0026#34; SAMPLE_RESPONSE_2 = \u0026#34;\u0026#34;\u0026#34; Question: What is the skier doing? === Answer: The skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing near a trail sign, which indicates their location on the mountain. === Question: Are there any other people or skiers visible in the image? === Answer: No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. === Question: What kind of skiing is the person doing? Cross-country or downhill? === Answer: Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that the skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts, whereas cross-country skiing often occurs on relatively flatter terrain with marked trails. === Question: Can you explain cross-country and downhill skiing a bit more? === Answer: Cross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences. 1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. There are two main techniques in cross-country skiing: - Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward. - Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It requires a smooth, groomed surface and is typically faster than the classic technique. Cross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used in downhill skiing, allowing for better ankle movement and control. 2. Downhill Skiing: Also known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and even jumps. Downhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing. Each discipline has its own set of rules, courses, and ski equipment. Downhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The boots are more rigid to provide better support and control during high-speed descents and sharp turns. In summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants. \u0026#34;\u0026#34;\u0026#34; messages = [ { \u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: SYSTEM_CONTENT, } ] fewshot_samples = [ { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_1, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_1, }, { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_2, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_2, } ] for sample in fewshot_samples: messages.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;context\u0026#39;]}) messages.append({\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;response\u0026#39;]}) query = \u0026#34;\u0026#34;\u0026#34; A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. \u0026#34;\u0026#34;\u0026#34; messages.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}) response = client.chat.completions.create(model=model, messages=messages).choices[0].message.content print(response) 使用Llama3-8B-Instruct生成的问答数据如下所示：\nQuestion: What type of vehicle is being packed with luggage? === Answer: The vehicles being packed with luggage are a black SUV (sport utility vehicle) and a van. === Question: Where is the luggage being packed into the vehicle(s)? === Answer: The luggage is being packed into the vehicles in an underground parking area, as well as in a public garage. === Question: How many people are trying to fit their luggage into the vehicle? === Answer: It appears that there is a group of people trying to fit their luggage into the SUV, but the exact number is not clear from the image. === Question: What is the purpose of the people packing their luggage into the vehicle(s)? === Answer: Based on the context of the image, it appears that the people are packing their luggage into the vehicles in preparation for a trip. The luggage and the vehicles suggest that they are likely going on a vacation or a long journey, and the underground parking area and public garage suggest that they are preparing to leave from a city or urban area. === Question: Can you explain the importance of underground parking and public garages in urban areas? === Answer: Yes, certainly! Underground parking and public garages play a crucial role in urban areas, especially in cities with limited street parking and high population density. These structures provide a convenient and secure way for residents and visitors to park their vehicles, reducing the need for on-street parking and minimizing the visual impact of parked cars on the urban landscape. Underground parking garages, in particular, offer a range of benefits, including: * Protection from the elements: Cars are shielded from inclement weather, reducing the risk of damage and making it easier to maintain the vehicles. * Security: Underground parking garages often have controlled access, surveillance cameras, and secure entry/exit points, providing a safer environment for parked vehicles and their occupants. * Convenience: Underground parking garages are often located near public transportation hubs, commercial centers, and residential areas, making it easier for people to park and access their destinations. * Urban planning: Underground parking garages can help to reduce the visual impact of parking on urban streets, freeing up surface areas for other uses and helping to create more pedestrian-friendly and aesthetically pleasing public spaces. In the context of the image, the underground parking area and public garage suggest that the people are preparing to leave from a city or urban area, and the presence of luggage and vehicles confirms that they are embarking on a trip. Detailed description构造 构造的方式与上面基本相同，只是此处使用到了论文提到的锚框\nfrom openai import OpenAI openai_api_key = \u0026#34;123456\u0026#34; # Same as --api-key in the deployment command openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; client = OpenAI(api_key=openai_api_key, base_url=openai_api_base) model = \u0026#39;./Meta-Llama-3-8B-Instruct\u0026#39; system_content_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/system_message.txt\u0026#39; sample_content_0_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/000_caps.txt\u0026#39; sample_response_0_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/000_conv.txt\u0026#39; sample_content_1_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/001_caps.txt\u0026#39; sample_response_1_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/001_conv.txt\u0026#39; sample_content_2_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/002_caps.txt\u0026#39; sample_response_2_path = \u0026#39;LLaVA/playground/data/prompts/detail_description/002_conv.txt\u0026#39; with open(system_content_path, \u0026#39;r\u0026#39;) as f: SYSTEM_CONTENT = f.read() with open(sample_content_0_path, \u0026#39;r\u0026#39;) as f: SAMPLE_CONTENT_0 = f.read() with open(sample_response_0_path, \u0026#39;r\u0026#39;) as f: SAMPLE_RESPONSE_0 = f.read() with open(sample_content_1_path, \u0026#39;r\u0026#39;) as f: SAMPLE_CONTENT_1 = f.read() with open(sample_response_1_path, \u0026#39;r\u0026#39;) as f: SAMPLE_RESPONSE_1 = f.read() with open(sample_content_2_path, \u0026#39;r\u0026#39;) as f: SAMPLE_CONTENT_2 = f.read() with open(sample_response_2_path, \u0026#39;r\u0026#39;) as f: SAMPLE_RESPONSE_2 = f.read() message = [ { \u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: SYSTEM_CONTENT, } ] fewshot_samples = [ { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_0, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_0, }, { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_1, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_1, }, { \u0026#39;context\u0026#39;: SAMPLE_CONTENT_2, \u0026#39;response\u0026#39;: SAMPLE_RESPONSE_2, } ] for sample in fewshot_samples: message.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;context\u0026#39;]}) message.append({\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: sample[\u0026#39;response\u0026#39;]}) query = \u0026#34;\u0026#34;\u0026#34; A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. person: [0.681, 0.242, 0.774, 0.694] person: [0.63, 0.222, 0.686, 0.516] person: [0.444, 0.233, 0.487, 0.34] backpack: [0.384, 0.696, 0.485, 0.914] backpack: [0.755, 0.413, 0.846, 0.692] suitcase: [0.758, 0.413, 0.845, 0.69] suitcase: [0.1, 0.497, 0.173, 0.579] bicycle: [0.282, 0.363, 0.327, 0.442] car: [0.786, 0.25, 0.848, 0.322] car: [0.783, 0.27, 0.827, 0.335] car: [0.86, 0.254, 0.891, 0.3] car: [0.261, 0.101, 0.787, 0.626] \u0026#34;\u0026#34;\u0026#34; message.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: query}) response = client.chat.completions.create(model=model, messages=message).choices[0].message.content print(response) Llama3-8B-Instruct生成的数据如下：\nThis image shows a group of people standing outside a black vehicle, likely an SUV, in an underground parking area. The vehicle is being packed for a trip, with various luggage items scattered around it. There are multiple backpacks and suitcases, with one suitcase sitting near the front of the vehicle and another leaning against the side. The people in the image are attempting to fit all of their luggage into the SUV, with one person standing close to the left side of the vehicle, another near the middle, and a third person on the right side. A bicycle can be seen parked nearby, possibly belonging to one of the individuals. The parking area is dimly lit, with the only light source coming from the fluorescent lights above. The walls of the garage are made of concrete, and there is a chair or bench in the background. Complex reasonging构造 这部分的构造和Detailed description的代码完全相同，我们直接展示Llama3-8B-Instruct生成的结果：\nQuestion: What might be the purpose of this group of people and their luggage? === Answer: Based on the image, it appears that this group of people is preparing for a trip. The various luggage, including suitcases, backpacks, and possibly even a bicycle, suggests that they are packing for a journey. The fact that they are surrounded by luggage and trying to fit everything into an SUV or van also implies that they are getting ready to depart. The presence of a public garage, with multiple vehicles parked, further supports this conclusion. It is likely that this group is planning a trip, possibly a family vacation or a business trip, and they are busy packing and preparing for their departure. 处理数据 我们已经可以通过Llama3来根据query生成对应的response，但这种格式并不适合大批量训练，我们需要将response整理成训练格式\nLLaVA的训练格式可以通过huggingface上开源的LLaVA-Instruct-150K看到，所有样本以列表的形式保存在一个json文件中，其中每个样本是一个字典，包含三个字段：\nid：全局唯一的字符串 image：图片的路径 conversations：上面的对话 代码的实现如下：\nimport os image_path = \u0026#34;path/to/123456.jpg\u0026#34; image_path = os.path.basename(image_path) id = os.path.splitext(image_path)[0] conversations = [] response_lines = response.split(\u0026#34;===\u0026#34;) for line in response_lines: line = line.strip() if line.startswith(\u0026#34;Question:\u0026#34;): question = line.replace(\u0026#34;Question:\u0026#34;, \u0026#34;\u0026#34;).strip() conversations.append({\u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: question}) elif line.startswith(\u0026#34;Answer:\u0026#34;): answer = line.replace(\u0026#34;Answer:\u0026#34;, \u0026#34;\u0026#34;).strip() conversations.append({\u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: answer}) conversation_data = { \u0026#34;id\u0026#34;: id, \u0026#34;image\u0026#34;: image_path, \u0026#34;conversations\u0026#34;: conversations } import json new_json = f\u0026#34;/path/to/conversation.json\u0026#34; with open(new_json, \u0026#34;w\u0026#34;) as f: json.dump([conversation_data], f, indent=4) 构造完成过后的数据如下所示：\n[ { \u0026#34;id\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;123456.jpg\u0026#34;, \u0026#34;conversations\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;What type of vehicle is being packed with luggage?\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;The vehicle being packed with luggage appears to be a sport utility vehicle (SUV).\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Where is the SUV being packed with luggage?\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;The SUV is being packed with luggage in a public garage, likely an underground parking area.\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Are all the people shown in the image packing the luggage into the SUV?\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;No, it appears that there are multiple people trying to fit all of their luggage into the SUV, suggesting that there may be a group of travelers preparing for a trip.\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Can you describe the scene and the people\u0026#39;s actions?\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;The scene depicts a chaotic and busy atmosphere, with multiple people attempting to pack and fit their luggage into the SUV. The luggage surrounds the vehicle, and some people are trying to squeeze more bags into the already-packed vehicle. It appears that the group is in a rush to prepare for a trip and is working together to get everything loaded.\\n\\nThe people in the image are likely a group of travelers, possibly a family or friends, who are preparing for a trip. They are all working together to fit their luggage into the SUV, which suggests a sense of teamwork and cooperation. The scene is quite busy and potentially stressful, as the group is trying to pack a large amount of luggage into a relatively small vehicle.\\n\\nOverall, the image conveys a sense of chaos and busyness, but also a sense of camaraderie and teamwork as the group works together to prepare for their trip.\u0026#34; } ] } ] 参考 [1] Paper:Improved Baselines with Visual Instruction Tuning(https://arxiv.org/abs/2310.03744)\n[2] Blog:LLaVA-NeXT(https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n[3] Zhihu:HelloWorld-构造多模态数据(https://zhuanlan.zhihu.com/p/696831022)\n[4] Zhihu:HelloWorld-LLaVA1.5论文解读(https://zhuanlan.zhihu.com/p/696402890)\n","permalink":"http://localhost:1313/posts/llava_02/","summary":"\u003ch1 id=\"llava-15悟道\"\u003eLLaVA-1.5悟道\u003c/h1\u003e\n\u003cp\u003eLLaVA-1.5在LLaVA的基础上做了一小部分修改。模型上把之前的线性层换成了MLP，visual encoder从ViT-L-14换成了ViT-L-14-336px，数据集进行了扩充（Visual Instruction Tuning从158K涨到了665K）\u003c/p\u003e\n\u003ch2 id=\"论文贡献\"\u003e论文贡献\u003c/h2\u003e\n\u003cp\u003e论文主要贡献：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e验证了LLaVA架构的高效性：使用最少的计算量和数据达到了更优的性能\u003c/li\u003e\n\u003cli\u003e简单修改了LLaVA的架构\u003c/li\u003e\n\u003cli\u003e在prompt中明确回复格式以平衡模型的长短回复\u003c/li\u003e\n\u003cli\u003e分析了分辨率、数据规模和LLM大小对性能的影响\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"网络架构\"\u003e网络架构\u003c/h2\u003e\n\u003cp\u003e整体架构上没有太大的改变，如下图所示：\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_02/LLaVA-1.5.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eLLaVA v1.5.(Image source:\u003ca href=\"https://arxiv.org/abs/2310.03744\"\u003eLLaVA v1.5\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e很重要的一点，是作者使用了AnyRes策略，来拓展视觉的分辨率（因为CLIP支持的最大分辨率是336px）\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_02/AnyRes.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eLLaVA v1.5 HD.(Image source:\u003ca href=\"https://arxiv.org/abs/2310.03744\"\u003eLLaVA v1.5\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003eAnyRes策略的具体步骤：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将高分辨率图像切分成块，每块的大小取决于visual encoder能处理的分辨率大小。visual encoder单独处理每一块\u003c/li\u003e\n\u003cli\u003e将高分辨图像resize成visual encoder可以处理的大小进行编码\u003c/li\u003e\n\u003cli\u003e将上面两步的结果拼接起来一起作为视觉特征\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在LLM中，模型的幻觉归因于训练数据集中的错误或幻觉。作者认为输入分辨率不足以让模型识别数据中的所有细节，模型就会产生幻觉。\u003c/p\u003e\n\u003ch2 id=\"数据集规模\"\u003e数据集规模\u003c/h2\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_02/Instruction data.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eInstruction-following Data.(Image source:\u003ca href=\"https://arxiv.org/abs/2310.03744\"\u003eLLaVA v1.5\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003chr\u003e\n\u003ch1 id=\"llava-next\"\u003eLLaVA-NEXT\u003c/h1\u003e\n\u003cp\u003e与LLaVA-1.5相比，主要有以下提升：\u003c/p\u003e","title":"从零开始学LLaVA 02"},{"content":"从LLaVA-1.0论文开始 LLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\nPaper link: https://arxiv.org/abs/2304.08485\nGithub: https://github.com/haotian-liu/LLaVA\n论文亮点 多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类 多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space 网络架构 结构上vision encoder使用的是CLIP-L/14（使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征）进行编码，之后使用一个投影层（全连接层）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\nLLaVA Architecture.(Image source:LLaVA)\r数据集构成及数据收集细节 训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\n预训练数据集（CC3M过滤）：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\nCC3M Filtered.(Image source:LLaVA)\r将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\nCC3M image description.(Image source:LLaVA)\r指令格式如下：\nCC3M instructions.(Image source:LLaVA)\r指令微调数据集：使用COCO数据集，将图像描述、bbox、类别等信息输入给GPT4-text-only生成对话、细节描述和复杂推理三种类型instruction-following数据。最终生成58K对话、23K细节描述和77K复杂推理。\n对话：对图像内容进行多轮回答，且只考虑有明确答案的问题，通过这样的方式保证答案的质量 细节描述：使用另外一些描述的提示，随机让GPT4-text-only生成详细描述 复杂推理：上面都是视觉本身内容，设计了一些需要深度推理的问题，答案要有严格的逻辑 训练方式 训练的目标就是最大化似然概率\nTrain traget.(Image source:LLaVA)\r其中\nInstruction.(Image source:LLaVA)\r在计算自回归模型的loss时，只考虑回答$X_a$和\u0026lt;STOP\u0026gt;的tokens\nInput sequence.(Image source:LLaVA)\r推理 在我们阅读完LLaVA论文后，想要获得自己的LLaVA，第一件要做的事很自然是先跑通LLaVA的推理脚本。这里我考虑的是直接跑LLaVA仓库的代码，加载官方给出的模型，而不是huggingface集成好的模型。推理的入口函数是eval_model()。\n它的实现如下：\n# llava/eval/run_llava.py def eval_model(args): # Model # 将linear层和layernorm层的reset_parameters方法用空lambda函数代替，达到禁用pytorch默认初始化 disable_torch_init() # 初始化模型 model_name = get_model_name_from_path(args.model_path) tokenizer, model, image_processor, context_len = load_pretrained_model( args.model_path, args.model_base, model_name ) # 数据预处理 qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs if \u0026#34;llama-2\u0026#34; in model_name.lower(): conv_mode = \u0026#34;llava_llama_2\u0026#34; ... else: conv_mode = \u0026#34;llava_v0\u0026#34; if args.conv_mode is not None and conv_mode != args.conv_mode: print( \u0026#34;[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\u0026#34;.format( conv_mode, args.conv_mode, args.conv_mode ) ) else: args.conv_mode = conv_mode conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) # 推理 with torch.inference_mode(): output_ids = model.generate( input_ids, images=images_tensor, image_sizes=image_sizes, do_sample=True if args.temperature \u0026gt; 0 else False, temperature=args.temperature, top_p=args.top_p, num_beams=args.num_beams, max_new_tokens=args.max_new_tokens, use_cache=True, ) # 解码 outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() print(outputs) 整块函数的实现框架是：\n初始化模型：通过disable_torch_init()来加速模型加载 数据预处理：拼接caption和image token 推理：接受input_ids和images作为输入，输出output_ids 解码 之后具体来看看各个部分的实现\n初始化模型 初始化模型主要有两个函数get_model_name_from_path()和load_pretrained_model()。\nget_model_name_from_path()的实现比较简单，就是把模型路径名去除首尾\u0026quot;/\u0026ldquo;后，在将字符串按\u0026rdquo;/\u0026ldquo;分割成一个列表，之后根据列表最后一个元素是否已\u0026quot;checkpoint-\u0026ldquo;开头，返回model_name。\nload_pretrained_model()的实现则比较长，但并不复杂。\n# llava/model/builder.py def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\u0026#34;auto\u0026#34;, device=\u0026#34;cuda\u0026#34;, use_flash_attn=False, **kwargs): ... # 前面主要是一些关于device、量化参数和flash_attention的设置 # Load LLaVA model if \u0026#39;llava\u0026#39; in model.name.lower(): if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is None: # model_name要包含lora，同时提供model_base ... if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is not None: # 使用LlavaConfig从model_path中加载lora配置 ... # 通过参数model_base加载tokenizer tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) # 从base model中加载LLaVA，确保 token_num, token_dim = lm_head.out_features, lm_head.in_features model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs) ... # 加载LLaVA额外权重，检查是否存在non_lora_trainables文件，没有就假定从HF hub下载 ... # 加载lora权重 model = PeftModel.from_pretrained(model, model_path) # 合并lora权重 model = model.merge_and_unload() elif model_base is not None: ... else: if \u0026#39;mpt\u0026#39; in model_name.lower(): ... elif \u0026#39;mistral\u0026#39; in model_name.lower(): ... else: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) model = LlavaLlamaForCausalLM.from_pretrained( model_path, low_cpu_mem_usage=True, **kwargs ) else: # Load language model ... image_processor = None if \u0026#39;llava\u0026#39; in model_name.lower(): # 从模型的config中获取start和end token，如果不存在则默认为false。获取patch token，不存在默认为True mm_use_im_start_end = getattr(model.config, \u0026#34;mm_use_im_start_end\u0026#34;, False) mm_use_im_patch_token = getattr(model.config, \u0026#34;mm_use_im_patch_token\u0026#34;, True) if mm_use_im_patch_token: tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True) if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True) model.resize_token_embeddings(len(tokenizer)) # 加载vision tower vision_tower = model.get_vision_tower() if not vision_tower.is_loaded: vision_tower.load_model(device_map=device_map) if device_map != \u0026#39;auto\u0026#39;: vision_tower.to(device=device_map, dtype=torch.float16) image_processor = vision_tower.image_processor # 检查最大输入序列长度 if hasattr(model.config, \u0026#34;max_sequence_length\u0026#34;): context_len = model.config.max_sequence_length else: context_len = 2048 return tokenizer, model, image_processor, context_len load_pretrained_model()的实现逻辑是：\n初始化tokenizer、model 从model中加载vision_tower的image_processor 数据预处理 接下来是，是数据预处理的实现\n# llava/eval/run_llava.py def eval_model(args): ... # 加载query和定义image token qs = args.query image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN # 如果query中存在图片占位符 if IMAGE_PLACEHOLDER in qs: if model.config.mm_use_im_start_end: qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs) else: qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs) else: if model.config.mm_use_im_start_end: qs = image_token_se + \u0026#34;\\n\u0026#34; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#34;\\n\u0026#34; + qs # 获取对话模板并拼接成prompt if \u0026#34;llama-2\u0026#34; in model_name.lower(): ... conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() # 加载图片并处理图片 image_files = image_parser(args) images = load_images(image_files) image_sizes = [x.size for x in images] images_tensor = process_images( images, image_processor, model.config ).to(model.device, dtype=torch.float16) # tokenize prompt input_ids = ( tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#34;pt\u0026#34;) .unsqueeze(0) .cuda() ) 其中conv是Conversation类的实例，具体如下：\n# llava/conversation.py @dataclasses.dataclass class Conversation: system: str roles: List[str] messages: List[List[str]] offset: int sep_style: SeparatorStyle = SeparatorStyle.SINGLE sep: str = \u0026#34;###\u0026#34; sep2: str = None version: str = \u0026#34;Unknown\u0026#34; skip_next: bool = False def get_prompt(self): messages = self.messages if len(messages) \u0026gt; 0 and type(messages[0][1]) is tuple: ... if self.sep_style == SeparatorStyle.SINGLE: ... elif self.sep_style == SeparatorStyle.TWO: ... elif self.sep_style == SeparatorStyle.MPT: ... elif self.sep_style == SeparatorStyle.LLAMA_2: ... elif self.sep_style == SeparatorStyle.PLAIN: ... else: ... return ret def append_message(self, role, message): self.messages.append([role, message]) def process_image(self, image, image_process_mode, return_pil=False, image_format=\u0026#39;PNG\u0026#39;, max_len=1344, min_len=672): ... def get_images(self, return_pil=False): ... def to_gradio_chatbot(self): ... def copy(self): return Conversation( system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version) def dict(self): ... # messages: [[\u0026#39;USER\u0026#39;, \u0026#39;\u0026lt;image\u0026gt;\\nHow many pizzas are there in the image?\u0026#39;], [\u0026#39;ASSISTANT\u0026#39;, None]] 数据预处理的逻辑如下：\nquery拼接image占位符并用对话模板拼接成prompt 加载图像并resize tokenize prompt 我们可以简单画一张图来表示数据预处理流程：\nPreprocessing.(Image source:COCO-val-2014)\r模型推理 在推理时，主要会调用model.generate()，该方法定义在llava/model/language_model/llava_llama.py\n# llava/model/language_model/llava_llama.py class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM): config_class = LlavaConfig @torch.no_grad() def generate( self, inputs: Optional[torch.Tensor] = None, images: Optional[torch.Tensor] = None, image_sizes: Optional[torch.Tensor] = None, **kwargs, ) -\u0026gt; Union[GenerateOutput, torch.LongTensor]: position_ids = kwargs.pop(\u0026#34;position_ids\u0026#34;, None) attention_mask = kwargs.pop(\u0026#34;attention_mask\u0026#34;, None) if \u0026#34;inputs_embeds\u0026#34; in kwargs: raise NotImplementedError(\u0026#34;`inputs_embeds` is not supported\u0026#34;) if images is not None: ( inputs, position_ids, attention_mask, _, inputs_embeds, _ ) = self.prepare_inputs_labels_for_multimodal( inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes ) else: inputs_embeds = self.get_model().embed_tokens(inputs) return super().generate( position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs ) 整体的代码很短，主要集中在prepare_inputs_labels_for_multimodal()方法上\nprepare_inputs_labels_for_multimodal()的实现如下：\n# llava/model/llava_arch.py class LlavaMetaForCausalLM(ABC): def prepare_inputs_labels_for_multimodal( self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None ): vision_tower = self.get_vision_tower() if vision_tower is None or images is None or input_ids.shape[1] == 1: ... # 视觉特征 if type(images) is list or images.ndim == 5: ... else: image_features = self.encode_images(images) # TODO: image start / end is not implemented here to support pretraining. if getattr(self.config, \u0026#39;tune_mm_mlp_adapter\u0026#39;, False) and getattr(self.config, \u0026#39;mm_use_im_start_end\u0026#39;, False): raise NotImplementedError # 在推理时，attention_mask、position_ids和labels都为None # 所以实际是： # attention_mask = torch.ones_like(input_ids, dtype=torch.bool) # position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device) # labels = torch.full_like(input_ids, IGNORE_INDEX) # 使用attention_mask移除padding ... new_input_embeds = [] new_labels = [] cur_image_idx = 0 for batch_idx, cur_input_ids in enumerate(input_ids): num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum() if num_images == 0: ... # 计算文本特征 image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]] cur_input_ids_noim = [] cur_labels = labels[batch_idx] cur_labels_noim = [] for i in range(len(image_token_indices) - 1): cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]]) cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]]) split_sizes = [x.shape[0] for x in cur_labels_noim] cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim)) cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0) cur_new_input_embeds = [] cur_new_labels = [] # 将文本特征与图像特征进行拼接 for i in range(num_images + 1): cur_new_input_embeds.append(cur_input_embeds_no_im[i]) cur_new_labels.append(cur_labels_noim[i]) if i \u0026lt; num_images: cur_image_features = image_features[cur_image_idx] cur_image_idx += 1 cur_new_input_embeds.append(cur_image_features) cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype)) cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds] cur_new_input_embeds = torch.cat(cur_new_input_embeds) cur_new_labels = torch.cat(cur_new_labels) new_input_embeds.append(cur_new_input_embeds) new_labels.append(cur_new_labels) # 将输入特征进行padding ... 值得注意是提取图像特征，如果使用官方的llava-v1.5-7b的checkpoint，可以看到model中的两个参数select_layer: -2和select_feature: 'patch'，它们分别控制选择CLIP输出第几层特征和是否保留\u0026lt;CLS\u0026gt; token的特征。这也就是论文中提到的使用倒数第二层特征，具体的代码如下：\n# llava/model/multimodal_encoder/clip_encoder.py class CLIPVisionTower(nn.Module): def feature_select(self, image_forward_outs): image_features = image_forward_outs.hidden_states[self.select_layer] if self.select_feature == \u0026#39;patch\u0026#39;: image_features = image_features[:, 1:] elif self.select_feature == \u0026#39;cls_patch\u0026#39;: image_features = image_features else: raise ValueError(f\u0026#39;Unexpected select feature: {self.select_feature}\u0026#39;) return image_features prepare_inputs_labels_for_multimodal()的整体逻辑如下图所示：\nTokenize.(Image source:COCO-val-2014)\r得到inputs_embeds后，会将其传给父类的generate方法，这里会调用LlamaForCausalLM的generate()方法，它的实现定义在transformers的llama/modeling_llama.py中。\n输出后处理 最后一步就是通过tokenizer的batch_decode()将得到的output_ids解码成文本。\n# llava/eval/run_llava.py def eval_model(args): ... outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() print(outputs) 至此，LLaVA的推理过程就结束了，我们用一张图来表示整个过程：\nLLaVA Inference.(Image source:COCO-val-2014)\r参考 [1] Paper:Visual Instruction Tuning(https://arxiv.org/abs/2304.08485)\n[2] Github:LLaVA(https://github.com/haotian-liu/LLaVA)\n[3] Zhihu:HelloWorld-LLaVA论文解读(https://zhuanlan.zhihu.com/p/696112028)\n[4] Zhihu:HelloWorld-图解LLaVA推理(https://zhuanlan.zhihu.com/p/696654492)\n","permalink":"http://localhost:1313/posts/llava_01/","summary":"\u003ch1 id=\"从llava-10论文开始\"\u003e从LLaVA-1.0论文开始\u003c/h1\u003e\n\u003cp\u003eLLaVA: Visual Instruction Tuning(NeurIPS 2023 Oral)\u003c/p\u003e\n\u003cp\u003ePaper link: \u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003e\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003ehttps://arxiv.org/abs/2304.08485\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGithub: \u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003e\u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003ehttps://github.com/haotian-liu/LLaVA\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"论文亮点\"\u003e论文亮点\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e多模态 instruction-following 数据：使用GPT4-text-only将图像-文本对转换成适当的instruction-following格式，主要有对话（Conversation）、细节描述（Detailed description）和复杂推理（Complex reasoning）三类\u003c/li\u003e\n\u003cli\u003e多模态大模型：使用CLIP-L/14作为视觉编码器，大模型使用Vicuna（llama-13B），以及使用全连接层作为投影层，将视觉tokens映射到LLM的word embedding space\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"网络架构\"\u003e网络架构\u003c/h2\u003e\n\u003cp\u003e结构上vision encoder使用的是CLIP-L/14（\u003cstrong\u003e使用倒数第二层得到的特征，实验证明效果会更好，作者分析认为是因为最后一层聚焦图像的全局信息，倒数第二层更关注局部特征\u003c/strong\u003e）进行编码，之后使用一个投影层（\u003cstrong\u003e全连接层\u003c/strong\u003e）对齐到word embedding，输入的文本经过embedding模型得到word embedding，两个embedding拼接后一起输入给llama。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/LLaVA.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eLLaVA Architecture.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003ch2 id=\"数据集构成及数据收集细节\"\u003e数据集构成及数据收集细节\u003c/h2\u003e\n\u003cp\u003e训练分为两个阶段，第一个阶段只更新投影层，使用的是一个简单视觉问答的数据集（CC3M中过滤出595K）；第二个阶段训练llama和投影层，数据集使用的是COCO数据集（158K）和ScienceQA微调出了两个版本。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e预训练数据集（CC3M过滤）\u003c/strong\u003e：使用Spacy（NLP库）为CC3M上的每个caption提取名词短语，并计算每个唯一名词短语的频率。忽略掉频率小于3的名词短语，对于频率大于100的名词短语随机选择一个数量为100的子集。最终产生595K个图像-文本对。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_filtered.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M Filtered.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e将图像文本对采用简单的指令构造，用下图所示的问题提示GPT-4，生成对应的答案。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/LLaVA_01/cc3m_prompt.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\r\n\u003ccenter\u003eCC3M image description.(Image source:\u003ca href=\"https://arxiv.org/abs/2304.08485\"\u003eLLaVA\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\r\n\u003cp\u003e指令格式如下：\u003c/p\u003e","title":"从零开始学LLaVA 01"},{"content":"什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性（likelihood）$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\\theta^{*}$，即：\nθ∗=argmaxθ∏i=1npθ(x(i))=argmaxθ∑i=1nlog⁡pθ(x(i))\n但是，直接计算似然性$p_{\\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\\theta}(x)$可以依靠概率的链式法则$p_{\\theta}(x) = \\frac{p_{\\theta}(x | z)p_{\\theta}(z)}{p_{\\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为Evidence Lower Bound(ELBO)。因此，我们可以将最大化ELBO作为优化潜变量模型的代理目标，即：\nlog⁡pθ(x)≥Eqϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n此处的$q_{\\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\\theta}(z|x)$。现在，让我们尝试更深入地探讨为什么ELBO是我们希望最大化的目标。我们在这里给出第一种证明方法：\nlog⁡pθ(x)=log⁡∫pθ(x|z)pθ(z)dz=log⁡∫pθ(x|z)pθ(z)qϕ(z|x)qϕ(z|x)dz=log⁡Ex∼qϕ(z|x)[pθ(x|z)pθ(z)qϕ(z|x)]≥Ex∼qϕ(z|x)[pθ(x|z)pθ(z)qϕ(z|x)]\n在上面的推导中，我们直接使用琴生不等式。然而，这并没有为我们提供太多有用的信息，仅知道ELBO是数据的下界并不能真正告诉我们为什么要最大化它作为目标，为此我们给出另一种证明，在证明开始之前，我想先补充一点关于KL散度的知识。\nKL散度$D_{KL}(P|Q)$用来衡量使用分布$Q$表示分布$P$会丢失多少信息，一般分布$P$是数据的真实分布，分布$Q$是数据的理论分布、估计模型的分布或是$X$的近似分布。KL散度仅当概率分布$P$和$Q$各自总和均为1，且对于任何x均满足$P(X) \u0026gt; 0$及$Q(x) \u0026gt; 0$才有定义。对于连续随机变量，其概率分布$P$和$Q$的KL散度可按积分方式定义为：\nDKL(P|Q)=∫p(x)ln⁡p(x)q(x)dx=Ex∼p(x)[ln⁡p(x)q(x)]\n接下来我们进行证明：\nlog⁡pθ(x)=log⁡pθ(x)∫qϕ(z|x)dz=∫qϕ(z|x)(log⁡pθ(x))dz=Ex∼qϕ(z|x)[log⁡pθ(x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)pθ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)pθ(z|x)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]+Ex∼qϕ(z|x)[log⁡qϕ(z|x)pθ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]+DKL(qϕ(z|x)||pθ(z|x))≥Ex∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n从这个推导中，我们可以清楚看到证据等于ELBO加上近似后验概率和真实后验概率的KL散度。事实上，正是这个KL散度被琴生不等式神奇地删除了，尽管这个KL散度似乎与我们上面的定义有所不同，我们先将它放在一边。理解这个不仅是理解ELBO与证据之间的关键，更是理解为什么优化ELBO是一个合适的目标的关键。\n首先，我们明确ELBO确实是一个下界，因为证据和ELBO之间的差异是严格非负项。其次，我们想要优化近似后验$q_{\\phi}(z|x)$的参数，以精确匹配真实后验$p_{\\theta}(z|x)$，这是通过KL散度来实现的。但正如我们前面所说我们无法获得真实后验$p_{\\theta}(z|x)$。然而注意到推导式中，证据$\\log p_{\\theta}(x)$对于参数$\\phi$始终是一个常数。由于ELBO和KL散度之和是一个常数，因此ELBO对于$\\phi$的最大化必然导致KL散度的最小化。因此，最大化ELBO可以作为学习如何完美建模真实潜在后验分布的代理。\n对于推导过程中出现的$D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z|x))$（逆KL散度）而不是$D_{KL}(p_{\\theta}||q_{\\phi})$（前向KL散度），在Eric Jang关于贝叶斯变分法的文章中给出了很好的解释。简单来说就是，前向KL散度更在意真实分布中的常见事件，而逆KL散度更在意真实分布中的罕见事件，如下图所示：\nForward and reversed KL divergence have different demands on how to match two distributions.(Image source:Eric Jang) 让我们进一步剖析ELBO：\nEx∼qϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)]+Ex∼qϕ(z|x)[log⁡pθ(z)qϕ(z|x)]=Ex∼qϕ(z|x)[log⁡pθ(x|z)]−DKL(qϕ(z|x)||pθ(z))\n整理一下：\nlog⁡pθ(x)−DKL(qϕ(z|x)||pθ(z|x))=Ex∼qϕ(z|x)[log⁡pθ(x|z)]−DKL(qϕ(z|x)||pθ(z))\n上式的否定定义就是我们损失函数的定义：\nLVAE(θ,ϕ)=−Ex∼qϕ(z|x)[log⁡pθ(x|z)]+DKL(qϕ(z|x)||pθ(z))θ∗,ϕ∗=argminminθ,ϕ⁡LVAE\n我们现在试图用图形来绘制上述过程:\nThe graphical model involved in VAE.(Image source:Lilian Weng) 条件概率$p_{\\theta}(x|z)$定义了一个生成模型，类似于AE中的解码器 近似后验概率$q_{\\phi}(z|x)$是概率编码器，类似于AE中的编码器 损失函数中的重构项需要$z \\sim q_{\\phi}(z|x)$。然而采样是一个随机过程，因此我们无法反向传播梯度。VAE的一个决定性特征就是如何对参数$\\theta$和$\\phi$同时优化。VAE编码器通常选择对具有对角协方差的多元高斯进行建模，并且先验通常选择标准多元高斯：\nz∼qϕ(z|x)=N(z;μϕ(x),σϕ2(x)I)z=μ+σ⊙ϵ,whereϵ∼N(0,I)\n其中，$\\odot$表示逐元素乘积。换句话说，任意高斯分布都可以解释为标准高斯分布。在这个重参数版本下，可以根据需要计算$\\phi$的梯度，以优化$\\mu_{\\phi}$和$\\sigma_{\\phi}$。\nIllustration of variational autoencoder model with the multivariate Gaussian assumption.(Image source:Lilian Weng) HVAE 什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) 参考 [1] Blog:Lilian-VAE(https://lilianweng.github.io/posts/2018-08-12-vae/)\n[2] Blog:evjang-Variational bayes(https://blog.evjang.com/2016/08/variational-bayes.html)\n[3] Paper:Denoising Diffusion Probabilistic Models(https://arxiv.org/abs/2006.11239)\n[4] Paper:Understanding Diffusion Models: A Unified Perspective(https://arxiv.org/abs/2208.11970)\n[5] Wiki:KL散度(https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)\n","permalink":"http://localhost:1313/posts/ddpm/","summary":"\u003ch1 id=\"什么是generative-model\"\u003e什么是Generative Model?\u003c/h1\u003e\n\u003cp\u003e我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\u003c/p\u003e\n\u003cp\u003e在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\u003c/p\u003e\n\u003ch2 id=\"ae--vae\"\u003eAE \u0026amp; VAE\u003c/h2\u003e\n\u003cp\u003e我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/AE.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eAutoencoder.(Image source:\u003ca href=\"https://lilianweng.github.io/posts/2018-08-12-vae/\"\u003eLilian Weng\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e这就是\u003cstrong\u003eAutoencoder(AE)\u003c/strong\u003e的想法，\u003cstrong\u003eAE\u003c/strong\u003e包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariational Autoencoder（VAE）\u003c/strong\u003e的思想实际上与\u003cstrong\u003eAE\u003c/strong\u003e不太相似。在\u003cstrong\u003eVAE\u003c/strong\u003e中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先验概率$p_{\\theta}(z)$\u003c/li\u003e\n\u003cli\u003e似然性（likelihood）$p_{\\theta}(x | z)$\u003c/li\u003e\n\u003cli\u003e后验概率$p_{\\theta}(z | x)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$\u003c/li\u003e\n\u003cli\u003e然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e那么这个分布我们似乎就可以写出来了:\u003c/p\u003e\n\u003cp\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo data-mjx-texclass=\"OP\"\u003e∫\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e|\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ez\u003c/mi\u003e\u003c/math\u003e\u003c/p\u003e","title":"Diffusion Models"},{"content":"","permalink":"http://localhost:1313/about/","summary":"about","title":"关于"}]
[{"content":"什么是Generative Model? 我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\n在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\nAE \u0026amp; VAE 我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\nAutoencoder.(Image source:Lilian Weng) 这就是Autoencoder(AE)的想法，AE包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\nVariational Autoencoder（VAE）的思想实际上与AE不太相似。在VAE中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\n先验概率$p_{\\theta}(z)$ 似然性$p_{\\theta}(x | z)$ 后验概率$p_{\\theta}(z | x)$ 假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\n先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$ 然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$ 那么这个分布我们似乎就可以写出来了:\npθ∗(x(i))=∫pθ∗(x(i)|z)pθ∗(z)dz\n为了让生成的样本$x^{(i)}$更符合真实数据，就是找到最大化生成样本的概率的最优参数$\\theta^{*}$，即：\nθ∗=argmaxθ∏i=1npθ(x(i))=argmaxθ∑i=1nlog⁡pθ(x(i))\n但是，直接计算似然性$p_{\\theta}(x)$很困难，因为对复杂模型而言，积分潜变量$z$是棘手的。或许你可能会想到，我们计算似然性$p_{\\theta}(x)$可以依靠概率的链式法则$p_{\\theta}(x) = \\frac{p_{\\theta}(x | z)p_{\\theta}(z)}{p_{\\theta}(z |x)}$，但很遗憾我们无法直接计算$p_{\\theta}(z | x)$。然而，我们使用这两个等式可以推导出一个下界，称为Evidence Lower Bound(ELBO)。因此，我们可以将最大化ELBO作为优化潜变量模型的代理目标，即：\nlog⁡pθ(x)≥Eqϕ(z|x)[log⁡pθ(x|z)pθ(z)qϕ(z|x)]\n此处的$q_{\\phi}(z|x)$是一个近似的后验概率，它可以被认为是一个可参数化的模型，用于估计给定观测值$x$的潜变量的真实分布，也就是说它试图逼近$p_{\\theta}(z|x)$\n什么是Diffusion Models？ Diffusion models的灵感来自非平衡热力学。它们定义了一个马尔科夫链扩散步骤，以缓慢地向数据添加随机噪声，然后学习逆扩散过程以从噪声中构建所需的数据样本。与VAE或Flow-based models不同，diffusion models是通过固定程序学习的，并且潜变量具有高纬度（与原始数据相同）。\nDiffusion Process.(Image source:Hung-yi Lee) 根据上面的描述，我们脑海中可以很自然的将扩散过程表示出来。但是在denoising diffusion probabilistic models（DDPM;Ho et al.2020）中的算法并不这么简单。\nTraining Training.(Image source:DDPM) ","permalink":"https://attract666.github.io/posts/ddpm/","summary":"\u003ch1 id=\"什么是generative-model\"\u003e什么是Generative Model?\u003c/h1\u003e\n\u003cp\u003e我们可以看到的现实中的各种图片，视为已经观察到的数据，那么理论上必然存在一个分布$p(x)$可以描述它们。如果我们想要生成一张图片，很自然的想法就是从这个分布$p(x)$中采样一个点。也就是说，如果我们可以通过某种方法学习到对真实数据映射的数据分布$p(x)$，那么我们就可以通过这个分布生成新的样本。但是现实生活中，这个分布$p(x)$是复杂的，我们很难真的完全建模出来，因此有时我们也使用学习到的模型来评估观察到的货采样数据的可能性。\u003c/p\u003e\n\u003cp\u003e在现有的Generative Model有以下几个大方向。GAN对复杂分布的采样过程进行建模，并以对抗方式学习。likelihood-based旨在学习为观察到的数据样本分配高可能性的模型，这包括自回归模型和VAE。energy-based将分布学习为任意灵活的能量函数，然后将其归一化。\u003c/p\u003e\n\u003ch2 id=\"ae--vae\"\u003eAE \u0026amp; VAE\u003c/h2\u003e\n\u003cp\u003e我们所观察到的数据可以被视为一些更高层次的表示的函数所生成的，我们可以近似的描述我们观察到的数据的潜在表示，用随机变量$z$表示，也称之为潜变量（latent variable）。在生成建模中，我们通常寻求学习较低维度的潜在表示。这是因为如果没有强大的先验知识，试图学习比观察更高维的表示是徒劳的。另一方面，学习低维潜变量也可以被视为一种压缩形式，并且可以潜在揭示描述观察结果的语义上有意义的结构。\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"/img/DDPM/AE.png\" width=\"70%\" title=\"\"\u003e\u003c/center\u003e\n\u003ccenter\u003eAutoencoder.(Image source:\u003ca href=\"https://lilianweng.github.io/posts/2018-08-12-vae/\"\u003eLilian Weng\u003c/a\u003e)\u003c/center\u003e\u003cbr/\u003e\n\u003cp\u003e这就是\u003cstrong\u003eAutoencoder(AE)\u003c/strong\u003e的想法，\u003cstrong\u003eAE\u003c/strong\u003e包含编码器$g(.)$和解码器$f(.)$，输入$x$的低维压缩表示是$z = g_{\\phi}(x)$，重构后的输出是$x\u0026rsquo; = f_{\\theta}(g_{\\phi}(x))$。参数$(\\theta, \\phi)$的学习目标是输入与输出尽可能相似，即$x \\approx x\u0026rsquo;$，这个损失函数我们也称之为重构损失。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariational Autoencoder（VAE）\u003c/strong\u003e的思想实际上与\u003cstrong\u003eAE\u003c/strong\u003e不太相似。在\u003cstrong\u003eVAE\u003c/strong\u003e中，我们不想将输入映射到固定向量中，而是将其映射到分布中。我们将此分布记为$p_{\\theta}$，参数为$\\theta$，于是我们可以有如下定义：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先验概率$p_{\\theta}(z)$\u003c/li\u003e\n\u003cli\u003e似然性$p_{\\theta}(x | z)$\u003c/li\u003e\n\u003cli\u003e后验概率$p_{\\theta}(z | x)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e假设我们知道这个分布的真实参数$\\theta^{*}$，为了使得我们的生成模型生成看起来更像真实数据的样本$x^{(i)}$，我们可以：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先从已知的分布$p_{\\theta^{*}}(z)$中采样$z^{(i)}$\u003c/li\u003e\n\u003cli\u003e然后从条件分布$p_{\\theta^{*}}(x | z = z^{(i)})$中采样一个$x^{(i)}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e那么这个分布我们似乎就可以写出来了:\u003c/p\u003e\n\u003cp\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo data-mjx-texclass=\"OP\"\u003e∫\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e|\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ez\u003c/mi\u003e\u003c/math\u003e\u003c/p\u003e","title":"Diffusion Models"},{"content":"","permalink":"https://attract666.github.io/about/","summary":"about","title":"关于"}]